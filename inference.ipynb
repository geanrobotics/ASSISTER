{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f09f07b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import cv2\n",
    "import skimage.transform\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c33710f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = {1: 'walk',\n",
    " 2: 'straight',\n",
    " 3: 'three',\n",
    " 4: 'meters',\n",
    " 5: 'pedestrian',\n",
    " 6: 'on',\n",
    " 7: 'your',\n",
    " 8: 'left',\n",
    " 9: 'and',\n",
    " 10: 'then',\n",
    " 11: 'vegetation',\n",
    " 12: 'right',\n",
    " 13: 'turn',\n",
    " 14: 'to',\n",
    " 15: 'nine',\n",
    " 16: 'forward',\n",
    " 17: 'one',\n",
    " 18: ',',\n",
    " 19: 'two',\n",
    " 20: 'afterwards,',\n",
    " 21: 'four',\n",
    " 22: 'around',\n",
    " 23: 'eleven',\n",
    " 24: 'go',\n",
    " 25: 'ten',\n",
    " 26: 'road',\n",
    " 27: 'in',\n",
    " 28: 'front',\n",
    " 29: 'five',\n",
    " 30: 'six',\n",
    " 31: 'seven',\n",
    " 32: 'fence',\n",
    " 33: 'building',\n",
    " 34: '',\n",
    " 35: 'wall',\n",
    " 36: 'vehicle',\n",
    " 37: 'pole',\n",
    " 38: 'static',\n",
    " 39: 'dynamic',\n",
    " 40: 'traffic_sign',\n",
    " 41: '<unk>',\n",
    " 42: '<start>',\n",
    " 43: '<end>',\n",
    " 0: '<pad>'}\n",
    "\n",
    "word2id = {'walk': 1,\n",
    " 'straight': 2,\n",
    " 'three': 3,\n",
    " 'meters': 4,\n",
    " 'pedestrian': 5,\n",
    " 'on': 6,\n",
    " 'your': 7,\n",
    " 'left': 8,\n",
    " 'and': 9,\n",
    " 'then': 10,\n",
    " 'vegetation': 11,\n",
    " 'right': 12,\n",
    " 'turn': 13,\n",
    " 'to': 14,\n",
    " 'nine': 15,\n",
    " 'forward': 16,\n",
    " 'one': 17,\n",
    " ',': 18,\n",
    " 'two': 19,\n",
    " 'afterwards,': 20,\n",
    " 'four': 21,\n",
    " 'around': 22,\n",
    " 'eleven': 23,\n",
    " 'go': 24,\n",
    " 'ten': 25,\n",
    " 'road': 26,\n",
    " 'in': 27,\n",
    " 'front': 28,\n",
    " 'five': 29,\n",
    " 'six': 30,\n",
    " 'seven': 31,\n",
    " 'fence': 32,\n",
    " 'building': 33,\n",
    " '': 34,\n",
    " 'wall': 35,\n",
    " 'vehicle': 36,\n",
    " 'pole': 37,\n",
    " 'static': 38,\n",
    " 'dynamic': 39,\n",
    " 'traffic_sign': 40,\n",
    " '<unk>': 41,\n",
    " '<start>': 42,\n",
    " '<end>': 43,\n",
    " '<pad>': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5391b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sets device to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "307adc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_caption(enc_caption):\n",
    "    dec_caption = [\n",
    "        id2word[id]\n",
    "        for id in caption.numpy()\n",
    "        if id2word[id] not in [\"<start>\", \"<end>\", \"<unk>\", \"<pad>\"]\n",
    "    ]\n",
    "    return \" \".join(dec_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef4ac207",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, enc_image_size=14):\n",
    "        super(ImageEncoder, self).__init__()\n",
    "        self.enc_image_size = enc_image_size\n",
    "\n",
    "        # pretrained ImageNet ResNet-101\n",
    "        resnet = torchvision.models.resnet101(pretrained=True)\n",
    "\n",
    "        # remove linear and pool layers\n",
    "        modules = list(resnet.children())[:-2]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "\n",
    "        # resize image to fixed size using adaptive pool to allow input images of variable size\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((enc_image_size, enc_image_size))\n",
    "\n",
    "        self.fine_tune()\n",
    "\n",
    "    def fine_tune(self, fine_tune=True):\n",
    "        \"\"\"\n",
    "        Allow or prevent computation of the gradients for convolutional blocks 2 through 4 of the image encoder.\n",
    "        :param fine_tune: boolean\n",
    "        \"\"\"\n",
    "        for param in self.resnet.parameters():\n",
    "            param.requires_grad = False\n",
    "        # if fine-tuning, fine-tune convolutional blocks 2 through 4\n",
    "        for child in list(self.resnet.children())[5:]:\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = fine_tune\n",
    "    \n",
    "    def forward(self, images):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "        :param images: images, a tensor of dim (batch_size, 3, image_size, image_size)\n",
    "        :return enc_images: encoded repr of images, a tensor of dim (batch_size, enc_image_size, enc_image_size, 2048)\n",
    "        \"\"\"\n",
    "        out = self.resnet(images)       # (batch_size, 2048, image_size/32, image_size/32)\n",
    "        out = self.adaptive_pool(out)   # (batch_size, 2048, enc_image_size, enc_image_size)\n",
    "        out = out.permute(0, 2, 3, 1)   # (batch_size, enc_image_size, enc_image_size, 2048)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54307443",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n",
    "        \"\"\"\n",
    "        :param encoder_dim: feature size of encoded images\n",
    "        :param decoder_dim: size of decoder's RNN\n",
    "        :param attention_dim: size of the attention network\n",
    "        \"\"\"\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.encoder_attn = nn.Linear(encoder_dim, attention_dim)   # linear layer to transform encoder's output\n",
    "        self.decoder_attn = nn.Linear(decoder_dim, attention_dim)   # linear layer to transform decoder's output\n",
    "        self.full_attn = nn.Linear(attention_dim, 1)\n",
    "    \n",
    "    def forward(self, encoder_out, decoder_hidden):\n",
    "        \"\"\"\n",
    "        Forward propagation (uses Multiplicative attention).\n",
    "        :param encoder_out: encoded images, a tensor of dim (batch_size, num_pixels, encoder_dim)\n",
    "        :param decoder_hidden: previous decoder output, a tensor of dim (batch_size, decoder_dim)\n",
    "        \"\"\"\n",
    "        attn1 = self.encoder_attn(encoder_out)          # (batch_size, num_pixels, attention_dim)\n",
    "        attn2 = self.decoder_attn(decoder_hidden)       # (batch_size, attention_dim)\n",
    "        attn = self.full_attn(F.relu(attn1 + attn2.unsqueeze(1)))    # (batch_size, num_pixels, 1)\n",
    "\n",
    "        # apply softmax to calculate weights for weighted encoding based on attention\n",
    "        alpha = F.softmax(attn, dim=1)                  # (batch_size, num_pixels, 1)\n",
    "        attn_weighted_encoding = (encoder_out * alpha).sum(dim=1)  # (batch_size, encoder_dim)\n",
    "        alpha = alpha.squeeze(2)  # (batch_size, num_pixels)\n",
    "        return attn_weighted_encoding, alpha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fd5c49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderWithAttention(nn.Module):\n",
    "    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size, encoder_dim=2048, dropout=0.5):\n",
    "        \"\"\"\n",
    "        :param attention_dim: size of attention network\n",
    "        :param embed_dim: embedding_size\n",
    "        :param decoder_dim: feature size of decoder's RNN\n",
    "        :param vocab_size: size of vocabulary\n",
    "        :param encoder_dim: feature size of encoded images\n",
    "        :param dropout: dropout\n",
    "        \"\"\"\n",
    "        super(DecoderWithAttention, self).__init__()\n",
    "        self.encoder_dim = encoder_dim\n",
    "        self.attention_dim = attention_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        self.decoder_dim = decoder_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.attention = BahdanauAttention(encoder_dim, decoder_dim, attention_dim)     # attention network\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)                    # embedding layer\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        self.decode_step = nn.LSTMCell(embed_dim + encoder_dim, decoder_dim, bias=True) # decoding LSTMCell\n",
    "        self.init_h = nn.Linear(encoder_dim, decoder_dim)   # linear layer to find initial hidden state of LSTMCell\n",
    "        self.init_c = nn.Linear(encoder_dim, decoder_dim)    # linear layer to find initial cell state of LSTMCell\n",
    "        self.f_beta = nn.Linear(decoder_dim, encoder_dim)   # linear layer to create a sigmoid-activated gate\n",
    "\n",
    "        self.fc = nn.Linear(decoder_dim, vocab_size)        # linear layer to find scores over vocabulary\n",
    "        \n",
    "        self.goal = nn.Linear(2, encoder_dim)\n",
    "        \n",
    "        self.init_weights()     # initialize some layers with the uniform distribution\n",
    "    \n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Initialize some layers with the uniform distribution for easier convergence.\n",
    "        \"\"\"\n",
    "        self.embedding.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.fc.bias.data.fill_(0)\n",
    "        self.fc.weight.data.uniform_(-0.1, 0.1)\n",
    "    \n",
    "    def init_hidden_state(self, encoder_out):\n",
    "        \"\"\"\n",
    "        Creates the initial hidden and cell states for the decoder's LSTM based on the encoded images.\n",
    "        :param encoder_out: encoded_images, a tensor of dimension (batch_size, num_pixels, encoder_dim)\n",
    "        :return hidden_state, cell_state\n",
    "        \"\"\"\n",
    "        mean_encoder_out = encoder_out.mean(dim=1)  # (batch_size, encoder_dim)\n",
    "        h = self.init_h(mean_encoder_out)   # (batch_size, decoder_dim)\n",
    "        c = self.init_c(mean_encoder_out)   # (batch_size, decoder_dim)\n",
    "        return h, c\n",
    "    \n",
    "    def forward(self, encoder_out, encoded_captions, caption_lens, goal):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "        :param encoder_out: encoded images, a tensor of dimension (batch_size, enc_image_size, enc_image_size, encoder_dim)\n",
    "        :param encoded_captions: encoded captions, a tensor of dimension (batch_size, max_caption_length)\n",
    "        :param caption_lengths: caption lengths, a tensor of dimension (batch_size, 1)\n",
    "        :return: scores for vocabulary, sorted encoded captions, decode lengths, weights, sort indices\n",
    "        \"\"\"\n",
    "        batch_size = encoder_out.size(0)\n",
    "\n",
    "        # flatten image\n",
    "        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)     # (batch_size, num_pixels, encoder_dim)\n",
    "        \n",
    "        encoder_goal = self.goal(goal).view(batch_size, -1, encoder_dim) \n",
    "        encoder_out = torch.cat([encoder_out, encoder_goal], 1)\n",
    "        \n",
    "        num_pixels = encoder_out.size(1)\n",
    "#         print(encoder_dim, encoder_out.shape, self.encoder_dim, encoder_goal.shape)\n",
    "        \n",
    "        # sort the input data by the decreasing caption length\n",
    "        caption_lens, sort_idx = caption_lens.squeeze(1).sort(dim=0, descending=True)\n",
    "        encoder_out = encoder_out[sort_idx]\n",
    "        encoded_captions = encoded_captions[sort_idx]\n",
    "\n",
    "        # embedding\n",
    "        embeddings = self.embedding(encoded_captions)   # (batch_size, max_caption_length, embed_dim)\n",
    "\n",
    "        # initialize lstm state\n",
    "        h, c = self.init_hidden_state(encoder_out)      # (batch_size, decoder_dim)\n",
    "\n",
    "        # We won't decode at the <end> position, since we've finished generating as soon as we generate <end>\n",
    "        # So, decoding lengths are caption lengths - 1\n",
    "        decode_lens = (caption_lens - 1).tolist()\n",
    "\n",
    "        # create tensors to hold word prediction scores and alphas\n",
    "        predictions = torch.zeros(batch_size, max(decode_lens), vocab_size).to(device)\n",
    "        alphas = torch.zeros(batch_size, max(decode_lens), num_pixels).to(device)\n",
    "\n",
    "        # At each time-step, decode by attention-weighing the encoder's output based on the \n",
    "        # decoder's previous hidden state output then generate a new word in the decoder with \n",
    "        # the previous word and the attention weighted encoding\n",
    "        for t in range(max(decode_lens)):\n",
    "            # get the batch size for each time step t\n",
    "            batch_size_t = sum([l > t for l in decode_lens])\n",
    "            \n",
    "#             print(decode_lens, encoder_out.shape, h.shape, batch_size_t)\n",
    "            # get the attention weighted encodings (batch_size_t, encoder_dim)\n",
    "            attention_weighted_encoding, alpha = self.attention(encoder_out[:batch_size_t], h[:batch_size_t])\n",
    "\n",
    "            gate = F.sigmoid(self.f_beta(h[:batch_size_t]))     # sigmoid gating scalar, (batch_size_t, encoder_dim)\n",
    "            attention_weighted_encoding = gate * attention_weighted_encoding\n",
    "\n",
    "            # get the decoder hidden state and cell state based on the embeddings of timestep t word \n",
    "            # and the attention weighted encoding\n",
    "            h, c = self.decode_step(\n",
    "                torch.cat([embeddings[:batch_size_t, t, :], attention_weighted_encoding], dim=1),\n",
    "                (h[:batch_size_t], c[:batch_size_t])\n",
    "            )   # (batch_size_t, decoder_dim)\n",
    "\n",
    "            # get the next word prediction\n",
    "            preds = self.fc(self.dropout(h))    # (batch_size_t, vocab_size)\n",
    "\n",
    "            # save the prediction and alpha for every time step\n",
    "            predictions[:batch_size_t, t, :] = preds\n",
    "            alphas[:batch_size_t, t, :] = alpha\n",
    "#             print(alpha.shape)\n",
    "    \n",
    "        return predictions, encoded_captions, decode_lens, alphas, sort_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ca7d4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6719b7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model parameters\n",
    "# embed_dim = 512      # dimension of word embeddings\n",
    "# attention_dim = 512  # dimension of attention linear layers\n",
    "# decoder_dim = 512    # dimension of decoder RNN\n",
    "# encoder_dim = 2048\n",
    "# encoder_lr = 1e-4  # learning rate for encoder if fine-tuning\n",
    "# decoder_lr = 4e-4  # learning rate for decoder\n",
    "# grad_clip = 5.  # clip gradients at an absolute value of\n",
    "# alpha_c = 1.  # regularization parameter for 'doubly stochastic attention', as in the paper\n",
    "vocab_size = len(word2id)\n",
    "# best_bleu4 = 0.  # BLEU-4 score right now\n",
    "# lr_decay_factor = 0.8\n",
    "# lr_decay_patience = 8\n",
    "# best_bleu4 = 0\n",
    "\n",
    "# start_epoch = 1\n",
    "# num_epochs = 10\n",
    "# epochs_since_improvement = 0  # keeps track of number of epochs since there's been an improvement in validation BLEU\n",
    "\n",
    "# fine_tune_encoder = False  # fine-tune encoder?\n",
    "# checkpoint = None  # image_captioning_best.pth\n",
    "# cudnn.benchmark = True  # set to true only if inputs to model are fixed size; otherwise lot of computational overhead\n",
    "\n",
    "\n",
    "checkpoint = torch.load(\"./image_captioning_checkpoint_9.pth\")\n",
    "\n",
    "start_epoch = checkpoint['epoch'] + 1\n",
    "best_bleu4 = checkpoint['bleu-4']\n",
    "encoder = checkpoint['encoder']\n",
    "decoder = checkpoint['decoder']\n",
    "encoder_optimizer = checkpoint['encoder_optimizer']\n",
    "decoder_optimizer = checkpoint['decoder_optimizer']\n",
    "\n",
    "\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ba3dd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_image_goal_caption(encoder, decoder, image_path, goal, word_map, beam_size=4):\n",
    "    \"\"\"\n",
    "    Reads an image and captions it with beam search as well as plot attention maps.\n",
    "    \"\"\"\n",
    "    k = beam_size\n",
    "    \n",
    "    # id to word mapping\n",
    "    rev_word_map = {id: word for word, id in word_map.items()}\n",
    "\n",
    "    # read and pre-process image\n",
    "    img = np.array(Image.open(image_path).convert('RGB'))\n",
    "    img = np.array(Image.open(image_path).convert('RGB'))\n",
    "    img = cv2.resize(img, (256, 256))\n",
    "\n",
    "    # sanity check\n",
    "    assert img.shape == (256, 256, 3)\n",
    "    assert np.max(img) <= 255\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    img = transform(img)    # (3, 256, 256)\n",
    "    goal = torch.FloatTensor(goal).unsqueeze(0).to(device)\n",
    "\n",
    "    # encode the image\n",
    "    encoder_out = encoder(img.unsqueeze(0).to(device))     # (1, enc_image_size, enc_image_size, encoder_dim)\n",
    "    enc_image_size = encoder_out.size(1)\n",
    "    encoder_dim = encoder_out.size(3)\n",
    "\n",
    "    # flatten encoded image representation\n",
    "    encoder_out = encoder_out.view(1, -1, encoder_dim)  # (1, num_pixels, encoder_dim)\n",
    "    \n",
    "    encoder_goal = decoder.goal(goal).view(1, -1, encoder_dim) \n",
    "    encoder_out = torch.cat([encoder_out, encoder_goal], 1)\n",
    "    \n",
    "    num_pixels = encoder_out.size(1)\n",
    "\n",
    "    # we'll treat the problem as having a batch size of k\n",
    "    encoder_out = encoder_out.expand(k, num_pixels, encoder_dim)    # (k, num_pixels, encoder_dim)\n",
    "\n",
    "    # tensor to store top k previous words at each step; now they're just <start>\n",
    "    top_k_prev_words = torch.tensor([[word_map['<start>']]] * k, dtype=torch.long).to(device)   # (k, 1)\n",
    "\n",
    "    # tensor to store top k sequences; now they're just <start>\n",
    "    top_k_seqs = top_k_prev_words   # (k, 1)\n",
    "\n",
    "    # tensor to store top k sequences' scores; now they're just 0\n",
    "    top_k_scores = torch.zeros(k, 1).to(device)     # (k, 1)\n",
    "\n",
    "    # tensor to store top k sequences' alphas; now they're just 1s\n",
    "    top_k_seqs_alpha = torch.ones(k, 1, enc_image_size, enc_image_size).to(device)  # (k, 1, enc_image_size, enc_image_size)\n",
    "#     print(top_k_seqs_alpha.shape, \"*\"*5)\n",
    "    # lists to store completed sequences along with their alphas and scores\n",
    "    complete_seqs = []\n",
    "    complete_seqs_alpha = []\n",
    "    complete_seqs_scores = []\n",
    "    word_score = []\n",
    "\n",
    "    # start decoding\n",
    "    step = 1\n",
    "    h, c = decoder.init_hidden_state(encoder_out)\n",
    "\n",
    "    # s is a number less than or equal to k, because sequences are removed from this process once they hit <end>\n",
    "    while True:\n",
    "        embeddings = decoder.embedding(top_k_prev_words).squeeze(1)  # (s, embed_dim)\n",
    "\n",
    "        attention_weighted_encoding, alpha = decoder.attention(encoder_out, h)  # (s, encoder_dim), (s, num_pixels, 1)\n",
    "        alpha_word = alpha[:, 196].view(-1, 1, 1)\n",
    "        alpha = alpha[:, :196]\n",
    "        alpha = alpha.view(-1, enc_image_size, enc_image_size)     # (s, enc_image_size, enc_image_size)\n",
    "\n",
    "        gate = F.sigmoid(decoder.f_beta(h))      # gating scalar, (s, encoder_dim)\n",
    "        attention_weighted_encoding = gate * attention_weighted_encoding\n",
    "\n",
    "        h, c = decoder.decode_step(\n",
    "            torch.cat([embeddings, attention_weighted_encoding], dim=1), (h, c)\n",
    "        )   # (s, decoder_dim)\n",
    "\n",
    "        scores = decoder.fc(h)      # (s, vocab_size)\n",
    "        scores = F.log_softmax(scores, dim=1)\n",
    "\n",
    "        # add the scores to prev scores\n",
    "        scores = top_k_scores.expand_as(scores) + scores    # (s, vocab_size)\n",
    "\n",
    "        # all the k points will have the same score for the first step (since same k previous words, h, c)\n",
    "        if step == 1:\n",
    "            top_k_scores, top_k_words = scores[0].topk(k, 0, True, True)    # (s)\n",
    "        else:\n",
    "            # unroll and find top scores, and their unrolled indices\n",
    "            top_k_scores, top_k_words = scores.view(-1).topk(k, 0, True, True)    # (s)\n",
    "        \n",
    "        # convert unrolled indices to actual indices of scores\n",
    "        prev_word_inds = top_k_words // vocab_size  # (s)\n",
    "        next_word_inds = top_k_words % vocab_size  # (s)\n",
    "\n",
    "        # add new words to sequences, alphas\n",
    "        top_k_seqs = torch.cat([top_k_seqs[prev_word_inds], next_word_inds.unsqueeze(1)], dim=1)    # (s, step + 1)\n",
    "        top_k_seqs_alpha = torch.cat(\n",
    "            [top_k_seqs_alpha[prev_word_inds], alpha[prev_word_inds].unsqueeze(1)], dim=1\n",
    "        )   # (s, step + 1, enc_image_size, enc_image_size)\n",
    "\n",
    "        # which sequences are incomplete (didn't reach <end>)?\n",
    "        incomplete_inds = [ind for ind, next_word in enumerate(next_word_inds) \n",
    "                            if next_word != word_map['<end>']]\n",
    "        complete_inds = list(set(range(len(next_word_inds))) - set(incomplete_inds))\n",
    "#         print([complete_inds], top_k_seqs_alpha[complete_inds].shape, top_k_seqs_alpha.shape)\n",
    "        # set aside complete sequences\n",
    "        if len(complete_inds) > 0:\n",
    "            complete_seqs.extend(top_k_seqs[complete_inds].tolist())\n",
    "            complete_seqs_alpha.extend(top_k_seqs_alpha[complete_inds].tolist())\n",
    "            complete_seqs_scores.extend(top_k_scores[complete_inds])\n",
    "        k -= len(complete_inds)  # reduce beam length accordingly\n",
    "\n",
    "        # proceed with incomplete sequences\n",
    "        if k == 0:\n",
    "            break\n",
    "        \n",
    "        top_k_seqs = top_k_seqs[incomplete_inds]\n",
    "        top_k_seqs_alpha = top_k_seqs_alpha[incomplete_inds]\n",
    "        h = h[prev_word_inds[incomplete_inds]]\n",
    "        c = c[prev_word_inds[incomplete_inds]]\n",
    "        encoder_out = encoder_out[prev_word_inds[incomplete_inds]]\n",
    "        top_k_scores = top_k_scores[incomplete_inds].unsqueeze(1)\n",
    "        top_k_prev_words = next_word_inds[incomplete_inds].unsqueeze(1)\n",
    "\n",
    "        # break if things have been going on too long\n",
    "        if step > 50:\n",
    "            break\n",
    "        step += 1\n",
    "    \n",
    "    # select sequence with max score\n",
    "#     print(complete_seqs, complete_seqs_scores)\n",
    "    if len(complete_seqs_scores):\n",
    "        i = complete_seqs_scores.index(max(complete_seqs_scores))\n",
    "        seq = complete_seqs[i]\n",
    "        caption = [rev_word_map[ind] for ind in seq]\n",
    "    else:\n",
    "        caption = \"\"\n",
    "    \n",
    "    return caption\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f43b169f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> turn to your one go straight three meters stop road building right your  building front on building right your  road <end>'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(generate_image_goal_caption(encoder, decoder, \"./rgb/000000016965.jpg\", (0,0), word2id, beam_size=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259a280b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
