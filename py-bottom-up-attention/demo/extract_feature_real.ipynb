{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "058cac82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import csv\n",
    "import base64\n",
    "import detectron2\n",
    "import glob\n",
    "# import some common detectron2 utilities\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog\n",
    "\n",
    "# import some common libraries\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "\n",
    "# Show the image in ipynb\n",
    "from IPython.display import clear_output, Image, display\n",
    "import PIL.Image\n",
    "def showarray(a, fmt='jpeg'):\n",
    "    a = np.uint8(np.clip(a, 0, 255))\n",
    "    f = io.BytesIO()\n",
    "    PIL.Image.fromarray(a).save(f, fmt)\n",
    "    display(Image(data=f.getvalue()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9f3a2b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/data2/zhongkai/VIP/real_world_dataset/random_split/train/image_map.json\", 'r') as f:\n",
    "    maps = json.load(f)\n",
    "PATH = \"/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "65948dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.modeling.postprocessing import detector_postprocess\n",
    "from detectron2.modeling.roi_heads.fast_rcnn import FastRCNNOutputLayers, FastRCNNOutputs, fast_rcnn_inference_single_image\n",
    "\n",
    "def doit(raw_image, NUM_OBJECTS):\n",
    "    with torch.no_grad():\n",
    "        raw_height, raw_width = raw_image.shape[:2]\n",
    "        print(\"Original image size: \", (raw_height, raw_width))\n",
    "        \n",
    "        # Preprocessing\n",
    "        image = predictor.transform_gen.get_transform(raw_image).apply_image(raw_image)\n",
    "        print(\"Transformed image size: \", image.shape[:2])\n",
    "        image = torch.as_tensor(image.astype(\"float32\").transpose(2, 0, 1))\n",
    "        inputs = [{\"image\": image, \"height\": raw_height, \"width\": raw_width}]\n",
    "        images = predictor.model.preprocess_image(inputs)\n",
    "        \n",
    "        # Run Backbone Res1-Res4\n",
    "        features = predictor.model.backbone(images.tensor)\n",
    "        \n",
    "        # Generate proposals with RPN\n",
    "        proposals, _ = predictor.model.proposal_generator(images, features, None)\n",
    "        proposal = proposals[0]\n",
    "        print('Proposal Boxes size:', proposal.proposal_boxes.tensor.shape)\n",
    "        \n",
    "        # Run RoI head for each proposal (RoI Pooling + Res5)\n",
    "        proposal_boxes = [x.proposal_boxes for x in proposals]\n",
    "        features = [features[f] for f in predictor.model.roi_heads.in_features]\n",
    "        box_features = predictor.model.roi_heads._shared_roi_transform(\n",
    "            features, proposal_boxes\n",
    "        )\n",
    "        feature_pooled = box_features.mean(dim=[2, 3])  # pooled to 1x1\n",
    "        print('Pooled features size:', feature_pooled.shape)\n",
    "        \n",
    "        # Predict classes and boxes for each proposal.\n",
    "        pred_class_logits, pred_attr_logits, pred_proposal_deltas = predictor.model.roi_heads.box_predictor(feature_pooled)\n",
    "        outputs = FastRCNNOutputs(\n",
    "            predictor.model.roi_heads.box2box_transform,\n",
    "            pred_class_logits,\n",
    "            pred_proposal_deltas,\n",
    "            proposals,\n",
    "            predictor.model.roi_heads.smooth_l1_beta,\n",
    "        )\n",
    "        probs = outputs.predict_probs()[0]\n",
    "        boxes = outputs.predict_boxes()[0]\n",
    "        \n",
    "        attr_prob = pred_attr_logits[..., :-1].softmax(-1)\n",
    "        max_attr_prob, max_attr_label = attr_prob.max(-1)\n",
    "        \n",
    "        # Note: BUTD uses raw RoI predictions,\n",
    "        #       we use the predicted boxes instead.\n",
    "        # boxes = proposal_boxes[0].tensor    \n",
    "        \n",
    "        # NMS\n",
    "        for nms_thresh in np.arange(0.5, 1.0, 0.1):\n",
    "            instances, ids = fast_rcnn_inference_single_image(\n",
    "                boxes, probs, image.shape[1:], \n",
    "                score_thresh=0.2, nms_thresh=nms_thresh, topk_per_image=NUM_OBJECTS\n",
    "            )\n",
    "            if len(ids) == NUM_OBJECTS:\n",
    "                break\n",
    "                \n",
    "        instances = detector_postprocess(instances, raw_height, raw_width)\n",
    "        roi_features = feature_pooled[ids].detach()\n",
    "        max_attr_prob = max_attr_prob[ids].detach()\n",
    "        max_attr_label = max_attr_label[ids].detach()\n",
    "        instances.attr_scores = max_attr_prob\n",
    "        instances.attr_classes = max_attr_label\n",
    "        \n",
    "        print(instances)\n",
    "        \n",
    "        return instances, roi_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5fe080d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load VG Classes\n",
    "data_path = 'data/genome/1600-400-20'\n",
    "\n",
    "vg_classes = []\n",
    "with open(os.path.join(data_path, 'objects_vocab.txt')) as f:\n",
    "    for object in f.readlines():\n",
    "        vg_classes.append(object.split(',')[0].lower().strip())\n",
    "\n",
    "MetadataCatalog.get(\"vg\").thing_classes = vg_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c4d5b296",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config '../configs/VG-Detection/faster_rcnn_R_101_C4_attr_caffemaxpool.yaml' has no VERSION. Assuming it to be compatible with latest v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modifications for VG in ResNet Backbone (modeling/backbone/resnet.py):\n",
      "\tUsing pad 0 in stem max_pool instead of pad 1.\n",
      "\n",
      "Modifications for VG in RPN (modeling/proposal_generator/rpn.py):\n",
      "\tUse hidden dim 512 instead fo the same dim as Res4 (1024).\n",
      "\n",
      "Modifications for VG in RoI heads (modeling/roi_heads/roi_heads.py):\n",
      "\t1. Change the stride of conv1 and shortcut in Res5.Block1 from 2 to 1.\n",
      "\t2. Modifying all conv2 with (padding: 1 --> 2) and (dilation: 1 --> 2).\n",
      "\tFor more details, please check 'https://github.com/peteanderson80/bottom-up-attention/blob/master/models/vg/ResNet-101/faster_rcnn_end2end_final/test.prototxt'.\n",
      "\n",
      "Modifications for VG in RoI heads (modeling/roi_heads/fast_rcnn.py))\n",
      "\tEmbedding: 1601 --> 256\tLinear: 2304 --> 512\tLinear: 512 --> 401\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(\"../configs/VG-Detection/faster_rcnn_R_101_C4_attr_caffemaxpool.yaml\")\n",
    "cfg.MODEL.RPN.POST_NMS_TOPK_TEST = 300\n",
    "cfg.MODEL.ROI_HEADS.NMS_THRESH_TEST = 0.6\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.2\n",
    "cfg.MODEL.DEVICE = 'cuda:1'\n",
    "# VG Weight\n",
    "cfg.MODEL.WEIGHTS = \"http://nlp.cs.unc.edu/models/faster_rcnn_from_caffe_attr.pkl\"\n",
    "predictor = DefaultPredictor(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d211ff74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_feature(image, ids, num_objects=30):\n",
    "    instances, features = doit(image, num_objects)\n",
    "\n",
    "    boxes = instances.pred_boxes.tensor.cpu().numpy()\n",
    "    image_height, image_width = instances.image_size\n",
    "    feature = features.cpu().numpy()\n",
    "\n",
    "    box_width = boxes[:, 2] - boxes[:, 0]\n",
    "    box_height = boxes[:, 3] - boxes[:, 1]\n",
    "    scaled_width = box_width/image_width\n",
    "    scaled_height = box_height/image_height\n",
    "    scaled_x = boxes[:, 0]/image_width\n",
    "    scaled_y = boxes[:, 1] / image_height\n",
    "    scaled_width = scaled_width[..., np.newaxis]\n",
    "    scaled_height = scaled_height[..., np.newaxis]\n",
    "    scaled_x = scaled_x[..., np.newaxis]\n",
    "    scaled_y = scaled_y[..., np.newaxis]\n",
    "\n",
    "    spatial_features = np.concatenate((scaled_x, scaled_y, scaled_x+scaled_width, scaled_y+scaled_height, scaled_width, scaled_height), axis=1)\n",
    "\n",
    "    full_features = np.concatenate((feature, spatial_features), axis=1)\n",
    "    fea_base64 = base64.b64encode(full_features).decode('utf-8')\n",
    "    fea_info = {\"num_boxes\": boxes.shape[0], \"features\": fea_base64}\n",
    "\n",
    "    objects = instances.pred_classes.cpu().numpy()\n",
    "    conf = instances.scores.cpu().numpy()\n",
    "\n",
    "    labels = []\n",
    "    for i in range(len(boxes)):\n",
    "        labels.append({\n",
    "            \"class\": vg_classes[objects[i]],\n",
    "            \"rect\": list(boxes[i, :]), \n",
    "            \"conf\": conf[i]\n",
    "        })\n",
    "    return {'image_id': ids, 'images': fea_info}, {'image_id': ids, 'list':labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a38289",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2492a40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_lineidx_file(filein, idxout):\n",
    "    idxout_tmp = idxout + '.tmp'\n",
    "    with open(filein, 'r') as tsvin, open(idxout_tmp,'w') as tsvout:\n",
    "        fsize = os.fstat(tsvin.fileno()).st_size\n",
    "        fpos = 0\n",
    "        while fpos!=fsize:\n",
    "            tsvout.write(str(fpos)+\"\\n\")\n",
    "            tsvin.readline()\n",
    "            fpos = tsvin.tell()\n",
    "    os.rename(idxout_tmp, idxout)\n",
    "\n",
    "def generate_tsv(feature_path=\"../test/train.feature.tsv\", label_path=\"../test/train.label.tsv\"):\n",
    "    with open(feature_path, 'a') as tsvfile, open(label_path, 'a') as labeltsvfile:\n",
    "        writer = csv.DictWriter(tsvfile, delimiter='\\t', fieldnames=['image_id', 'images'])\n",
    "        Labelwriter=csv.DictWriter(labeltsvfile, delimiter='\\t', fieldnames=['image_id', 'list'])\n",
    "        for img_path in glob.glob(f\"{PATH}/*.jpg\"):\n",
    "            print(img_path)\n",
    "            im = cv2.imread(img_path)\n",
    "            im_rgb = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "            index = img_path.split('/')[-1].split('.')[0]\n",
    "            print(index)\n",
    "            rst = generate_feature(im_rgb, ids=int(index))\n",
    "\n",
    "            writer.writerow(rst[0])\n",
    "            Labelwriter.writerow(rst[1])\n",
    "    generate_lineidx_file(feature_path, feature_path.replace(\"tsv\", \"lineidx\"))\n",
    "    generate_lineidx_file(label_path, label_path.replace(\"tsv\", \"lineidx\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "220e5ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove '../test/train*.tsv': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!rm ../test/train*.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "35122147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1177.jpg\n",
      "1177\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([177, 4])\n",
      "Pooled features size: torch.Size([177, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/193.jpg\n",
      "193\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([134, 4])\n",
      "Pooled features size: torch.Size([134, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1678.jpg\n",
      "1678\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([143, 4])\n",
      "Pooled features size: torch.Size([143, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1188.jpg\n",
      "1188\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([134, 4])\n",
      "Pooled features size: torch.Size([134, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/725.jpg\n",
      "725\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([132, 4])\n",
      "Pooled features size: torch.Size([132, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1530.jpg\n",
      "1530\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([151, 4])\n",
      "Pooled features size: torch.Size([151, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1457.jpg\n",
      "1457\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([152, 4])\n",
      "Pooled features size: torch.Size([152, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/347.jpg\n",
      "347\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([125, 4])\n",
      "Pooled features size: torch.Size([125, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1455.jpg\n",
      "1455\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([136, 4])\n",
      "Pooled features size: torch.Size([136, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1189.jpg\n",
      "1189\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([148, 4])\n",
      "Pooled features size: torch.Size([148, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/302.jpg\n",
      "302\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([138, 4])\n",
      "Pooled features size: torch.Size([138, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1508.jpg\n",
      "1508\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([132, 4])\n",
      "Pooled features size: torch.Size([132, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/85.jpg\n",
      "85\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([144, 4])\n",
      "Pooled features size: torch.Size([144, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1863.jpg\n",
      "1863\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([158, 4])\n",
      "Pooled features size: torch.Size([158, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1871.jpg\n",
      "1871\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([122, 4])\n",
      "Pooled features size: torch.Size([122, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1669.jpg\n",
      "1669\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([131, 4])\n",
      "Pooled features size: torch.Size([131, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1872.jpg\n",
      "1872\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([141, 4])\n",
      "Pooled features size: torch.Size([141, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/265.jpg\n",
      "265\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([145, 4])\n",
      "Pooled features size: torch.Size([145, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1910.jpg\n",
      "1910\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([127, 4])\n",
      "Pooled features size: torch.Size([127, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/671.jpg\n",
      "671\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([137, 4])\n",
      "Pooled features size: torch.Size([137, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/683.jpg\n",
      "683\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([120, 4])\n",
      "Pooled features size: torch.Size([120, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/712.jpg\n",
      "712\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([150, 4])\n",
      "Pooled features size: torch.Size([150, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/815.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "815\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([150, 4])\n",
      "Pooled features size: torch.Size([150, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/212.jpg\n",
      "212\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([143, 4])\n",
      "Pooled features size: torch.Size([143, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/835.jpg\n",
      "835\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([132, 4])\n",
      "Pooled features size: torch.Size([132, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/587.jpg\n",
      "587\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([164, 4])\n",
      "Pooled features size: torch.Size([164, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/660.jpg\n",
      "660\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([128, 4])\n",
      "Pooled features size: torch.Size([128, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1733.jpg\n",
      "1733\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([129, 4])\n",
      "Pooled features size: torch.Size([129, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/144.jpg\n",
      "144\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([135, 4])\n",
      "Pooled features size: torch.Size([135, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/614.jpg\n",
      "614\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([150, 4])\n",
      "Pooled features size: torch.Size([150, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/829.jpg\n",
      "829\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([135, 4])\n",
      "Pooled features size: torch.Size([135, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1886.jpg\n",
      "1886\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([161, 4])\n",
      "Pooled features size: torch.Size([161, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1560.jpg\n",
      "1560\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([130, 4])\n",
      "Pooled features size: torch.Size([130, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1854.jpg\n",
      "1854\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([143, 4])\n",
      "Pooled features size: torch.Size([143, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/111.jpg\n",
      "111\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([149, 4])\n",
      "Pooled features size: torch.Size([149, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1571.jpg\n",
      "1571\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([132, 4])\n",
      "Pooled features size: torch.Size([132, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/869.jpg\n",
      "869\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([140, 4])\n",
      "Pooled features size: torch.Size([140, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1125.jpg\n",
      "1125\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([139, 4])\n",
      "Pooled features size: torch.Size([139, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/14.jpg\n",
      "14\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([127, 4])\n",
      "Pooled features size: torch.Size([127, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/263.jpg\n",
      "263\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([132, 4])\n",
      "Pooled features size: torch.Size([132, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1692.jpg\n",
      "1692\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([141, 4])\n",
      "Pooled features size: torch.Size([141, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/250.jpg\n",
      "250\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([125, 4])\n",
      "Pooled features size: torch.Size([125, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/439.jpg\n",
      "439\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([148, 4])\n",
      "Pooled features size: torch.Size([148, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1628.jpg\n",
      "1628\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([130, 4])\n",
      "Pooled features size: torch.Size([130, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/316.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "316\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([144, 4])\n",
      "Pooled features size: torch.Size([144, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1248.jpg\n",
      "1248\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([161, 4])\n",
      "Pooled features size: torch.Size([161, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1209.jpg\n",
      "1209\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([124, 4])\n",
      "Pooled features size: torch.Size([124, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/510.jpg\n",
      "510\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([120, 4])\n",
      "Pooled features size: torch.Size([120, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1440.jpg\n",
      "1440\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([139, 4])\n",
      "Pooled features size: torch.Size([139, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/558.jpg\n",
      "558\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([125, 4])\n",
      "Pooled features size: torch.Size([125, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/321.jpg\n",
      "321\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([130, 4])\n",
      "Pooled features size: torch.Size([130, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1558.jpg\n",
      "1558\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([138, 4])\n",
      "Pooled features size: torch.Size([138, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/950.jpg\n",
      "950\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([122, 4])\n",
      "Pooled features size: torch.Size([122, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/258.jpg\n",
      "258\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([140, 4])\n",
      "Pooled features size: torch.Size([140, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1046.jpg\n",
      "1046\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([129, 4])\n",
      "Pooled features size: torch.Size([129, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/601.jpg\n",
      "601\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([140, 4])\n",
      "Pooled features size: torch.Size([140, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/269.jpg\n",
      "269\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([131, 4])\n",
      "Pooled features size: torch.Size([131, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/622.jpg\n",
      "622\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([129, 4])\n",
      "Pooled features size: torch.Size([129, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1725.jpg\n",
      "1725\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([165, 4])\n",
      "Pooled features size: torch.Size([165, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1139.jpg\n",
      "1139\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([152, 4])\n",
      "Pooled features size: torch.Size([152, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1069.jpg\n",
      "1069\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([136, 4])\n",
      "Pooled features size: torch.Size([136, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/185.jpg\n",
      "185\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([119, 4])\n",
      "Pooled features size: torch.Size([119, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/781.jpg\n",
      "781\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([140, 4])\n",
      "Pooled features size: torch.Size([140, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/474.jpg\n",
      "474\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([144, 4])\n",
      "Pooled features size: torch.Size([144, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/972.jpg\n",
      "972\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([118, 4])\n",
      "Pooled features size: torch.Size([118, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1587.jpg\n",
      "1587\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([147, 4])\n",
      "Pooled features size: torch.Size([147, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/331.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "331\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([143, 4])\n",
      "Pooled features size: torch.Size([143, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1657.jpg\n",
      "1657\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([165, 4])\n",
      "Pooled features size: torch.Size([165, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1210.jpg\n",
      "1210\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([132, 4])\n",
      "Pooled features size: torch.Size([132, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1004.jpg\n",
      "1004\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([144, 4])\n",
      "Pooled features size: torch.Size([144, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/201.jpg\n",
      "201\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([137, 4])\n",
      "Pooled features size: torch.Size([137, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1404.jpg\n",
      "1404\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([136, 4])\n",
      "Pooled features size: torch.Size([136, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1341.jpg\n",
      "1341\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([136, 4])\n",
      "Pooled features size: torch.Size([136, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/861.jpg\n",
      "861\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([157, 4])\n",
      "Pooled features size: torch.Size([157, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/353.jpg\n",
      "353\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([157, 4])\n",
      "Pooled features size: torch.Size([157, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/554.jpg\n",
      "554\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([145, 4])\n",
      "Pooled features size: torch.Size([145, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/168.jpg\n",
      "168\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([135, 4])\n",
      "Pooled features size: torch.Size([135, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1067.jpg\n",
      "1067\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([113, 4])\n",
      "Pooled features size: torch.Size([113, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1570.jpg\n",
      "1570\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([128, 4])\n",
      "Pooled features size: torch.Size([128, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/115.jpg\n",
      "115\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([144, 4])\n",
      "Pooled features size: torch.Size([144, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1763.jpg\n",
      "1763\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([131, 4])\n",
      "Pooled features size: torch.Size([131, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/502.jpg\n",
      "502\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([133, 4])\n",
      "Pooled features size: torch.Size([133, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1287.jpg\n",
      "1287\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([128, 4])\n",
      "Pooled features size: torch.Size([128, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1344.jpg\n",
      "1344\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([138, 4])\n",
      "Pooled features size: torch.Size([138, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/372.jpg\n",
      "372\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([154, 4])\n",
      "Pooled features size: torch.Size([154, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1500.jpg\n",
      "1500\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([137, 4])\n",
      "Pooled features size: torch.Size([137, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/285.jpg\n",
      "285\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([159, 4])\n",
      "Pooled features size: torch.Size([159, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/408.jpg\n",
      "408\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([148, 4])\n",
      "Pooled features size: torch.Size([148, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1099.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1099\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([155, 4])\n",
      "Pooled features size: torch.Size([155, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1494.jpg\n",
      "1494\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([117, 4])\n",
      "Pooled features size: torch.Size([117, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1732.jpg\n",
      "1732\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([137, 4])\n",
      "Pooled features size: torch.Size([137, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1038.jpg\n",
      "1038\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([134, 4])\n",
      "Pooled features size: torch.Size([134, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1176.jpg\n",
      "1176\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([155, 4])\n",
      "Pooled features size: torch.Size([155, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1605.jpg\n",
      "1605\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([151, 4])\n",
      "Pooled features size: torch.Size([151, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1524.jpg\n",
      "1524\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([140, 4])\n",
      "Pooled features size: torch.Size([140, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/808.jpg\n",
      "808\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([130, 4])\n",
      "Pooled features size: torch.Size([130, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/998.jpg\n",
      "998\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([138, 4])\n",
      "Pooled features size: torch.Size([138, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1620.jpg\n",
      "1620\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([126, 4])\n",
      "Pooled features size: torch.Size([126, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/759.jpg\n",
      "759\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([144, 4])\n",
      "Pooled features size: torch.Size([144, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1042.jpg\n",
      "1042\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([133, 4])\n",
      "Pooled features size: torch.Size([133, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/511.jpg\n",
      "511\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([155, 4])\n",
      "Pooled features size: torch.Size([155, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/322.jpg\n",
      "322\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([131, 4])\n",
      "Pooled features size: torch.Size([131, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1144.jpg\n",
      "1144\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([120, 4])\n",
      "Pooled features size: torch.Size([120, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1623.jpg\n",
      "1623\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([139, 4])\n",
      "Pooled features size: torch.Size([139, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/378.jpg\n",
      "378\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([144, 4])\n",
      "Pooled features size: torch.Size([144, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/629.jpg\n",
      "629\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([141, 4])\n",
      "Pooled features size: torch.Size([141, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1066.jpg\n",
      "1066\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([128, 4])\n",
      "Pooled features size: torch.Size([128, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1299.jpg\n",
      "1299\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([141, 4])\n",
      "Pooled features size: torch.Size([141, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1821.jpg\n",
      "1821\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([118, 4])\n",
      "Pooled features size: torch.Size([118, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/909.jpg\n",
      "909\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([174, 4])\n",
      "Pooled features size: torch.Size([174, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1444.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1444\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([123, 4])\n",
      "Pooled features size: torch.Size([123, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1262.jpg\n",
      "1262\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([132, 4])\n",
      "Pooled features size: torch.Size([132, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1043.jpg\n",
      "1043\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([129, 4])\n",
      "Pooled features size: torch.Size([129, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/831.jpg\n",
      "831\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([151, 4])\n",
      "Pooled features size: torch.Size([151, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/365.jpg\n",
      "365\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([129, 4])\n",
      "Pooled features size: torch.Size([129, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/274.jpg\n",
      "274\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([129, 4])\n",
      "Pooled features size: torch.Size([129, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/37.jpg\n",
      "37\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([130, 4])\n",
      "Pooled features size: torch.Size([130, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1556.jpg\n",
      "1556\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([138, 4])\n",
      "Pooled features size: torch.Size([138, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1713.jpg\n",
      "1713\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([159, 4])\n",
      "Pooled features size: torch.Size([159, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1135.jpg\n",
      "1135\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([136, 4])\n",
      "Pooled features size: torch.Size([136, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1184.jpg\n",
      "1184\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([134, 4])\n",
      "Pooled features size: torch.Size([134, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1118.jpg\n",
      "1118\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([147, 4])\n",
      "Pooled features size: torch.Size([147, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/994.jpg\n",
      "994\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([125, 4])\n",
      "Pooled features size: torch.Size([125, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/418.jpg\n",
      "418\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([111, 4])\n",
      "Pooled features size: torch.Size([111, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/690.jpg\n",
      "690\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([142, 4])\n",
      "Pooled features size: torch.Size([142, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/438.jpg\n",
      "438\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([132, 4])\n",
      "Pooled features size: torch.Size([132, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/492.jpg\n",
      "492\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([140, 4])\n",
      "Pooled features size: torch.Size([140, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/744.jpg\n",
      "744\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([131, 4])\n",
      "Pooled features size: torch.Size([131, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1720.jpg\n",
      "1720\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([149, 4])\n",
      "Pooled features size: torch.Size([149, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/460.jpg\n",
      "460\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([131, 4])\n",
      "Pooled features size: torch.Size([131, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/917.jpg\n",
      "917\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([162, 4])\n",
      "Pooled features size: torch.Size([162, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/982.jpg\n",
      "982\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([118, 4])\n",
      "Pooled features size: torch.Size([118, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1913.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1913\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([124, 4])\n",
      "Pooled features size: torch.Size([124, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1410.jpg\n",
      "1410\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([117, 4])\n",
      "Pooled features size: torch.Size([117, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1253.jpg\n",
      "1253\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([125, 4])\n",
      "Pooled features size: torch.Size([125, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1111.jpg\n",
      "1111\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([132, 4])\n",
      "Pooled features size: torch.Size([132, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/377.jpg\n",
      "377\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([137, 4])\n",
      "Pooled features size: torch.Size([137, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1566.jpg\n",
      "1566\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([131, 4])\n",
      "Pooled features size: torch.Size([131, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/187.jpg\n",
      "187\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([135, 4])\n",
      "Pooled features size: torch.Size([135, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/204.jpg\n",
      "204\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([124, 4])\n",
      "Pooled features size: torch.Size([124, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1635.jpg\n",
      "1635\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([149, 4])\n",
      "Pooled features size: torch.Size([149, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1904.jpg\n",
      "1904\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([121, 4])\n",
      "Pooled features size: torch.Size([121, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1764.jpg\n",
      "1764\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([130, 4])\n",
      "Pooled features size: torch.Size([130, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1256.jpg\n",
      "1256\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([133, 4])\n",
      "Pooled features size: torch.Size([133, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/945.jpg\n",
      "945\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([130, 4])\n",
      "Pooled features size: torch.Size([130, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/491.jpg\n",
      "491\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([157, 4])\n",
      "Pooled features size: torch.Size([157, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1168.jpg\n",
      "1168\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([162, 4])\n",
      "Pooled features size: torch.Size([162, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/534.jpg\n",
      "534\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([134, 4])\n",
      "Pooled features size: torch.Size([134, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1394.jpg\n",
      "1394\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([158, 4])\n",
      "Pooled features size: torch.Size([158, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1700.jpg\n",
      "1700\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([140, 4])\n",
      "Pooled features size: torch.Size([140, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1089.jpg\n",
      "1089\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([145, 4])\n",
      "Pooled features size: torch.Size([145, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1216.jpg\n",
      "1216\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([130, 4])\n",
      "Pooled features size: torch.Size([130, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/711.jpg\n",
      "711\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([130, 4])\n",
      "Pooled features size: torch.Size([130, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1300.jpg\n",
      "1300\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([127, 4])\n",
      "Pooled features size: torch.Size([127, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1065.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1065\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([132, 4])\n",
      "Pooled features size: torch.Size([132, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1058.jpg\n",
      "1058\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([136, 4])\n",
      "Pooled features size: torch.Size([136, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/734.jpg\n",
      "734\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([142, 4])\n",
      "Pooled features size: torch.Size([142, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1693.jpg\n",
      "1693\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([143, 4])\n",
      "Pooled features size: torch.Size([143, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/391.jpg\n",
      "391\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([126, 4])\n",
      "Pooled features size: torch.Size([126, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/479.jpg\n",
      "479\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([144, 4])\n",
      "Pooled features size: torch.Size([144, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1230.jpg\n",
      "1230\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([125, 4])\n",
      "Pooled features size: torch.Size([125, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/110.jpg\n",
      "110\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([117, 4])\n",
      "Pooled features size: torch.Size([117, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1130.jpg\n",
      "1130\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([134, 4])\n",
      "Pooled features size: torch.Size([134, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/132.jpg\n",
      "132\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([135, 4])\n",
      "Pooled features size: torch.Size([135, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/298.jpg\n",
      "298\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([131, 4])\n",
      "Pooled features size: torch.Size([131, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1480.jpg\n",
      "1480\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([144, 4])\n",
      "Pooled features size: torch.Size([144, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/842.jpg\n",
      "842\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([130, 4])\n",
      "Pooled features size: torch.Size([130, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/24.jpg\n",
      "24\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([137, 4])\n",
      "Pooled features size: torch.Size([137, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1264.jpg\n",
      "1264\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([120, 4])\n",
      "Pooled features size: torch.Size([120, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/528.jpg\n",
      "528\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([167, 4])\n",
      "Pooled features size: torch.Size([167, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1365.jpg\n",
      "1365\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([135, 4])\n",
      "Pooled features size: torch.Size([135, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/117.jpg\n",
      "117\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([140, 4])\n",
      "Pooled features size: torch.Size([140, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1606.jpg\n",
      "1606\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([155, 4])\n",
      "Pooled features size: torch.Size([155, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/290.jpg\n",
      "290\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([157, 4])\n",
      "Pooled features size: torch.Size([157, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/451.jpg\n",
      "451\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([139, 4])\n",
      "Pooled features size: torch.Size([139, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/278.jpg\n",
      "278\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([146, 4])\n",
      "Pooled features size: torch.Size([146, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1091.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1091\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([154, 4])\n",
      "Pooled features size: torch.Size([154, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/197.jpg\n",
      "197\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([129, 4])\n",
      "Pooled features size: torch.Size([129, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/444.jpg\n",
      "444\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([126, 4])\n",
      "Pooled features size: torch.Size([126, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1767.jpg\n",
      "1767\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([137, 4])\n",
      "Pooled features size: torch.Size([137, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/453.jpg\n",
      "453\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([128, 4])\n",
      "Pooled features size: torch.Size([128, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/642.jpg\n",
      "642\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([143, 4])\n",
      "Pooled features size: torch.Size([143, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/172.jpg\n",
      "172\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([153, 4])\n",
      "Pooled features size: torch.Size([153, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1233.jpg\n",
      "1233\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([158, 4])\n",
      "Pooled features size: torch.Size([158, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/680.jpg\n",
      "680\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([140, 4])\n",
      "Pooled features size: torch.Size([140, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/752.jpg\n",
      "752\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([121, 4])\n",
      "Pooled features size: torch.Size([121, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/702.jpg\n",
      "702\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([123, 4])\n",
      "Pooled features size: torch.Size([123, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1578.jpg\n",
      "1578\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([114, 4])\n",
      "Pooled features size: torch.Size([114, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/991.jpg\n",
      "991\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([139, 4])\n",
      "Pooled features size: torch.Size([139, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1259.jpg\n",
      "1259\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([120, 4])\n",
      "Pooled features size: torch.Size([120, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/812.jpg\n",
      "812\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([120, 4])\n",
      "Pooled features size: torch.Size([120, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/802.jpg\n",
      "802\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([166, 4])\n",
      "Pooled features size: torch.Size([166, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/765.jpg\n",
      "765\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([139, 4])\n",
      "Pooled features size: torch.Size([139, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1054.jpg\n",
      "1054\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([146, 4])\n",
      "Pooled features size: torch.Size([146, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1220.jpg\n",
      "1220\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([123, 4])\n",
      "Pooled features size: torch.Size([123, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1561.jpg\n",
      "1561\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([124, 4])\n",
      "Pooled features size: torch.Size([124, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/392.jpg\n",
      "392\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([118, 4])\n",
      "Pooled features size: torch.Size([118, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1231.jpg\n",
      "1231\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([165, 4])\n",
      "Pooled features size: torch.Size([165, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1037.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1037\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([134, 4])\n",
      "Pooled features size: torch.Size([134, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1148.jpg\n",
      "1148\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([137, 4])\n",
      "Pooled features size: torch.Size([137, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1128.jpg\n",
      "1128\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([133, 4])\n",
      "Pooled features size: torch.Size([133, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1484.jpg\n",
      "1484\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([125, 4])\n",
      "Pooled features size: torch.Size([125, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/213.jpg\n",
      "213\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([151, 4])\n",
      "Pooled features size: torch.Size([151, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/164.jpg\n",
      "164\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([130, 4])\n",
      "Pooled features size: torch.Size([130, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1612.jpg\n",
      "1612\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([139, 4])\n",
      "Pooled features size: torch.Size([139, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1909.jpg\n",
      "1909\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([153, 4])\n",
      "Pooled features size: torch.Size([153, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1675.jpg\n",
      "1675\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([134, 4])\n",
      "Pooled features size: torch.Size([134, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/965.jpg\n",
      "965\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([131, 4])\n",
      "Pooled features size: torch.Size([131, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/931.jpg\n",
      "931\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([133, 4])\n",
      "Pooled features size: torch.Size([133, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/708.jpg\n",
      "708\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([115, 4])\n",
      "Pooled features size: torch.Size([115, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1619.jpg\n",
      "1619\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([148, 4])\n",
      "Pooled features size: torch.Size([148, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1363.jpg\n",
      "1363\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([149, 4])\n",
      "Pooled features size: torch.Size([149, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1171.jpg\n",
      "1171\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([145, 4])\n",
      "Pooled features size: torch.Size([145, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1086.jpg\n",
      "1086\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([119, 4])\n",
      "Pooled features size: torch.Size([119, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/254.jpg\n",
      "254\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([133, 4])\n",
      "Pooled features size: torch.Size([133, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1899.jpg\n",
      "1899\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([120, 4])\n",
      "Pooled features size: torch.Size([120, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1719.jpg\n",
      "1719\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([154, 4])\n",
      "Pooled features size: torch.Size([154, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/729.jpg\n",
      "729\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([131, 4])\n",
      "Pooled features size: torch.Size([131, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/89.jpg\n",
      "89\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([126, 4])\n",
      "Pooled features size: torch.Size([126, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1541.jpg\n",
      "1541\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([114, 4])\n",
      "Pooled features size: torch.Size([114, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/260.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "260\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([153, 4])\n",
      "Pooled features size: torch.Size([153, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1607.jpg\n",
      "1607\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([143, 4])\n",
      "Pooled features size: torch.Size([143, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/504.jpg\n",
      "504\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([131, 4])\n",
      "Pooled features size: torch.Size([131, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/698.jpg\n",
      "698\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([172, 4])\n",
      "Pooled features size: torch.Size([172, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/858.jpg\n",
      "858\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([144, 4])\n",
      "Pooled features size: torch.Size([144, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1534.jpg\n",
      "1534\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([132, 4])\n",
      "Pooled features size: torch.Size([132, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/413.jpg\n",
      "413\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([133, 4])\n",
      "Pooled features size: torch.Size([133, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1386.jpg\n",
      "1386\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([132, 4])\n",
      "Pooled features size: torch.Size([132, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1227.jpg\n",
      "1227\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([131, 4])\n",
      "Pooled features size: torch.Size([131, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1456.jpg\n",
      "1456\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([139, 4])\n",
      "Pooled features size: torch.Size([139, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/867.jpg\n",
      "867\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([160, 4])\n",
      "Pooled features size: torch.Size([160, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1098.jpg\n",
      "1098\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([123, 4])\n",
      "Pooled features size: torch.Size([123, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/608.jpg\n",
      "608\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([139, 4])\n",
      "Pooled features size: torch.Size([139, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/714.jpg\n",
      "714\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([136, 4])\n",
      "Pooled features size: torch.Size([136, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/753.jpg\n",
      "753\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([125, 4])\n",
      "Pooled features size: torch.Size([125, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/942.jpg\n",
      "942\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([133, 4])\n",
      "Pooled features size: torch.Size([133, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1246.jpg\n",
      "1246\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([134, 4])\n",
      "Pooled features size: torch.Size([134, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/376.jpg\n",
      "376\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([153, 4])\n",
      "Pooled features size: torch.Size([153, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1437.jpg\n",
      "1437\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([138, 4])\n",
      "Pooled features size: torch.Size([138, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/78.jpg\n",
      "78\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([139, 4])\n",
      "Pooled features size: torch.Size([139, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/148.jpg\n",
      "148\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([129, 4])\n",
      "Pooled features size: torch.Size([129, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1814.jpg\n",
      "1814\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([174, 4])\n",
      "Pooled features size: torch.Size([174, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1351.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1351\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([180, 4])\n",
      "Pooled features size: torch.Size([180, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/522.jpg\n",
      "522\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([142, 4])\n",
      "Pooled features size: torch.Size([142, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/36.jpg\n",
      "36\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([124, 4])\n",
      "Pooled features size: torch.Size([124, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/915.jpg\n",
      "915\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([163, 4])\n",
      "Pooled features size: torch.Size([163, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1582.jpg\n",
      "1582\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([152, 4])\n",
      "Pooled features size: torch.Size([152, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/88.jpg\n",
      "88\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([129, 4])\n",
      "Pooled features size: torch.Size([129, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/518.jpg\n",
      "518\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([128, 4])\n",
      "Pooled features size: torch.Size([128, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1250.jpg\n",
      "1250\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([139, 4])\n",
      "Pooled features size: torch.Size([139, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1110.jpg\n",
      "1110\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([123, 4])\n",
      "Pooled features size: torch.Size([123, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/223.jpg\n",
      "223\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([138, 4])\n",
      "Pooled features size: torch.Size([138, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1670.jpg\n",
      "1670\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([131, 4])\n",
      "Pooled features size: torch.Size([131, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/937.jpg\n",
      "937\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([138, 4])\n",
      "Pooled features size: torch.Size([138, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/899.jpg\n",
      "899\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([145, 4])\n",
      "Pooled features size: torch.Size([145, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1319.jpg\n",
      "1319\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([119, 4])\n",
      "Pooled features size: torch.Size([119, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1136.jpg\n",
      "1136\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([156, 4])\n",
      "Pooled features size: torch.Size([156, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/536.jpg\n",
      "536\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([130, 4])\n",
      "Pooled features size: torch.Size([130, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/850.jpg\n",
      "850\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([136, 4])\n",
      "Pooled features size: torch.Size([136, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1102.jpg\n",
      "1102\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([126, 4])\n",
      "Pooled features size: torch.Size([126, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1267.jpg\n",
      "1267\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([153, 4])\n",
      "Pooled features size: torch.Size([153, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1034.jpg\n",
      "1034\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([154, 4])\n",
      "Pooled features size: torch.Size([154, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/735.jpg\n",
      "735\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([130, 4])\n",
      "Pooled features size: torch.Size([130, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1829.jpg\n",
      "1829\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([131, 4])\n",
      "Pooled features size: torch.Size([131, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1175.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1175\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([144, 4])\n",
      "Pooled features size: torch.Size([144, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1868.jpg\n",
      "1868\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([125, 4])\n",
      "Pooled features size: torch.Size([125, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1514.jpg\n",
      "1514\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([158, 4])\n",
      "Pooled features size: torch.Size([158, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/754.jpg\n",
      "754\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([117, 4])\n",
      "Pooled features size: torch.Size([117, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/92.jpg\n",
      "92\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([162, 4])\n",
      "Pooled features size: torch.Size([162, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/505.jpg\n",
      "505\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([133, 4])\n",
      "Pooled features size: torch.Size([133, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1584.jpg\n",
      "1584\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([162, 4])\n",
      "Pooled features size: torch.Size([162, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/412.jpg\n",
      "412\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([134, 4])\n",
      "Pooled features size: torch.Size([134, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1000.jpg\n",
      "1000\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([141, 4])\n",
      "Pooled features size: torch.Size([141, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/30.jpg\n",
      "30\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([136, 4])\n",
      "Pooled features size: torch.Size([136, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1617.jpg\n",
      "1617\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([167, 4])\n",
      "Pooled features size: torch.Size([167, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/559.jpg\n",
      "559\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([136, 4])\n",
      "Pooled features size: torch.Size([136, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1151.jpg\n",
      "1151\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([131, 4])\n",
      "Pooled features size: torch.Size([131, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1451.jpg\n",
      "1451\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([146, 4])\n",
      "Pooled features size: torch.Size([146, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1785.jpg\n",
      "1785\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([141, 4])\n",
      "Pooled features size: torch.Size([141, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1398.jpg\n",
      "1398\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([125, 4])\n",
      "Pooled features size: torch.Size([125, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/701.jpg\n",
      "701\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([154, 4])\n",
      "Pooled features size: torch.Size([154, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/556.jpg\n",
      "556\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([121, 4])\n",
      "Pooled features size: torch.Size([121, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/46.jpg\n",
      "46\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([134, 4])\n",
      "Pooled features size: torch.Size([134, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/935.jpg\n",
      "935\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([121, 4])\n",
      "Pooled features size: torch.Size([121, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/496.jpg\n",
      "496\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([134, 4])\n",
      "Pooled features size: torch.Size([134, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/138.jpg\n",
      "138\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([124, 4])\n",
      "Pooled features size: torch.Size([124, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/517.jpg\n",
      "517\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([164, 4])\n",
      "Pooled features size: torch.Size([164, 2048])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/18.jpg\n",
      "18\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([144, 4])\n",
      "Pooled features size: torch.Size([144, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1793.jpg\n",
      "1793\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([143, 4])\n",
      "Pooled features size: torch.Size([143, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1285.jpg\n",
      "1285\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([126, 4])\n",
      "Pooled features size: torch.Size([126, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1825.jpg\n",
      "1825\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([122, 4])\n",
      "Pooled features size: torch.Size([122, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1342.jpg\n",
      "1342\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([131, 4])\n",
      "Pooled features size: torch.Size([131, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1018.jpg\n",
      "1018\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([132, 4])\n",
      "Pooled features size: torch.Size([132, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1211.jpg\n",
      "1211\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([156, 4])\n",
      "Pooled features size: torch.Size([156, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1800.jpg\n",
      "1800\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([130, 4])\n",
      "Pooled features size: torch.Size([130, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/594.jpg\n",
      "594\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([142, 4])\n",
      "Pooled features size: torch.Size([142, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1491.jpg\n",
      "1491\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([125, 4])\n",
      "Pooled features size: torch.Size([125, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/722.jpg\n",
      "722\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([153, 4])\n",
      "Pooled features size: torch.Size([153, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1867.jpg\n",
      "1867\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([117, 4])\n",
      "Pooled features size: torch.Size([117, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1896.jpg\n",
      "1896\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([131, 4])\n",
      "Pooled features size: torch.Size([131, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/820.jpg\n",
      "820\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([123, 4])\n",
      "Pooled features size: torch.Size([123, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/150.jpg\n",
      "150\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([144, 4])\n",
      "Pooled features size: torch.Size([144, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1900.jpg\n",
      "1900\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([132, 4])\n",
      "Pooled features size: torch.Size([132, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/889.jpg\n",
      "889\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([115, 4])\n",
      "Pooled features size: torch.Size([115, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1890.jpg\n",
      "1890\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([125, 4])\n",
      "Pooled features size: torch.Size([125, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/976.jpg\n",
      "976\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([146, 4])\n",
      "Pooled features size: torch.Size([146, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1618.jpg\n",
      "1618\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([119, 4])\n",
      "Pooled features size: torch.Size([119, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/853.jpg\n",
      "853\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([116, 4])\n",
      "Pooled features size: torch.Size([116, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/951.jpg\n",
      "951\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([138, 4])\n",
      "Pooled features size: torch.Size([138, 2048])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1360.jpg\n",
      "1360\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([129, 4])\n",
      "Pooled features size: torch.Size([129, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1842.jpg\n",
      "1842\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([154, 4])\n",
      "Pooled features size: torch.Size([154, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1633.jpg\n",
      "1633\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([117, 4])\n",
      "Pooled features size: torch.Size([117, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1252.jpg\n",
      "1252\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([143, 4])\n",
      "Pooled features size: torch.Size([143, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1589.jpg\n",
      "1589\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([122, 4])\n",
      "Pooled features size: torch.Size([122, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1673.jpg\n",
      "1673\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([106, 4])\n",
      "Pooled features size: torch.Size([106, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1857.jpg\n",
      "1857\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([151, 4])\n",
      "Pooled features size: torch.Size([151, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1883.jpg\n",
      "1883\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([143, 4])\n",
      "Pooled features size: torch.Size([143, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1849.jpg\n",
      "1849\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([131, 4])\n",
      "Pooled features size: torch.Size([131, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1316.jpg\n",
      "1316\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([137, 4])\n",
      "Pooled features size: torch.Size([137, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/811.jpg\n",
      "811\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([138, 4])\n",
      "Pooled features size: torch.Size([138, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1641.jpg\n",
      "1641\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([147, 4])\n",
      "Pooled features size: torch.Size([147, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/420.jpg\n",
      "420\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([122, 4])\n",
      "Pooled features size: torch.Size([122, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/612.jpg\n",
      "612\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([129, 4])\n",
      "Pooled features size: torch.Size([129, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/826.jpg\n",
      "826\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([128, 4])\n",
      "Pooled features size: torch.Size([128, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1583.jpg\n",
      "1583\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([131, 4])\n",
      "Pooled features size: torch.Size([131, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1658.jpg\n",
      "1658\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([125, 4])\n",
      "Pooled features size: torch.Size([125, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1138.jpg\n",
      "1138\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([119, 4])\n",
      "Pooled features size: torch.Size([119, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/521.jpg\n",
      "521\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([129, 4])\n",
      "Pooled features size: torch.Size([129, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1013.jpg\n",
      "1013\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([138, 4])\n",
      "Pooled features size: torch.Size([138, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/480.jpg\n",
      "480\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([135, 4])\n",
      "Pooled features size: torch.Size([135, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/938.jpg\n",
      "938\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([117, 4])\n",
      "Pooled features size: torch.Size([117, 2048])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1032.jpg\n",
      "1032\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([162, 4])\n",
      "Pooled features size: torch.Size([162, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1593.jpg\n",
      "1593\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([140, 4])\n",
      "Pooled features size: torch.Size([140, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/358.jpg\n",
      "358\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([124, 4])\n",
      "Pooled features size: torch.Size([124, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/897.jpg\n",
      "897\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([115, 4])\n",
      "Pooled features size: torch.Size([115, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/66.jpg\n",
      "66\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([127, 4])\n",
      "Pooled features size: torch.Size([127, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/471.jpg\n",
      "471\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([163, 4])\n",
      "Pooled features size: torch.Size([163, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/369.jpg\n",
      "369\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([123, 4])\n",
      "Pooled features size: torch.Size([123, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1820.jpg\n",
      "1820\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([164, 4])\n",
      "Pooled features size: torch.Size([164, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1452.jpg\n",
      "1452\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([148, 4])\n",
      "Pooled features size: torch.Size([148, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1384.jpg\n",
      "1384\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([142, 4])\n",
      "Pooled features size: torch.Size([142, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1282.jpg\n",
      "1282\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([129, 4])\n",
      "Pooled features size: torch.Size([129, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1752.jpg\n",
      "1752\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([130, 4])\n",
      "Pooled features size: torch.Size([130, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1548.jpg\n",
      "1548\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([147, 4])\n",
      "Pooled features size: torch.Size([147, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/307.jpg\n",
      "307\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([150, 4])\n",
      "Pooled features size: torch.Size([150, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/200.jpg\n",
      "200\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([124, 4])\n",
      "Pooled features size: torch.Size([124, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/721.jpg\n",
      "721\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([146, 4])\n",
      "Pooled features size: torch.Size([146, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/262.jpg\n",
      "262\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([173, 4])\n",
      "Pooled features size: torch.Size([173, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/877.jpg\n",
      "877\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([133, 4])\n",
      "Pooled features size: torch.Size([133, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1414.jpg\n",
      "1414\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([152, 4])\n",
      "Pooled features size: torch.Size([152, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1107.jpg\n",
      "1107\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([137, 4])\n",
      "Pooled features size: torch.Size([137, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1431.jpg\n",
      "1431\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([148, 4])\n",
      "Pooled features size: torch.Size([148, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/337.jpg\n",
      "337\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([122, 4])\n",
      "Pooled features size: torch.Size([122, 2048])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/569.jpg\n",
      "569\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([144, 4])\n",
      "Pooled features size: torch.Size([144, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1845.jpg\n",
      "1845\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([131, 4])\n",
      "Pooled features size: torch.Size([131, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1254.jpg\n",
      "1254\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([199, 4])\n",
      "Pooled features size: torch.Size([199, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/360.jpg\n",
      "360\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([132, 4])\n",
      "Pooled features size: torch.Size([132, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/448.jpg\n",
      "448\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([114, 4])\n",
      "Pooled features size: torch.Size([114, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1549.jpg\n",
      "1549\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([119, 4])\n",
      "Pooled features size: torch.Size([119, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/903.jpg\n",
      "903\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([108, 4])\n",
      "Pooled features size: torch.Size([108, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/5.jpg\n",
      "5\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([127, 4])\n",
      "Pooled features size: torch.Size([127, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/561.jpg\n",
      "561\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([125, 4])\n",
      "Pooled features size: torch.Size([125, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1545.jpg\n",
      "1545\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([153, 4])\n",
      "Pooled features size: torch.Size([153, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/545.jpg\n",
      "545\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([157, 4])\n",
      "Pooled features size: torch.Size([157, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/56.jpg\n",
      "56\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([143, 4])\n",
      "Pooled features size: torch.Size([143, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/60.jpg\n",
      "60\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([161, 4])\n",
      "Pooled features size: torch.Size([161, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1462.jpg\n",
      "1462\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([120, 4])\n",
      "Pooled features size: torch.Size([120, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/568.jpg\n",
      "568\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([135, 4])\n",
      "Pooled features size: torch.Size([135, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1292.jpg\n",
      "1292\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([136, 4])\n",
      "Pooled features size: torch.Size([136, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/180.jpg\n",
      "180\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([140, 4])\n",
      "Pooled features size: torch.Size([140, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1178.jpg\n",
      "1178\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([125, 4])\n",
      "Pooled features size: torch.Size([125, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1162.jpg\n",
      "1162\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([148, 4])\n",
      "Pooled features size: torch.Size([148, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1423.jpg\n",
      "1423\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([130, 4])\n",
      "Pooled features size: torch.Size([130, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/941.jpg\n",
      "941\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([142, 4])\n",
      "Pooled features size: torch.Size([142, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/326.jpg\n",
      "326\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([122, 4])\n",
      "Pooled features size: torch.Size([122, 2048])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1851.jpg\n",
      "1851\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([123, 4])\n",
      "Pooled features size: torch.Size([123, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/648.jpg\n",
      "648\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([132, 4])\n",
      "Pooled features size: torch.Size([132, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1577.jpg\n",
      "1577\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([150, 4])\n",
      "Pooled features size: torch.Size([150, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1838.jpg\n",
      "1838\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([161, 4])\n",
      "Pooled features size: torch.Size([161, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/602.jpg\n",
      "602\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([147, 4])\n",
      "Pooled features size: torch.Size([147, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/390.jpg\n",
      "390\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([171, 4])\n",
      "Pooled features size: torch.Size([171, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/773.jpg\n",
      "773\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([125, 4])\n",
      "Pooled features size: torch.Size([125, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1640.jpg\n",
      "1640\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([156, 4])\n",
      "Pooled features size: torch.Size([156, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1087.jpg\n",
      "1087\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([134, 4])\n",
      "Pooled features size: torch.Size([134, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/839.jpg\n",
      "839\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([160, 4])\n",
      "Pooled features size: torch.Size([160, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1753.jpg\n",
      "1753\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([126, 4])\n",
      "Pooled features size: torch.Size([126, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/297.jpg\n",
      "297\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([150, 4])\n",
      "Pooled features size: torch.Size([150, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1564.jpg\n",
      "1564\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([156, 4])\n",
      "Pooled features size: torch.Size([156, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/611.jpg\n",
      "611\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([144, 4])\n",
      "Pooled features size: torch.Size([144, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/4.jpg\n",
      "4\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([117, 4])\n",
      "Pooled features size: torch.Size([117, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1057.jpg\n",
      "1057\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([145, 4])\n",
      "Pooled features size: torch.Size([145, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1116.jpg\n",
      "1116\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([125, 4])\n",
      "Pooled features size: torch.Size([125, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1729.jpg\n",
      "1729\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([133, 4])\n",
      "Pooled features size: torch.Size([133, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1115.jpg\n",
      "1115\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([151, 4])\n",
      "Pooled features size: torch.Size([151, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/417.jpg\n",
      "417\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([122, 4])\n",
      "Pooled features size: torch.Size([122, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1249.jpg\n",
      "1249\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([127, 4])\n",
      "Pooled features size: torch.Size([127, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/833.jpg\n",
      "833\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([161, 4])\n",
      "Pooled features size: torch.Size([161, 2048])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/123.jpg\n",
      "123\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([131, 4])\n",
      "Pooled features size: torch.Size([131, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1006.jpg\n",
      "1006\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([132, 4])\n",
      "Pooled features size: torch.Size([132, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/97.jpg\n",
      "97\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([127, 4])\n",
      "Pooled features size: torch.Size([127, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/71.jpg\n",
      "71\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([148, 4])\n",
      "Pooled features size: torch.Size([148, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1705.jpg\n",
      "1705\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([133, 4])\n",
      "Pooled features size: torch.Size([133, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1908.jpg\n",
      "1908\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([136, 4])\n",
      "Pooled features size: torch.Size([136, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1579.jpg\n",
      "1579\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([164, 4])\n",
      "Pooled features size: torch.Size([164, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/619.jpg\n",
      "619\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([133, 4])\n",
      "Pooled features size: torch.Size([133, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/636.jpg\n",
      "636\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([171, 4])\n",
      "Pooled features size: torch.Size([171, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1172.jpg\n",
      "1172\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([133, 4])\n",
      "Pooled features size: torch.Size([133, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/756.jpg\n",
      "756\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([145, 4])\n",
      "Pooled features size: torch.Size([145, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/738.jpg\n",
      "738\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([119, 4])\n",
      "Pooled features size: torch.Size([119, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1088.jpg\n",
      "1088\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([126, 4])\n",
      "Pooled features size: torch.Size([126, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/194.jpg\n",
      "194\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([130, 4])\n",
      "Pooled features size: torch.Size([130, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1203.jpg\n",
      "1203\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([169, 4])\n",
      "Pooled features size: torch.Size([169, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/949.jpg\n",
      "949\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([152, 4])\n",
      "Pooled features size: torch.Size([152, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1392.jpg\n",
      "1392\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([152, 4])\n",
      "Pooled features size: torch.Size([152, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/696.jpg\n",
      "696\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([162, 4])\n",
      "Pooled features size: torch.Size([162, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/228.jpg\n",
      "228\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([137, 4])\n",
      "Pooled features size: torch.Size([137, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/574.jpg\n",
      "574\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([125, 4])\n",
      "Pooled features size: torch.Size([125, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1523.jpg\n",
      "1523\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([128, 4])\n",
      "Pooled features size: torch.Size([128, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1701.jpg\n",
      "1701\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([131, 4])\n",
      "Pooled features size: torch.Size([131, 2048])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/684.jpg\n",
      "684\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([139, 4])\n",
      "Pooled features size: torch.Size([139, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1081.jpg\n",
      "1081\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([161, 4])\n",
      "Pooled features size: torch.Size([161, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1468.jpg\n",
      "1468\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([160, 4])\n",
      "Pooled features size: torch.Size([160, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/726.jpg\n",
      "726\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([145, 4])\n",
      "Pooled features size: torch.Size([145, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/23.jpg\n",
      "23\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([126, 4])\n",
      "Pooled features size: torch.Size([126, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/486.jpg\n",
      "486\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([143, 4])\n",
      "Pooled features size: torch.Size([143, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1226.jpg\n",
      "1226\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([129, 4])\n",
      "Pooled features size: torch.Size([129, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1041.jpg\n",
      "1041\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([129, 4])\n",
      "Pooled features size: torch.Size([129, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/99.jpg\n",
      "99\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([118, 4])\n",
      "Pooled features size: torch.Size([118, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/933.jpg\n",
      "933\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([143, 4])\n",
      "Pooled features size: torch.Size([143, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/323.jpg\n",
      "323\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([142, 4])\n",
      "Pooled features size: torch.Size([142, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/713.jpg\n",
      "713\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([137, 4])\n",
      "Pooled features size: torch.Size([137, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/576.jpg\n",
      "576\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([133, 4])\n",
      "Pooled features size: torch.Size([133, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1094.jpg\n",
      "1094\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([147, 4])\n",
      "Pooled features size: torch.Size([147, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/557.jpg\n",
      "557\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([131, 4])\n",
      "Pooled features size: torch.Size([131, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/638.jpg\n",
      "638\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([133, 4])\n",
      "Pooled features size: torch.Size([133, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1882.jpg\n",
      "1882\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([119, 4])\n",
      "Pooled features size: torch.Size([119, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1308.jpg\n",
      "1308\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([151, 4])\n",
      "Pooled features size: torch.Size([151, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1884.jpg\n",
      "1884\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([160, 4])\n",
      "Pooled features size: torch.Size([160, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1822.jpg\n",
      "1822\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([131, 4])\n",
      "Pooled features size: torch.Size([131, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/463.jpg\n",
      "463\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([158, 4])\n",
      "Pooled features size: torch.Size([158, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1645.jpg\n",
      "1645\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([128, 4])\n",
      "Pooled features size: torch.Size([128, 2048])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1707.jpg\n",
      "1707\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([160, 4])\n",
      "Pooled features size: torch.Size([160, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1735.jpg\n",
      "1735\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([124, 4])\n",
      "Pooled features size: torch.Size([124, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/171.jpg\n",
      "171\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([127, 4])\n",
      "Pooled features size: torch.Size([127, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1347.jpg\n",
      "1347\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([143, 4])\n",
      "Pooled features size: torch.Size([143, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1804.jpg\n",
      "1804\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([154, 4])\n",
      "Pooled features size: torch.Size([154, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1856.jpg\n",
      "1856\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([153, 4])\n",
      "Pooled features size: torch.Size([153, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/160.jpg\n",
      "160\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([137, 4])\n",
      "Pooled features size: torch.Size([137, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1905.jpg\n",
      "1905\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([137, 4])\n",
      "Pooled features size: torch.Size([137, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1063.jpg\n",
      "1063\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([161, 4])\n",
      "Pooled features size: torch.Size([161, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1096.jpg\n",
      "1096\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([168, 4])\n",
      "Pooled features size: torch.Size([168, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/96.jpg\n",
      "96\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([125, 4])\n",
      "Pooled features size: torch.Size([125, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1688.jpg\n",
      "1688\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([154, 4])\n",
      "Pooled features size: torch.Size([154, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/179.jpg\n",
      "179\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([128, 4])\n",
      "Pooled features size: torch.Size([128, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1396.jpg\n",
      "1396\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([161, 4])\n",
      "Pooled features size: torch.Size([161, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/605.jpg\n",
      "605\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([124, 4])\n",
      "Pooled features size: torch.Size([124, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1881.jpg\n",
      "1881\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([124, 4])\n",
      "Pooled features size: torch.Size([124, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/818.jpg\n",
      "818\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([132, 4])\n",
      "Pooled features size: torch.Size([132, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1540.jpg\n",
      "1540\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([125, 4])\n",
      "Pooled features size: torch.Size([125, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/770.jpg\n",
      "770\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([146, 4])\n",
      "Pooled features size: torch.Size([146, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/911.jpg\n",
      "911\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([131, 4])\n",
      "Pooled features size: torch.Size([131, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1659.jpg\n",
      "1659\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([128, 4])\n",
      "Pooled features size: torch.Size([128, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/804.jpg\n",
      "804\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([155, 4])\n",
      "Pooled features size: torch.Size([155, 2048])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/355.jpg\n",
      "355\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([141, 4])\n",
      "Pooled features size: torch.Size([141, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/943.jpg\n",
      "943\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([130, 4])\n",
      "Pooled features size: torch.Size([130, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1251.jpg\n",
      "1251\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([133, 4])\n",
      "Pooled features size: torch.Size([133, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1722.jpg\n",
      "1722\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([142, 4])\n",
      "Pooled features size: torch.Size([142, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/242.jpg\n",
      "242\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([109, 4])\n",
      "Pooled features size: torch.Size([109, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/284.jpg\n",
      "284\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([138, 4])\n",
      "Pooled features size: torch.Size([138, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1003.jpg\n",
      "1003\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([130, 4])\n",
      "Pooled features size: torch.Size([130, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/198.jpg\n",
      "198\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([148, 4])\n",
      "Pooled features size: torch.Size([148, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/134.jpg\n",
      "134\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([130, 4])\n",
      "Pooled features size: torch.Size([130, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/986.jpg\n",
      "986\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([156, 4])\n",
      "Pooled features size: torch.Size([156, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1367.jpg\n",
      "1367\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([147, 4])\n",
      "Pooled features size: torch.Size([147, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/141.jpg\n",
      "141\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([147, 4])\n",
      "Pooled features size: torch.Size([147, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1795.jpg\n",
      "1795\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([155, 4])\n",
      "Pooled features size: torch.Size([155, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/296.jpg\n",
      "296\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([134, 4])\n",
      "Pooled features size: torch.Size([134, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/906.jpg\n",
      "906\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([119, 4])\n",
      "Pooled features size: torch.Size([119, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1051.jpg\n",
      "1051\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([120, 4])\n",
      "Pooled features size: torch.Size([120, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1141.jpg\n",
      "1141\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([138, 4])\n",
      "Pooled features size: torch.Size([138, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1332.jpg\n",
      "1332\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([125, 4])\n",
      "Pooled features size: torch.Size([125, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/189.jpg\n",
      "189\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([171, 4])\n",
      "Pooled features size: torch.Size([171, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1771.jpg\n",
      "1771\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([129, 4])\n",
      "Pooled features size: torch.Size([129, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1142.jpg\n",
      "1142\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([123, 4])\n",
      "Pooled features size: torch.Size([123, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1338.jpg\n",
      "1338\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([142, 4])\n",
      "Pooled features size: torch.Size([142, 2048])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1864.jpg\n",
      "1864\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([141, 4])\n",
      "Pooled features size: torch.Size([141, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/673.jpg\n",
      "673\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([125, 4])\n",
      "Pooled features size: torch.Size([125, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/579.jpg\n",
      "579\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([133, 4])\n",
      "Pooled features size: torch.Size([133, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/924.jpg\n",
      "924\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([150, 4])\n",
      "Pooled features size: torch.Size([150, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/888.jpg\n",
      "888\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([147, 4])\n",
      "Pooled features size: torch.Size([147, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1651.jpg\n",
      "1651\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([191, 4])\n",
      "Pooled features size: torch.Size([191, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1680.jpg\n",
      "1680\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([136, 4])\n",
      "Pooled features size: torch.Size([136, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1419.jpg\n",
      "1419\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([156, 4])\n",
      "Pooled features size: torch.Size([156, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1811.jpg\n",
      "1811\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([123, 4])\n",
      "Pooled features size: torch.Size([123, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1283.jpg\n",
      "1283\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([147, 4])\n",
      "Pooled features size: torch.Size([147, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1574.jpg\n",
      "1574\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([127, 4])\n",
      "Pooled features size: torch.Size([127, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/484.jpg\n",
      "484\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([133, 4])\n",
      "Pooled features size: torch.Size([133, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1015.jpg\n",
      "1015\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([120, 4])\n",
      "Pooled features size: torch.Size([120, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1324.jpg\n",
      "1324\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([120, 4])\n",
      "Pooled features size: torch.Size([120, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1090.jpg\n",
      "1090\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([162, 4])\n",
      "Pooled features size: torch.Size([162, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/15.jpg\n",
      "15\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([153, 4])\n",
      "Pooled features size: torch.Size([153, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1634.jpg\n",
      "1634\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([122, 4])\n",
      "Pooled features size: torch.Size([122, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1442.jpg\n",
      "1442\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([143, 4])\n",
      "Pooled features size: torch.Size([143, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/362.jpg\n",
      "362\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([141, 4])\n",
      "Pooled features size: torch.Size([141, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/817.jpg\n",
      "817\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([147, 4])\n",
      "Pooled features size: torch.Size([147, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1553.jpg\n",
      "1553\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([160, 4])\n",
      "Pooled features size: torch.Size([160, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/989.jpg\n",
      "989\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([129, 4])\n",
      "Pooled features size: torch.Size([129, 2048])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/715.jpg\n",
      "715\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([123, 4])\n",
      "Pooled features size: torch.Size([123, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/162.jpg\n",
      "162\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([141, 4])\n",
      "Pooled features size: torch.Size([141, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/176.jpg\n",
      "176\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([118, 4])\n",
      "Pooled features size: torch.Size([118, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1469.jpg\n",
      "1469\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([137, 4])\n",
      "Pooled features size: torch.Size([137, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/936.jpg\n",
      "936\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([130, 4])\n",
      "Pooled features size: torch.Size([130, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1161.jpg\n",
      "1161\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([138, 4])\n",
      "Pooled features size: torch.Size([138, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1290.jpg\n",
      "1290\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([121, 4])\n",
      "Pooled features size: torch.Size([121, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/984.jpg\n",
      "984\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([148, 4])\n",
      "Pooled features size: torch.Size([148, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1356.jpg\n",
      "1356\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([165, 4])\n",
      "Pooled features size: torch.Size([165, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/482.jpg\n",
      "482\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([128, 4])\n",
      "Pooled features size: torch.Size([128, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/282.jpg\n",
      "282\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([131, 4])\n",
      "Pooled features size: torch.Size([131, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1040.jpg\n",
      "1040\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([129, 4])\n",
      "Pooled features size: torch.Size([129, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1263.jpg\n",
      "1263\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([151, 4])\n",
      "Pooled features size: torch.Size([151, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/979.jpg\n",
      "979\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([145, 4])\n",
      "Pooled features size: torch.Size([145, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1412.jpg\n",
      "1412\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([165, 4])\n",
      "Pooled features size: torch.Size([165, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/28.jpg\n",
      "28\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([123, 4])\n",
      "Pooled features size: torch.Size([123, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/91.jpg\n",
      "91\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([158, 4])\n",
      "Pooled features size: torch.Size([158, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/566.jpg\n",
      "566\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([120, 4])\n",
      "Pooled features size: torch.Size([120, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/449.jpg\n",
      "449\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([191, 4])\n",
      "Pooled features size: torch.Size([191, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/828.jpg\n",
      "828\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([120, 4])\n",
      "Pooled features size: torch.Size([120, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1690.jpg\n",
      "1690\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([141, 4])\n",
      "Pooled features size: torch.Size([141, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1101.jpg\n",
      "1101\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([156, 4])\n",
      "Pooled features size: torch.Size([156, 2048])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/973.jpg\n",
      "973\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([128, 4])\n",
      "Pooled features size: torch.Size([128, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/495.jpg\n",
      "495\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([130, 4])\n",
      "Pooled features size: torch.Size([130, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/530.jpg\n",
      "530\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([149, 4])\n",
      "Pooled features size: torch.Size([149, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1506.jpg\n",
      "1506\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([140, 4])\n",
      "Pooled features size: torch.Size([140, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/932.jpg\n",
      "932\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([112, 4])\n",
      "Pooled features size: torch.Size([112, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1489.jpg\n",
      "1489\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([155, 4])\n",
      "Pooled features size: torch.Size([155, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1748.jpg\n",
      "1748\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([118, 4])\n",
      "Pooled features size: torch.Size([118, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1812.jpg\n",
      "1812\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([155, 4])\n",
      "Pooled features size: torch.Size([155, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1535.jpg\n",
      "1535\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([134, 4])\n",
      "Pooled features size: torch.Size([134, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1621.jpg\n",
      "1621\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([126, 4])\n",
      "Pooled features size: torch.Size([126, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/776.jpg\n",
      "776\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([139, 4])\n",
      "Pooled features size: torch.Size([139, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1674.jpg\n",
      "1674\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([145, 4])\n",
      "Pooled features size: torch.Size([145, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/457.jpg\n",
      "457\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([127, 4])\n",
      "Pooled features size: torch.Size([127, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1286.jpg\n",
      "1286\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([142, 4])\n",
      "Pooled features size: torch.Size([142, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/267.jpg\n",
      "267\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([121, 4])\n",
      "Pooled features size: torch.Size([121, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/121.jpg\n",
      "121\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([131, 4])\n",
      "Pooled features size: torch.Size([131, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/341.jpg\n",
      "341\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([148, 4])\n",
      "Pooled features size: torch.Size([148, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1001.jpg\n",
      "1001\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([141, 4])\n",
      "Pooled features size: torch.Size([141, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/908.jpg\n",
      "908\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([138, 4])\n",
      "Pooled features size: torch.Size([138, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1897.jpg\n",
      "1897\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([142, 4])\n",
      "Pooled features size: torch.Size([142, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/493.jpg\n",
      "493\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([134, 4])\n",
      "Pooled features size: torch.Size([134, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/864.jpg\n",
      "864\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([115, 4])\n",
      "Pooled features size: torch.Size([115, 2048])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1445.jpg\n",
      "1445\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([147, 4])\n",
      "Pooled features size: torch.Size([147, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/616.jpg\n",
      "616\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([137, 4])\n",
      "Pooled features size: torch.Size([137, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/151.jpg\n",
      "151\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([129, 4])\n",
      "Pooled features size: torch.Size([129, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1730.jpg\n",
      "1730\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([127, 4])\n",
      "Pooled features size: torch.Size([127, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1459.jpg\n",
      "1459\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([134, 4])\n",
      "Pooled features size: torch.Size([134, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1383.jpg\n",
      "1383\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([132, 4])\n",
      "Pooled features size: torch.Size([132, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1782.jpg\n",
      "1782\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([127, 4])\n",
      "Pooled features size: torch.Size([127, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/830.jpg\n",
      "830\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([145, 4])\n",
      "Pooled features size: torch.Size([145, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/514.jpg\n",
      "514\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([139, 4])\n",
      "Pooled features size: torch.Size([139, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1477.jpg\n",
      "1477\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([127, 4])\n",
      "Pooled features size: torch.Size([127, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/45.jpg\n",
      "45\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([162, 4])\n",
      "Pooled features size: torch.Size([162, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1520.jpg\n",
      "1520\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([144, 4])\n",
      "Pooled features size: torch.Size([144, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1599.jpg\n",
      "1599\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([129, 4])\n",
      "Pooled features size: torch.Size([129, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1809.jpg\n",
      "1809\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([129, 4])\n",
      "Pooled features size: torch.Size([129, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/664.jpg\n",
      "664\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([138, 4])\n",
      "Pooled features size: torch.Size([138, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/195.jpg\n",
      "195\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([130, 4])\n",
      "Pooled features size: torch.Size([130, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/639.jpg\n",
      "639\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([143, 4])\n",
      "Pooled features size: torch.Size([143, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1083.jpg\n",
      "1083\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([143, 4])\n",
      "Pooled features size: torch.Size([143, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1154.jpg\n",
      "1154\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([188, 4])\n",
      "Pooled features size: torch.Size([188, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1052.jpg\n",
      "1052\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([131, 4])\n",
      "Pooled features size: torch.Size([131, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/196.jpg\n",
      "196\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([136, 4])\n",
      "Pooled features size: torch.Size([136, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/501.jpg\n",
      "501\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([120, 4])\n",
      "Pooled features size: torch.Size([120, 2048])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1788.jpg\n",
      "1788\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([127, 4])\n",
      "Pooled features size: torch.Size([127, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1888.jpg\n",
      "1888\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([123, 4])\n",
      "Pooled features size: torch.Size([123, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1450.jpg\n",
      "1450\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([122, 4])\n",
      "Pooled features size: torch.Size([122, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1164.jpg\n",
      "1164\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([149, 4])\n",
      "Pooled features size: torch.Size([149, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1277.jpg\n",
      "1277\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([124, 4])\n",
      "Pooled features size: torch.Size([124, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1074.jpg\n",
      "1074\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([141, 4])\n",
      "Pooled features size: torch.Size([141, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/140.jpg\n",
      "140\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([138, 4])\n",
      "Pooled features size: torch.Size([138, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/86.jpg\n",
      "86\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([136, 4])\n",
      "Pooled features size: torch.Size([136, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/741.jpg\n",
      "741\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([122, 4])\n",
      "Pooled features size: torch.Size([122, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1533.jpg\n",
      "1533\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([126, 4])\n",
      "Pooled features size: torch.Size([126, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/748.jpg\n",
      "748\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([127, 4])\n",
      "Pooled features size: torch.Size([127, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/886.jpg\n",
      "886\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([120, 4])\n",
      "Pooled features size: torch.Size([120, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/210.jpg\n",
      "210\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([133, 4])\n",
      "Pooled features size: torch.Size([133, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/306.jpg\n",
      "306\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([126, 4])\n",
      "Pooled features size: torch.Size([126, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1190.jpg\n",
      "1190\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([137, 4])\n",
      "Pooled features size: torch.Size([137, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1381.jpg\n",
      "1381\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([115, 4])\n",
      "Pooled features size: torch.Size([115, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/775.jpg\n",
      "775\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([129, 4])\n",
      "Pooled features size: torch.Size([129, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/617.jpg\n",
      "617\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([113, 4])\n",
      "Pooled features size: torch.Size([113, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1293.jpg\n",
      "1293\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([167, 4])\n",
      "Pooled features size: torch.Size([167, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1357.jpg\n",
      "1357\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([118, 4])\n",
      "Pooled features size: torch.Size([118, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/824.jpg\n",
      "824\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([136, 4])\n",
      "Pooled features size: torch.Size([136, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1682.jpg\n",
      "1682\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([150, 4])\n",
      "Pooled features size: torch.Size([150, 2048])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/229.jpg\n",
      "229\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([131, 4])\n",
      "Pooled features size: torch.Size([131, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/423.jpg\n",
      "423\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([166, 4])\n",
      "Pooled features size: torch.Size([166, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1614.jpg\n",
      "1614\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([162, 4])\n",
      "Pooled features size: torch.Size([162, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/995.jpg\n",
      "995\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([144, 4])\n",
      "Pooled features size: torch.Size([144, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1626.jpg\n",
      "1626\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([144, 4])\n",
      "Pooled features size: torch.Size([144, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/477.jpg\n",
      "477\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([132, 4])\n",
      "Pooled features size: torch.Size([132, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1068.jpg\n",
      "1068\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([127, 4])\n",
      "Pooled features size: torch.Size([127, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/291.jpg\n",
      "291\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([127, 4])\n",
      "Pooled features size: torch.Size([127, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/894.jpg\n",
      "894\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([115, 4])\n",
      "Pooled features size: torch.Size([115, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1903.jpg\n",
      "1903\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([142, 4])\n",
      "Pooled features size: torch.Size([142, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1532.jpg\n",
      "1532\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([135, 4])\n",
      "Pooled features size: torch.Size([135, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/211.jpg\n",
      "211\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([163, 4])\n",
      "Pooled features size: torch.Size([163, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1761.jpg\n",
      "1761\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([124, 4])\n",
      "Pooled features size: torch.Size([124, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/135.jpg\n",
      "135\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([138, 4])\n",
      "Pooled features size: torch.Size([138, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1727.jpg\n",
      "1727\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([151, 4])\n",
      "Pooled features size: torch.Size([151, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1762.jpg\n",
      "1762\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([134, 4])\n",
      "Pooled features size: torch.Size([134, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/825.jpg\n",
      "825\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([144, 4])\n",
      "Pooled features size: torch.Size([144, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/17.jpg\n",
      "17\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([145, 4])\n",
      "Pooled features size: torch.Size([145, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1192.jpg\n",
      "1192\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([193, 4])\n",
      "Pooled features size: torch.Size([193, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/73.jpg\n",
      "73\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([135, 4])\n",
      "Pooled features size: torch.Size([135, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/797.jpg\n",
      "797\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([126, 4])\n",
      "Pooled features size: torch.Size([126, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/338.jpg\n",
      "338\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([124, 4])\n",
      "Pooled features size: torch.Size([124, 2048])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/393.jpg\n",
      "393\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([142, 4])\n",
      "Pooled features size: torch.Size([142, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/905.jpg\n",
      "905\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([140, 4])\n",
      "Pooled features size: torch.Size([140, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/383.jpg\n",
      "383\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([164, 4])\n",
      "Pooled features size: torch.Size([164, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/840.jpg\n",
      "840\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([137, 4])\n",
      "Pooled features size: torch.Size([137, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/845.jpg\n",
      "845\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([139, 4])\n",
      "Pooled features size: torch.Size([139, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/860.jpg\n",
      "860\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([124, 4])\n",
      "Pooled features size: torch.Size([124, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1109.jpg\n",
      "1109\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([127, 4])\n",
      "Pooled features size: torch.Size([127, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1072.jpg\n",
      "1072\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([118, 4])\n",
      "Pooled features size: torch.Size([118, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1449.jpg\n",
      "1449\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([154, 4])\n",
      "Pooled features size: torch.Size([154, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1198.jpg\n",
      "1198\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([139, 4])\n",
      "Pooled features size: torch.Size([139, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1153.jpg\n",
      "1153\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([142, 4])\n",
      "Pooled features size: torch.Size([142, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/455.jpg\n",
      "455\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([162, 4])\n",
      "Pooled features size: torch.Size([162, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1461.jpg\n",
      "1461\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([132, 4])\n",
      "Pooled features size: torch.Size([132, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1610.jpg\n",
      "1610\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([136, 4])\n",
      "Pooled features size: torch.Size([136, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/236.jpg\n",
      "236\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([134, 4])\n",
      "Pooled features size: torch.Size([134, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/870.jpg\n",
      "870\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([143, 4])\n",
      "Pooled features size: torch.Size([143, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/710.jpg\n",
      "710\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([125, 4])\n",
      "Pooled features size: torch.Size([125, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/127.jpg\n",
      "127\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([139, 4])\n",
      "Pooled features size: torch.Size([139, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/473.jpg\n",
      "473\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([136, 4])\n",
      "Pooled features size: torch.Size([136, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1519.jpg\n",
      "1519\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([132, 4])\n",
      "Pooled features size: torch.Size([132, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1314.jpg\n",
      "1314\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([149, 4])\n",
      "Pooled features size: torch.Size([149, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1775.jpg\n",
      "1775\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([134, 4])\n",
      "Pooled features size: torch.Size([134, 2048])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1662.jpg\n",
      "1662\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([123, 4])\n",
      "Pooled features size: torch.Size([123, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1742.jpg\n",
      "1742\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([122, 4])\n",
      "Pooled features size: torch.Size([122, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1219.jpg\n",
      "1219\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([144, 4])\n",
      "Pooled features size: torch.Size([144, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1303.jpg\n",
      "1303\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([139, 4])\n",
      "Pooled features size: torch.Size([139, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1758.jpg\n",
      "1758\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([123, 4])\n",
      "Pooled features size: torch.Size([123, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/787.jpg\n",
      "787\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([135, 4])\n",
      "Pooled features size: torch.Size([135, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1853.jpg\n",
      "1853\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([195, 4])\n",
      "Pooled features size: torch.Size([195, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/271.jpg\n",
      "271\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([132, 4])\n",
      "Pooled features size: torch.Size([132, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/184.jpg\n",
      "184\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([120, 4])\n",
      "Pooled features size: torch.Size([120, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1022.jpg\n",
      "1022\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([143, 4])\n",
      "Pooled features size: torch.Size([143, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1596.jpg\n",
      "1596\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([148, 4])\n",
      "Pooled features size: torch.Size([148, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/481.jpg\n",
      "481\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([125, 4])\n",
      "Pooled features size: torch.Size([125, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1158.jpg\n",
      "1158\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([118, 4])\n",
      "Pooled features size: torch.Size([118, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1329.jpg\n",
      "1329\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([153, 4])\n",
      "Pooled features size: torch.Size([153, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/445.jpg\n",
      "445\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([134, 4])\n",
      "Pooled features size: torch.Size([134, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/478.jpg\n",
      "478\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([149, 4])\n",
      "Pooled features size: torch.Size([149, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/216.jpg\n",
      "216\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([157, 4])\n",
      "Pooled features size: torch.Size([157, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1053.jpg\n",
      "1053\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([130, 4])\n",
      "Pooled features size: torch.Size([130, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1521.jpg\n",
      "1521\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([139, 4])\n",
      "Pooled features size: torch.Size([139, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/343.jpg\n",
      "343\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([136, 4])\n",
      "Pooled features size: torch.Size([136, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1122.jpg\n",
      "1122\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([139, 4])\n",
      "Pooled features size: torch.Size([139, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/590.jpg\n",
      "590\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([140, 4])\n",
      "Pooled features size: torch.Size([140, 2048])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/593.jpg\n",
      "593\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([144, 4])\n",
      "Pooled features size: torch.Size([144, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1755.jpg\n",
      "1755\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([127, 4])\n",
      "Pooled features size: torch.Size([127, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1223.jpg\n",
      "1223\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([190, 4])\n",
      "Pooled features size: torch.Size([190, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/450.jpg\n",
      "450\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([135, 4])\n",
      "Pooled features size: torch.Size([135, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/512.jpg\n",
      "512\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([123, 4])\n",
      "Pooled features size: torch.Size([123, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1835.jpg\n",
      "1835\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([133, 4])\n",
      "Pooled features size: torch.Size([133, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1008.jpg\n",
      "1008\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([123, 4])\n",
      "Pooled features size: torch.Size([123, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1143.jpg\n",
      "1143\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([166, 4])\n",
      "Pooled features size: torch.Size([166, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/310.jpg\n",
      "310\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([146, 4])\n",
      "Pooled features size: torch.Size([146, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/681.jpg\n",
      "681\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([150, 4])\n",
      "Pooled features size: torch.Size([150, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/977.jpg\n",
      "977\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([161, 4])\n",
      "Pooled features size: torch.Size([161, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1309.jpg\n",
      "1309\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([128, 4])\n",
      "Pooled features size: torch.Size([128, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1901.jpg\n",
      "1901\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([161, 4])\n",
      "Pooled features size: torch.Size([161, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/866.jpg\n",
      "866\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([116, 4])\n",
      "Pooled features size: torch.Size([116, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/75.jpg\n",
      "75\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([149, 4])\n",
      "Pooled features size: torch.Size([149, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/658.jpg\n",
      "658\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([133, 4])\n",
      "Pooled features size: torch.Size([133, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1024.jpg\n",
      "1024\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([143, 4])\n",
      "Pooled features size: torch.Size([143, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1907.jpg\n",
      "1907\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([125, 4])\n",
      "Pooled features size: torch.Size([125, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1746.jpg\n",
      "1746\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([136, 4])\n",
      "Pooled features size: torch.Size([136, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/548.jpg\n",
      "548\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([137, 4])\n",
      "Pooled features size: torch.Size([137, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1760.jpg\n",
      "1760\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([127, 4])\n",
      "Pooled features size: torch.Size([127, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1664.jpg\n",
      "1664\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([139, 4])\n",
      "Pooled features size: torch.Size([139, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/990.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "990\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([136, 4])\n",
      "Pooled features size: torch.Size([136, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1372.jpg\n",
      "1372\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([142, 4])\n",
      "Pooled features size: torch.Size([142, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1402.jpg\n",
      "1402\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([147, 4])\n",
      "Pooled features size: torch.Size([147, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/675.jpg\n",
      "675\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([123, 4])\n",
      "Pooled features size: torch.Size([123, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/923.jpg\n",
      "923\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([135, 4])\n",
      "Pooled features size: torch.Size([135, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/126.jpg\n",
      "126\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([158, 4])\n",
      "Pooled features size: torch.Size([158, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1861.jpg\n",
      "1861\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([149, 4])\n",
      "Pooled features size: torch.Size([149, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1575.jpg\n",
      "1575\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([132, 4])\n",
      "Pooled features size: torch.Size([132, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1471.jpg\n",
      "1471\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([147, 4])\n",
      "Pooled features size: torch.Size([147, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1257.jpg\n",
      "1257\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([120, 4])\n",
      "Pooled features size: torch.Size([120, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/357.jpg\n",
      "357\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([144, 4])\n",
      "Pooled features size: torch.Size([144, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/657.jpg\n",
      "657\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([110, 4])\n",
      "Pooled features size: torch.Size([110, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/98.jpg\n",
      "98\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([137, 4])\n",
      "Pooled features size: torch.Size([137, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/954.jpg\n",
      "954\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([133, 4])\n",
      "Pooled features size: torch.Size([133, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1892.jpg\n",
      "1892\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([130, 4])\n",
      "Pooled features size: torch.Size([130, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1466.jpg\n",
      "1466\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([137, 4])\n",
      "Pooled features size: torch.Size([137, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/520.jpg\n",
      "520\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([133, 4])\n",
      "Pooled features size: torch.Size([133, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/273.jpg\n",
      "273\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([141, 4])\n",
      "Pooled features size: torch.Size([141, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1029.jpg\n",
      "1029\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([164, 4])\n",
      "Pooled features size: torch.Size([164, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1627.jpg\n",
      "1627\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([155, 4])\n",
      "Pooled features size: torch.Size([155, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/170.jpg\n",
      "170\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([143, 4])\n",
      "Pooled features size: torch.Size([143, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1866.jpg\n",
      "1866\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([167, 4])\n",
      "Pooled features size: torch.Size([167, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/848.jpg\n",
      "848\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([125, 4])\n",
      "Pooled features size: torch.Size([125, 2048])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/244.jpg\n",
      "244\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([129, 4])\n",
      "Pooled features size: torch.Size([129, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/249.jpg\n",
      "249\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([118, 4])\n",
      "Pooled features size: torch.Size([118, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/618.jpg\n",
      "618\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([123, 4])\n",
      "Pooled features size: torch.Size([123, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1140.jpg\n",
      "1140\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([144, 4])\n",
      "Pooled features size: torch.Size([144, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1840.jpg\n",
      "1840\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([147, 4])\n",
      "Pooled features size: torch.Size([147, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1630.jpg\n",
      "1630\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([124, 4])\n",
      "Pooled features size: torch.Size([124, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/806.jpg\n",
      "806\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([161, 4])\n",
      "Pooled features size: torch.Size([161, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/570.jpg\n",
      "570\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([131, 4])\n",
      "Pooled features size: torch.Size([131, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/956.jpg\n",
      "956\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([131, 4])\n",
      "Pooled features size: torch.Size([131, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1271.jpg\n",
      "1271\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([138, 4])\n",
      "Pooled features size: torch.Size([138, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/922.jpg\n",
      "922\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([135, 4])\n",
      "Pooled features size: torch.Size([135, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/38.jpg\n",
      "38\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([132, 4])\n",
      "Pooled features size: torch.Size([132, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1813.jpg\n",
      "1813\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([160, 4])\n",
      "Pooled features size: torch.Size([160, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/259.jpg\n",
      "259\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([141, 4])\n",
      "Pooled features size: torch.Size([141, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1179.jpg\n",
      "1179\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([140, 4])\n",
      "Pooled features size: torch.Size([140, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/599.jpg\n",
      "599\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([140, 4])\n",
      "Pooled features size: torch.Size([140, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1173.jpg\n",
      "1173\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([143, 4])\n",
      "Pooled features size: torch.Size([143, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1602.jpg\n",
      "1602\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([124, 4])\n",
      "Pooled features size: torch.Size([124, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/467.jpg\n",
      "467\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([168, 4])\n",
      "Pooled features size: torch.Size([168, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1819.jpg\n",
      "1819\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([139, 4])\n",
      "Pooled features size: torch.Size([139, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/807.jpg\n",
      "807\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([124, 4])\n",
      "Pooled features size: torch.Size([124, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/971.jpg\n",
      "971\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([127, 4])\n",
      "Pooled features size: torch.Size([127, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/208.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([137, 4])\n",
      "Pooled features size: torch.Size([137, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1585.jpg\n",
      "1585\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([130, 4])\n",
      "Pooled features size: torch.Size([130, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/233.jpg\n",
      "233\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([134, 4])\n",
      "Pooled features size: torch.Size([134, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1229.jpg\n",
      "1229\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([111, 4])\n",
      "Pooled features size: torch.Size([111, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1510.jpg\n",
      "1510\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([129, 4])\n",
      "Pooled features size: torch.Size([129, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1174.jpg\n",
      "1174\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([139, 4])\n",
      "Pooled features size: torch.Size([139, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/54.jpg\n",
      "54\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([120, 4])\n",
      "Pooled features size: torch.Size([120, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/221.jpg\n",
      "221\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([132, 4])\n",
      "Pooled features size: torch.Size([132, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/100.jpg\n",
      "100\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([126, 4])\n",
      "Pooled features size: torch.Size([126, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1699.jpg\n",
      "1699\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([138, 4])\n",
      "Pooled features size: torch.Size([138, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1413.jpg\n",
      "1413\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([154, 4])\n",
      "Pooled features size: torch.Size([154, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/421.jpg\n",
      "421\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([131, 4])\n",
      "Pooled features size: torch.Size([131, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1382.jpg\n",
      "1382\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([124, 4])\n",
      "Pooled features size: torch.Size([124, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/641.jpg\n",
      "641\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([133, 4])\n",
      "Pooled features size: torch.Size([133, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1865.jpg\n",
      "1865\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([140, 4])\n",
      "Pooled features size: torch.Size([140, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/962.jpg\n",
      "962\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([132, 4])\n",
      "Pooled features size: torch.Size([132, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/300.jpg\n",
      "300\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([144, 4])\n",
      "Pooled features size: torch.Size([144, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/595.jpg\n",
      "595\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([120, 4])\n",
      "Pooled features size: torch.Size([120, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/470.jpg\n",
      "470\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([146, 4])\n",
      "Pooled features size: torch.Size([146, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/799.jpg\n",
      "799\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([133, 4])\n",
      "Pooled features size: torch.Size([133, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/272.jpg\n",
      "272\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([143, 4])\n",
      "Pooled features size: torch.Size([143, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/737.jpg\n",
      "737\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([116, 4])\n",
      "Pooled features size: torch.Size([116, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/114.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([153, 4])\n",
      "Pooled features size: torch.Size([153, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1573.jpg\n",
      "1573\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([131, 4])\n",
      "Pooled features size: torch.Size([131, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/987.jpg\n",
      "987\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([118, 4])\n",
      "Pooled features size: torch.Size([118, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/433.jpg\n",
      "433\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([138, 4])\n",
      "Pooled features size: torch.Size([138, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1676.jpg\n",
      "1676\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([144, 4])\n",
      "Pooled features size: torch.Size([144, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1801.jpg\n",
      "1801\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([129, 4])\n",
      "Pooled features size: torch.Size([129, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/572.jpg\n",
      "572\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([133, 4])\n",
      "Pooled features size: torch.Size([133, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/44.jpg\n",
      "44\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([139, 4])\n",
      "Pooled features size: torch.Size([139, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1079.jpg\n",
      "1079\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([119, 4])\n",
      "Pooled features size: torch.Size([119, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1717.jpg\n",
      "1717\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([155, 4])\n",
      "Pooled features size: torch.Size([155, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/292.jpg\n",
      "292\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([145, 4])\n",
      "Pooled features size: torch.Size([145, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1734.jpg\n",
      "1734\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([163, 4])\n",
      "Pooled features size: torch.Size([163, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/67.jpg\n",
      "67\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([112, 4])\n",
      "Pooled features size: torch.Size([112, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1458.jpg\n",
      "1458\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([129, 4])\n",
      "Pooled features size: torch.Size([129, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1586.jpg\n",
      "1586\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([124, 4])\n",
      "Pooled features size: torch.Size([124, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1056.jpg\n",
      "1056\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([125, 4])\n",
      "Pooled features size: torch.Size([125, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/131.jpg\n",
      "131\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([129, 4])\n",
      "Pooled features size: torch.Size([129, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/529.jpg\n",
      "529\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([133, 4])\n",
      "Pooled features size: torch.Size([133, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1655.jpg\n",
      "1655\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([147, 4])\n",
      "Pooled features size: torch.Size([147, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1739.jpg\n",
      "1739\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([136, 4])\n",
      "Pooled features size: torch.Size([136, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/890.jpg\n",
      "890\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([132, 4])\n",
      "Pooled features size: torch.Size([132, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1481.jpg\n",
      "1481\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([137, 4])\n",
      "Pooled features size: torch.Size([137, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1062.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1062\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([128, 4])\n",
      "Pooled features size: torch.Size([128, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1870.jpg\n",
      "1870\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([159, 4])\n",
      "Pooled features size: torch.Size([159, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1243.jpg\n",
      "1243\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([142, 4])\n",
      "Pooled features size: torch.Size([142, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/380.jpg\n",
      "380\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([144, 4])\n",
      "Pooled features size: torch.Size([144, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/767.jpg\n",
      "767\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([122, 4])\n",
      "Pooled features size: torch.Size([122, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/113.jpg\n",
      "113\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([124, 4])\n",
      "Pooled features size: torch.Size([124, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/13.jpg\n",
      "13\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([157, 4])\n",
      "Pooled features size: torch.Size([157, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1799.jpg\n",
      "1799\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([135, 4])\n",
      "Pooled features size: torch.Size([135, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1204.jpg\n",
      "1204\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([148, 4])\n",
      "Pooled features size: torch.Size([148, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/70.jpg\n",
      "70\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([126, 4])\n",
      "Pooled features size: torch.Size([126, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/921.jpg\n",
      "921\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([148, 4])\n",
      "Pooled features size: torch.Size([148, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/981.jpg\n",
      "981\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([131, 4])\n",
      "Pooled features size: torch.Size([131, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/419.jpg\n",
      "419\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([176, 4])\n",
      "Pooled features size: torch.Size([176, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1624.jpg\n",
      "1624\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([122, 4])\n",
      "Pooled features size: torch.Size([122, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1201.jpg\n",
      "1201\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([128, 4])\n",
      "Pooled features size: torch.Size([128, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1244.jpg\n",
      "1244\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([161, 4])\n",
      "Pooled features size: torch.Size([161, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/742.jpg\n",
      "742\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([121, 4])\n",
      "Pooled features size: torch.Size([121, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1598.jpg\n",
      "1598\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([134, 4])\n",
      "Pooled features size: torch.Size([134, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/577.jpg\n",
      "577\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([132, 4])\n",
      "Pooled features size: torch.Size([132, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1106.jpg\n",
      "1106\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([119, 4])\n",
      "Pooled features size: torch.Size([119, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1120.jpg\n",
      "1120\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([163, 4])\n",
      "Pooled features size: torch.Size([163, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1129.jpg\n",
      "1129\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([141, 4])\n",
      "Pooled features size: torch.Size([141, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1542.jpg\n",
      "1542\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([160, 4])\n",
      "Pooled features size: torch.Size([160, 2048])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1077.jpg\n",
      "1077\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([134, 4])\n",
      "Pooled features size: torch.Size([134, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1019.jpg\n",
      "1019\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([148, 4])\n",
      "Pooled features size: torch.Size([148, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/794.jpg\n",
      "794\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([124, 4])\n",
      "Pooled features size: torch.Size([124, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/283.jpg\n",
      "283\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([127, 4])\n",
      "Pooled features size: torch.Size([127, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/651.jpg\n",
      "651\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([158, 4])\n",
      "Pooled features size: torch.Size([158, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1430.jpg\n",
      "1430\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([150, 4])\n",
      "Pooled features size: torch.Size([150, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1313.jpg\n",
      "1313\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([150, 4])\n",
      "Pooled features size: torch.Size([150, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1167.jpg\n",
      "1167\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([137, 4])\n",
      "Pooled features size: torch.Size([137, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/647.jpg\n",
      "647\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([123, 4])\n",
      "Pooled features size: torch.Size([123, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/777.jpg\n",
      "777\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([131, 4])\n",
      "Pooled features size: torch.Size([131, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1915.jpg\n",
      "1915\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([127, 4])\n",
      "Pooled features size: torch.Size([127, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1424.jpg\n",
      "1424\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([137, 4])\n",
      "Pooled features size: torch.Size([137, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/166.jpg\n",
      "166\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([146, 4])\n",
      "Pooled features size: torch.Size([146, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1048.jpg\n",
      "1048\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([150, 4])\n",
      "Pooled features size: torch.Size([150, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/874.jpg\n",
      "874\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([133, 4])\n",
      "Pooled features size: torch.Size([133, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/785.jpg\n",
      "785\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([125, 4])\n",
      "Pooled features size: torch.Size([125, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1160.jpg\n",
      "1160\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([130, 4])\n",
      "Pooled features size: torch.Size([130, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/220.jpg\n",
      "220\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([134, 4])\n",
      "Pooled features size: torch.Size([134, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/844.jpg\n",
      "844\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([121, 4])\n",
      "Pooled features size: torch.Size([121, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1119.jpg\n",
      "1119\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([145, 4])\n",
      "Pooled features size: torch.Size([145, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/879.jpg\n",
      "879\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([140, 4])\n",
      "Pooled features size: torch.Size([140, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/257.jpg\n",
      "257\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([150, 4])\n",
      "Pooled features size: torch.Size([150, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/416.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "416\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([131, 4])\n",
      "Pooled features size: torch.Size([131, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1475.jpg\n",
      "1475\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([138, 4])\n",
      "Pooled features size: torch.Size([138, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1311.jpg\n",
      "1311\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([157, 4])\n",
      "Pooled features size: torch.Size([157, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/823.jpg\n",
      "823\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([114, 4])\n",
      "Pooled features size: torch.Size([114, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/55.jpg\n",
      "55\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([117, 4])\n",
      "Pooled features size: torch.Size([117, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/2.jpg\n",
      "2\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([128, 4])\n",
      "Pooled features size: torch.Size([128, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/882.jpg\n",
      "882\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([162, 4])\n",
      "Pooled features size: torch.Size([162, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1667.jpg\n",
      "1667\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([130, 4])\n",
      "Pooled features size: torch.Size([130, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/993.jpg\n",
      "993\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([138, 4])\n",
      "Pooled features size: torch.Size([138, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/104.jpg\n",
      "104\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([121, 4])\n",
      "Pooled features size: torch.Size([121, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1543.jpg\n",
      "1543\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([131, 4])\n",
      "Pooled features size: torch.Size([131, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1336.jpg\n",
      "1336\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([125, 4])\n",
      "Pooled features size: torch.Size([125, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/598.jpg\n",
      "598\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([139, 4])\n",
      "Pooled features size: torch.Size([139, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/429.jpg\n",
      "429\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([111, 4])\n",
      "Pooled features size: torch.Size([111, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1288.jpg\n",
      "1288\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([144, 4])\n",
      "Pooled features size: torch.Size([144, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/789.jpg\n",
      "789\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([157, 4])\n",
      "Pooled features size: torch.Size([157, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/9.jpg\n",
      "9\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([156, 4])\n",
      "Pooled features size: torch.Size([156, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1134.jpg\n",
      "1134\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([151, 4])\n",
      "Pooled features size: torch.Size([151, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1581.jpg\n",
      "1581\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([122, 4])\n",
      "Pooled features size: torch.Size([122, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/395.jpg\n",
      "395\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([136, 4])\n",
      "Pooled features size: torch.Size([136, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/551.jpg\n",
      "551\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([159, 4])\n",
      "Pooled features size: torch.Size([159, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/440.jpg\n",
      "440\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([127, 4])\n",
      "Pooled features size: torch.Size([127, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/816.jpg\n",
      "816\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([123, 4])\n",
      "Pooled features size: torch.Size([123, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1021.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1021\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([133, 4])\n",
      "Pooled features size: torch.Size([133, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/374.jpg\n",
      "374\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([130, 4])\n",
      "Pooled features size: torch.Size([130, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/798.jpg\n",
      "798\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([159, 4])\n",
      "Pooled features size: torch.Size([159, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1661.jpg\n",
      "1661\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([165, 4])\n",
      "Pooled features size: torch.Size([165, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1685.jpg\n",
      "1685\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([139, 4])\n",
      "Pooled features size: torch.Size([139, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/827.jpg\n",
      "827\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([136, 4])\n",
      "Pooled features size: torch.Size([136, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1390.jpg\n",
      "1390\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([164, 4])\n",
      "Pooled features size: torch.Size([164, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/487.jpg\n",
      "487\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([154, 4])\n",
      "Pooled features size: torch.Size([154, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/963.jpg\n",
      "963\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([112, 4])\n",
      "Pooled features size: torch.Size([112, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1836.jpg\n",
      "1836\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([124, 4])\n",
      "Pooled features size: torch.Size([124, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1108.jpg\n",
      "1108\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([142, 4])\n",
      "Pooled features size: torch.Size([142, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1235.jpg\n",
      "1235\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([115, 4])\n",
      "Pooled features size: torch.Size([115, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/149.jpg\n",
      "149\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([136, 4])\n",
      "Pooled features size: torch.Size([136, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/80.jpg\n",
      "80\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([133, 4])\n",
      "Pooled features size: torch.Size([133, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1490.jpg\n",
      "1490\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([130, 4])\n",
      "Pooled features size: torch.Size([130, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1092.jpg\n",
      "1092\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([134, 4])\n",
      "Pooled features size: torch.Size([134, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1208.jpg\n",
      "1208\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([141, 4])\n",
      "Pooled features size: torch.Size([141, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/803.jpg\n",
      "803\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([127, 4])\n",
      "Pooled features size: torch.Size([127, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1202.jpg\n",
      "1202\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([133, 4])\n",
      "Pooled features size: torch.Size([133, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/31.jpg\n",
      "31\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([155, 4])\n",
      "Pooled features size: torch.Size([155, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1898.jpg\n",
      "1898\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([162, 4])\n",
      "Pooled features size: torch.Size([162, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1789.jpg\n",
      "1789\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([127, 4])\n",
      "Pooled features size: torch.Size([127, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/623.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "623\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([129, 4])\n",
      "Pooled features size: torch.Size([129, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/327.jpg\n",
      "327\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([133, 4])\n",
      "Pooled features size: torch.Size([133, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/822.jpg\n",
      "822\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([121, 4])\n",
      "Pooled features size: torch.Size([121, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/716.jpg\n",
      "716\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([126, 4])\n",
      "Pooled features size: torch.Size([126, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/533.jpg\n",
      "533\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([140, 4])\n",
      "Pooled features size: torch.Size([140, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/476.jpg\n",
      "476\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([145, 4])\n",
      "Pooled features size: torch.Size([145, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/649.jpg\n",
      "649\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([140, 4])\n",
      "Pooled features size: torch.Size([140, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/503.jpg\n",
      "503\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([140, 4])\n",
      "Pooled features size: torch.Size([140, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1565.jpg\n",
      "1565\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([132, 4])\n",
      "Pooled features size: torch.Size([132, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/967.jpg\n",
      "967\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([138, 4])\n",
      "Pooled features size: torch.Size([138, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1672.jpg\n",
      "1672\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([133, 4])\n",
      "Pooled features size: torch.Size([133, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/663.jpg\n",
      "663\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([130, 4])\n",
      "Pooled features size: torch.Size([130, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/403.jpg\n",
      "403\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([167, 4])\n",
      "Pooled features size: torch.Size([167, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1559.jpg\n",
      "1559\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([153, 4])\n",
      "Pooled features size: torch.Size([153, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/186.jpg\n",
      "186\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([132, 4])\n",
      "Pooled features size: torch.Size([132, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/124.jpg\n",
      "124\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([140, 4])\n",
      "Pooled features size: torch.Size([140, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/707.jpg\n",
      "707\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([158, 4])\n",
      "Pooled features size: torch.Size([158, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1554.jpg\n",
      "1554\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([125, 4])\n",
      "Pooled features size: torch.Size([125, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/919.jpg\n",
      "919\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([141, 4])\n",
      "Pooled features size: torch.Size([141, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1874.jpg\n",
      "1874\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([109, 4])\n",
      "Pooled features size: torch.Size([109, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/783.jpg\n",
      "783\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([157, 4])\n",
      "Pooled features size: torch.Size([157, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/571.jpg\n",
      "571\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([133, 4])\n",
      "Pooled features size: torch.Size([133, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1305.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1305\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([167, 4])\n",
      "Pooled features size: torch.Size([167, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/34.jpg\n",
      "34\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([134, 4])\n",
      "Pooled features size: torch.Size([134, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1380.jpg\n",
      "1380\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([129, 4])\n",
      "Pooled features size: torch.Size([129, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1061.jpg\n",
      "1061\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([145, 4])\n",
      "Pooled features size: torch.Size([145, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1709.jpg\n",
      "1709\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([142, 4])\n",
      "Pooled features size: torch.Size([142, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/650.jpg\n",
      "650\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([120, 4])\n",
      "Pooled features size: torch.Size([120, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1769.jpg\n",
      "1769\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([133, 4])\n",
      "Pooled features size: torch.Size([133, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1315.jpg\n",
      "1315\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([132, 4])\n",
      "Pooled features size: torch.Size([132, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/542.jpg\n",
      "542\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([128, 4])\n",
      "Pooled features size: torch.Size([128, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/786.jpg\n",
      "786\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([143, 4])\n",
      "Pooled features size: torch.Size([143, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/705.jpg\n",
      "705\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([148, 4])\n",
      "Pooled features size: torch.Size([148, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/837.jpg\n",
      "837\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([127, 4])\n",
      "Pooled features size: torch.Size([127, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/246.jpg\n",
      "246\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([144, 4])\n",
      "Pooled features size: torch.Size([144, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/884.jpg\n",
      "884\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([115, 4])\n",
      "Pooled features size: torch.Size([115, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/597.jpg\n",
      "597\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([148, 4])\n",
      "Pooled features size: torch.Size([148, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1697.jpg\n",
      "1697\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([119, 4])\n",
      "Pooled features size: torch.Size([119, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/969.jpg\n",
      "969\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([132, 4])\n",
      "Pooled features size: torch.Size([132, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/65.jpg\n",
      "65\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([111, 4])\n",
      "Pooled features size: torch.Size([111, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/555.jpg\n",
      "555\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([143, 4])\n",
      "Pooled features size: torch.Size([143, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/896.jpg\n",
      "896\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([129, 4])\n",
      "Pooled features size: torch.Size([129, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1622.jpg\n",
      "1622\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([115, 4])\n",
      "Pooled features size: torch.Size([115, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1434.jpg\n",
      "1434\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([128, 4])\n",
      "Pooled features size: torch.Size([128, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1224.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1224\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([127, 4])\n",
      "Pooled features size: torch.Size([127, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/427.jpg\n",
      "427\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([139, 4])\n",
      "Pooled features size: torch.Size([139, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1240.jpg\n",
      "1240\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([128, 4])\n",
      "Pooled features size: torch.Size([128, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/667.jpg\n",
      "667\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([130, 4])\n",
      "Pooled features size: torch.Size([130, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/192.jpg\n",
      "192\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([158, 4])\n",
      "Pooled features size: torch.Size([158, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/975.jpg\n",
      "975\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([131, 4])\n",
      "Pooled features size: torch.Size([131, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1152.jpg\n",
      "1152\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([148, 4])\n",
      "Pooled features size: torch.Size([148, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1275.jpg\n",
      "1275\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([158, 4])\n",
      "Pooled features size: torch.Size([158, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/604.jpg\n",
      "604\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([139, 4])\n",
      "Pooled features size: torch.Size([139, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1408.jpg\n",
      "1408\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([161, 4])\n",
      "Pooled features size: torch.Size([161, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/335.jpg\n",
      "335\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([136, 4])\n",
      "Pooled features size: torch.Size([136, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/245.jpg\n",
      "245\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([163, 4])\n",
      "Pooled features size: torch.Size([163, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/970.jpg\n",
      "970\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([127, 4])\n",
      "Pooled features size: torch.Size([127, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1331.jpg\n",
      "1331\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([127, 4])\n",
      "Pooled features size: torch.Size([127, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1779.jpg\n",
      "1779\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([129, 4])\n",
      "Pooled features size: torch.Size([129, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/320.jpg\n",
      "320\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([138, 4])\n",
      "Pooled features size: torch.Size([138, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1388.jpg\n",
      "1388\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([133, 4])\n",
      "Pooled features size: torch.Size([133, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/155.jpg\n",
      "155\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([141, 4])\n",
      "Pooled features size: torch.Size([141, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/125.jpg\n",
      "125\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([156, 4])\n",
      "Pooled features size: torch.Size([156, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1529.jpg\n",
      "1529\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([116, 4])\n",
      "Pooled features size: torch.Size([116, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/739.jpg\n",
      "739\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([127, 4])\n",
      "Pooled features size: torch.Size([127, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1531.jpg\n",
      "1531\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([137, 4])\n",
      "Pooled features size: torch.Size([137, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1199.jpg\n",
      "1199\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([147, 4])\n",
      "Pooled features size: torch.Size([147, 2048])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/668.jpg\n",
      "668\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([128, 4])\n",
      "Pooled features size: torch.Size([128, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/407.jpg\n",
      "407\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([121, 4])\n",
      "Pooled features size: torch.Size([121, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/247.jpg\n",
      "247\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([130, 4])\n",
      "Pooled features size: torch.Size([130, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1453.jpg\n",
      "1453\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([127, 4])\n",
      "Pooled features size: torch.Size([127, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/389.jpg\n",
      "389\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([135, 4])\n",
      "Pooled features size: torch.Size([135, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/373.jpg\n",
      "373\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([130, 4])\n",
      "Pooled features size: torch.Size([130, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/0.jpg\n",
      "0\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([123, 4])\n",
      "Pooled features size: torch.Size([123, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1513.jpg\n",
      "1513\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([130, 4])\n",
      "Pooled features size: torch.Size([130, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/724.jpg\n",
      "724\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([143, 4])\n",
      "Pooled features size: torch.Size([143, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/137.jpg\n",
      "137\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([130, 4])\n",
      "Pooled features size: torch.Size([130, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/219.jpg\n",
      "219\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([130, 4])\n",
      "Pooled features size: torch.Size([130, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/809.jpg\n",
      "809\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([158, 4])\n",
      "Pooled features size: torch.Size([158, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/342.jpg\n",
      "342\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([134, 4])\n",
      "Pooled features size: torch.Size([134, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/628.jpg\n",
      "628\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([164, 4])\n",
      "Pooled features size: torch.Size([164, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/810.jpg\n",
      "810\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([111, 4])\n",
      "Pooled features size: torch.Size([111, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/295.jpg\n",
      "295\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([169, 4])\n",
      "Pooled features size: torch.Size([169, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/401.jpg\n",
      "401\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([117, 4])\n",
      "Pooled features size: torch.Size([117, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/382.jpg\n",
      "382\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([143, 4])\n",
      "Pooled features size: torch.Size([143, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/539.jpg\n",
      "539\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([120, 4])\n",
      "Pooled features size: torch.Size([120, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/592.jpg\n",
      "592\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([153, 4])\n",
      "Pooled features size: torch.Size([153, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1749.jpg\n",
      "1749\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([132, 4])\n",
      "Pooled features size: torch.Size([132, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1509.jpg\n",
      "1509\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([125, 4])\n",
      "Pooled features size: torch.Size([125, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1486.jpg\n",
      "1486\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([125, 4])\n",
      "Pooled features size: torch.Size([125, 2048])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/843.jpg\n",
      "843\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([135, 4])\n",
      "Pooled features size: torch.Size([135, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/862.jpg\n",
      "862\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([127, 4])\n",
      "Pooled features size: torch.Size([127, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1427.jpg\n",
      "1427\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([133, 4])\n",
      "Pooled features size: torch.Size([133, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/303.jpg\n",
      "303\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([138, 4])\n",
      "Pooled features size: torch.Size([138, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1538.jpg\n",
      "1538\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([150, 4])\n",
      "Pooled features size: torch.Size([150, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1551.jpg\n",
      "1551\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([134, 4])\n",
      "Pooled features size: torch.Size([134, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1754.jpg\n",
      "1754\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([133, 4])\n",
      "Pooled features size: torch.Size([133, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1218.jpg\n",
      "1218\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([138, 4])\n",
      "Pooled features size: torch.Size([138, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1639.jpg\n",
      "1639\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([145, 4])\n",
      "Pooled features size: torch.Size([145, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1411.jpg\n",
      "1411\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([140, 4])\n",
      "Pooled features size: torch.Size([140, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/120.jpg\n",
      "120\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([138, 4])\n",
      "Pooled features size: torch.Size([138, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1266.jpg\n",
      "1266\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([138, 4])\n",
      "Pooled features size: torch.Size([138, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/105.jpg\n",
      "105\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([126, 4])\n",
      "Pooled features size: torch.Size([126, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/758.jpg\n",
      "758\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([123, 4])\n",
      "Pooled features size: torch.Size([123, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/7.jpg\n",
      "7\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([127, 4])\n",
      "Pooled features size: torch.Size([127, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/497.jpg\n",
      "497\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([116, 4])\n",
      "Pooled features size: torch.Size([116, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1033.jpg\n",
      "1033\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([194, 4])\n",
      "Pooled features size: torch.Size([194, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/255.jpg\n",
      "255\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([138, 4])\n",
      "Pooled features size: torch.Size([138, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/51.jpg\n",
      "51\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([124, 4])\n",
      "Pooled features size: torch.Size([124, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/736.jpg\n",
      "736\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([125, 4])\n",
      "Pooled features size: torch.Size([125, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1740.jpg\n",
      "1740\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([154, 4])\n",
      "Pooled features size: torch.Size([154, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1686.jpg\n",
      "1686\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([125, 4])\n",
      "Pooled features size: torch.Size([125, 2048])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1594.jpg\n",
      "1594\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([164, 4])\n",
      "Pooled features size: torch.Size([164, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1228.jpg\n",
      "1228\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([149, 4])\n",
      "Pooled features size: torch.Size([149, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/53.jpg\n",
      "53\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([159, 4])\n",
      "Pooled features size: torch.Size([159, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1512.jpg\n",
      "1512\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([112, 4])\n",
      "Pooled features size: torch.Size([112, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/41.jpg\n",
      "41\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([132, 4])\n",
      "Pooled features size: torch.Size([132, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/564.jpg\n",
      "564\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([160, 4])\n",
      "Pooled features size: torch.Size([160, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/489.jpg\n",
      "489\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([128, 4])\n",
      "Pooled features size: torch.Size([128, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/508.jpg\n",
      "508\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([126, 4])\n",
      "Pooled features size: torch.Size([126, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1757.jpg\n",
      "1757\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([158, 4])\n",
      "Pooled features size: torch.Size([158, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1436.jpg\n",
      "1436\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([142, 4])\n",
      "Pooled features size: torch.Size([142, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1238.jpg\n",
      "1238\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([137, 4])\n",
      "Pooled features size: torch.Size([137, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1580.jpg\n",
      "1580\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([117, 4])\n",
      "Pooled features size: torch.Size([117, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1385.jpg\n",
      "1385\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([134, 4])\n",
      "Pooled features size: torch.Size([134, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1358.jpg\n",
      "1358\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([168, 4])\n",
      "Pooled features size: torch.Size([168, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/351.jpg\n",
      "351\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([138, 4])\n",
      "Pooled features size: torch.Size([138, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/366.jpg\n",
      "366\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([122, 4])\n",
      "Pooled features size: torch.Size([122, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1023.jpg\n",
      "1023\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([160, 4])\n",
      "Pooled features size: torch.Size([160, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/68.jpg\n",
      "68\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([126, 4])\n",
      "Pooled features size: torch.Size([126, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/562.jpg\n",
      "562\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([165, 4])\n",
      "Pooled features size: torch.Size([165, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1823.jpg\n",
      "1823\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([131, 4])\n",
      "Pooled features size: torch.Size([131, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/852.jpg\n",
      "852\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([132, 4])\n",
      "Pooled features size: torch.Size([132, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1076.jpg\n",
      "1076\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([139, 4])\n",
      "Pooled features size: torch.Size([139, 2048])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/596.jpg\n",
      "596\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([143, 4])\n",
      "Pooled features size: torch.Size([143, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/784.jpg\n",
      "784\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([128, 4])\n",
      "Pooled features size: torch.Size([128, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/108.jpg\n",
      "108\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([155, 4])\n",
      "Pooled features size: torch.Size([155, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/361.jpg\n",
      "361\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([133, 4])\n",
      "Pooled features size: torch.Size([133, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/730.jpg\n",
      "730\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([162, 4])\n",
      "Pooled features size: torch.Size([162, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/678.jpg\n",
      "678\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([137, 4])\n",
      "Pooled features size: torch.Size([137, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1323.jpg\n",
      "1323\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([130, 4])\n",
      "Pooled features size: torch.Size([130, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/25.jpg\n",
      "25\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([124, 4])\n",
      "Pooled features size: torch.Size([124, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/371.jpg\n",
      "371\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([130, 4])\n",
      "Pooled features size: torch.Size([130, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/685.jpg\n",
      "685\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([116, 4])\n",
      "Pooled features size: torch.Size([116, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/394.jpg\n",
      "394\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([126, 4])\n",
      "Pooled features size: torch.Size([126, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1817.jpg\n",
      "1817\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([123, 4])\n",
      "Pooled features size: torch.Size([123, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1294.jpg\n",
      "1294\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([119, 4])\n",
      "Pooled features size: torch.Size([119, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/925.jpg\n",
      "925\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([168, 4])\n",
      "Pooled features size: torch.Size([168, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1781.jpg\n",
      "1781\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([162, 4])\n",
      "Pooled features size: torch.Size([162, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1181.jpg\n",
      "1181\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([152, 4])\n",
      "Pooled features size: torch.Size([152, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1260.jpg\n",
      "1260\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([120, 4])\n",
      "Pooled features size: torch.Size([120, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1656.jpg\n",
      "1656\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([125, 4])\n",
      "Pooled features size: torch.Size([125, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/849.jpg\n",
      "849\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([139, 4])\n",
      "Pooled features size: torch.Size([139, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/130.jpg\n",
      "130\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([122, 4])\n",
      "Pooled features size: torch.Size([122, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1642.jpg\n",
      "1642\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([140, 4])\n",
      "Pooled features size: torch.Size([140, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1078.jpg\n",
      "1078\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([152, 4])\n",
      "Pooled features size: torch.Size([152, 2048])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1791.jpg\n",
      "1791\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([131, 4])\n",
      "Pooled features size: torch.Size([131, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/367.jpg\n",
      "367\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([144, 4])\n",
      "Pooled features size: torch.Size([144, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1245.jpg\n",
      "1245\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([176, 4])\n",
      "Pooled features size: torch.Size([176, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1330.jpg\n",
      "1330\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([118, 4])\n",
      "Pooled features size: torch.Size([118, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1744.jpg\n",
      "1744\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([129, 4])\n",
      "Pooled features size: torch.Size([129, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/409.jpg\n",
      "409\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([127, 4])\n",
      "Pooled features size: torch.Size([127, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/832.jpg\n",
      "832\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([135, 4])\n",
      "Pooled features size: torch.Size([135, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1885.jpg\n",
      "1885\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([135, 4])\n",
      "Pooled features size: torch.Size([135, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1891.jpg\n",
      "1891\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([154, 4])\n",
      "Pooled features size: torch.Size([154, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1075.jpg\n",
      "1075\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([152, 4])\n",
      "Pooled features size: torch.Size([152, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/399.jpg\n",
      "399\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([120, 4])\n",
      "Pooled features size: torch.Size([120, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/589.jpg\n",
      "589\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([144, 4])\n",
      "Pooled features size: torch.Size([144, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/472.jpg\n",
      "472\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([135, 4])\n",
      "Pooled features size: torch.Size([135, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/239.jpg\n",
      "239\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([156, 4])\n",
      "Pooled features size: torch.Size([156, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1073.jpg\n",
      "1073\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([106, 4])\n",
      "Pooled features size: torch.Size([106, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/119.jpg\n",
      "119\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([118, 4])\n",
      "Pooled features size: torch.Size([118, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/266.jpg\n",
      "266\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([125, 4])\n",
      "Pooled features size: torch.Size([125, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1563.jpg\n",
      "1563\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([138, 4])\n",
      "Pooled features size: torch.Size([138, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1349.jpg\n",
      "1349\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([128, 4])\n",
      "Pooled features size: torch.Size([128, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/79.jpg\n",
      "79\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([156, 4])\n",
      "Pooled features size: torch.Size([156, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1132.jpg\n",
      "1132\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([128, 4])\n",
      "Pooled features size: torch.Size([128, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1783.jpg\n",
      "1783\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([141, 4])\n",
      "Pooled features size: torch.Size([141, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/957.jpg\n",
      "957\n",
      "Original image size:  (2160, 3840)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([134, 4])\n",
      "Pooled features size: torch.Size([134, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/84.jpg\n",
      "84\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([112, 4])\n",
      "Pooled features size: torch.Size([112, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/348.jpg\n",
      "348\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([131, 4])\n",
      "Pooled features size: torch.Size([131, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/854.jpg\n",
      "854\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([158, 4])\n",
      "Pooled features size: torch.Size([158, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1665.jpg\n",
      "1665\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([152, 4])\n",
      "Pooled features size: torch.Size([152, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1418.jpg\n",
      "1418\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([159, 4])\n",
      "Pooled features size: torch.Size([159, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1093.jpg\n",
      "1093\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([108, 4])\n",
      "Pooled features size: torch.Size([108, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/169.jpg\n",
      "169\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([133, 4])\n",
      "Pooled features size: torch.Size([133, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/12.jpg\n",
      "12\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([143, 4])\n",
      "Pooled features size: torch.Size([143, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/215.jpg\n",
      "215\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([155, 4])\n",
      "Pooled features size: torch.Size([155, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/855.jpg\n",
      "855\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([116, 4])\n",
      "Pooled features size: torch.Size([116, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1631.jpg\n",
      "1631\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([134, 4])\n",
      "Pooled features size: torch.Size([134, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1476.jpg\n",
      "1476\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([169, 4])\n",
      "Pooled features size: torch.Size([169, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1507.jpg\n",
      "1507\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([146, 4])\n",
      "Pooled features size: torch.Size([146, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/57.jpg\n",
      "57\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([123, 4])\n",
      "Pooled features size: torch.Size([123, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1439.jpg\n",
      "1439\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([126, 4])\n",
      "Pooled features size: torch.Size([126, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/159.jpg\n",
      "159\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([163, 4])\n",
      "Pooled features size: torch.Size([163, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1236.jpg\n",
      "1236\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([134, 4])\n",
      "Pooled features size: torch.Size([134, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/859.jpg\n",
      "859\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([135, 4])\n",
      "Pooled features size: torch.Size([135, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/328.jpg\n",
      "328\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([142, 4])\n",
      "Pooled features size: torch.Size([142, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1706.jpg\n",
      "1706\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([135, 4])\n",
      "Pooled features size: torch.Size([135, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1689.jpg\n",
      "1689\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([137, 4])\n",
      "Pooled features size: torch.Size([137, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1663.jpg\n",
      "1663\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([141, 4])\n",
      "Pooled features size: torch.Size([141, 2048])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/485.jpg\n",
      "485\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([128, 4])\n",
      "Pooled features size: torch.Size([128, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1165.jpg\n",
      "1165\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([125, 4])\n",
      "Pooled features size: torch.Size([125, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1798.jpg\n",
      "1798\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([137, 4])\n",
      "Pooled features size: torch.Size([137, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1687.jpg\n",
      "1687\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([153, 4])\n",
      "Pooled features size: torch.Size([153, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/526.jpg\n",
      "526\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([136, 4])\n",
      "Pooled features size: torch.Size([136, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1774.jpg\n",
      "1774\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([137, 4])\n",
      "Pooled features size: torch.Size([137, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/902.jpg\n",
      "902\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([146, 4])\n",
      "Pooled features size: torch.Size([146, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1794.jpg\n",
      "1794\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([158, 4])\n",
      "Pooled features size: torch.Size([158, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1191.jpg\n",
      "1191\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([123, 4])\n",
      "Pooled features size: torch.Size([123, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/900.jpg\n",
      "900\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([117, 4])\n",
      "Pooled features size: torch.Size([117, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1295.jpg\n",
      "1295\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([127, 4])\n",
      "Pooled features size: torch.Size([127, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/422.jpg\n",
      "422\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([137, 4])\n",
      "Pooled features size: torch.Size([137, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/385.jpg\n",
      "385\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([128, 4])\n",
      "Pooled features size: torch.Size([128, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/873.jpg\n",
      "873\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([129, 4])\n",
      "Pooled features size: torch.Size([129, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1497.jpg\n",
      "1497\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([121, 4])\n",
      "Pooled features size: torch.Size([121, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/386.jpg\n",
      "386\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([120, 4])\n",
      "Pooled features size: torch.Size([120, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1312.jpg\n",
      "1312\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([109, 4])\n",
      "Pooled features size: torch.Size([109, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/248.jpg\n",
      "248\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([152, 4])\n",
      "Pooled features size: torch.Size([152, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/682.jpg\n",
      "682\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([126, 4])\n",
      "Pooled features size: torch.Size([126, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1149.jpg\n",
      "1149\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([141, 4])\n",
      "Pooled features size: torch.Size([141, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/217.jpg\n",
      "217\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([127, 4])\n",
      "Pooled features size: torch.Size([127, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/325.jpg\n",
      "325\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([144, 4])\n",
      "Pooled features size: torch.Size([144, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1354.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1354\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([147, 4])\n",
      "Pooled features size: torch.Size([147, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/693.jpg\n",
      "693\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([127, 4])\n",
      "Pooled features size: torch.Size([127, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/966.jpg\n",
      "966\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([136, 4])\n",
      "Pooled features size: torch.Size([136, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/857.jpg\n",
      "857\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([139, 4])\n",
      "Pooled features size: torch.Size([139, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/688.jpg\n",
      "688\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([185, 4])\n",
      "Pooled features size: torch.Size([185, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/222.jpg\n",
      "222\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([146, 4])\n",
      "Pooled features size: torch.Size([146, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1911.jpg\n",
      "1911\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([130, 4])\n",
      "Pooled features size: torch.Size([130, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/704.jpg\n",
      "704\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([164, 4])\n",
      "Pooled features size: torch.Size([164, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1637.jpg\n",
      "1637\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([116, 4])\n",
      "Pooled features size: torch.Size([116, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1397.jpg\n",
      "1397\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([167, 4])\n",
      "Pooled features size: torch.Size([167, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/964.jpg\n",
      "964\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([127, 4])\n",
      "Pooled features size: torch.Size([127, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1112.jpg\n",
      "1112\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([151, 4])\n",
      "Pooled features size: torch.Size([151, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/947.jpg\n",
      "947\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([126, 4])\n",
      "Pooled features size: torch.Size([126, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1113.jpg\n",
      "1113\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([129, 4])\n",
      "Pooled features size: torch.Size([129, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1401.jpg\n",
      "1401\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([127, 4])\n",
      "Pooled features size: torch.Size([127, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/62.jpg\n",
      "62\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([136, 4])\n",
      "Pooled features size: torch.Size([136, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/939.jpg\n",
      "939\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([144, 4])\n",
      "Pooled features size: torch.Size([144, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/538.jpg\n",
      "538\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([125, 4])\n",
      "Pooled features size: torch.Size([125, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/483.jpg\n",
      "483\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([179, 4])\n",
      "Pooled features size: torch.Size([179, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/573.jpg\n",
      "573\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([130, 4])\n",
      "Pooled features size: torch.Size([130, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1403.jpg\n",
      "1403\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([123, 4])\n",
      "Pooled features size: torch.Size([123, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/999.jpg\n",
      "999\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([134, 4])\n",
      "Pooled features size: torch.Size([134, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/626.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "626\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([150, 4])\n",
      "Pooled features size: torch.Size([150, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1652.jpg\n",
      "1652\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([117, 4])\n",
      "Pooled features size: torch.Size([117, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/181.jpg\n",
      "181\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([131, 4])\n",
      "Pooled features size: torch.Size([131, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1409.jpg\n",
      "1409\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([138, 4])\n",
      "Pooled features size: torch.Size([138, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/16.jpg\n",
      "16\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([154, 4])\n",
      "Pooled features size: torch.Size([154, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1265.jpg\n",
      "1265\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([125, 4])\n",
      "Pooled features size: torch.Size([125, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1060.jpg\n",
      "1060\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([128, 4])\n",
      "Pooled features size: torch.Size([128, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1020.jpg\n",
      "1020\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([139, 4])\n",
      "Pooled features size: torch.Size([139, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/52.jpg\n",
      "52\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([153, 4])\n",
      "Pooled features size: torch.Size([153, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/553.jpg\n",
      "553\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([136, 4])\n",
      "Pooled features size: torch.Size([136, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/354.jpg\n",
      "354\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([119, 4])\n",
      "Pooled features size: torch.Size([119, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1117.jpg\n",
      "1117\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([125, 4])\n",
      "Pooled features size: torch.Size([125, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1731.jpg\n",
      "1731\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([134, 4])\n",
      "Pooled features size: torch.Size([134, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/968.jpg\n",
      "968\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([136, 4])\n",
      "Pooled features size: torch.Size([136, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/591.jpg\n",
      "591\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([123, 4])\n",
      "Pooled features size: torch.Size([123, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/494.jpg\n",
      "494\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([132, 4])\n",
      "Pooled features size: torch.Size([132, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1470.jpg\n",
      "1470\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([135, 4])\n",
      "Pooled features size: torch.Size([135, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/22.jpg\n",
      "22\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([117, 4])\n",
      "Pooled features size: torch.Size([117, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/609.jpg\n",
      "609\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([155, 4])\n",
      "Pooled features size: torch.Size([155, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1376.jpg\n",
      "1376\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([118, 4])\n",
      "Pooled features size: torch.Size([118, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1215.jpg\n",
      "1215\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([116, 4])\n",
      "Pooled features size: torch.Size([116, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1432.jpg\n",
      "1432\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([145, 4])\n",
      "Pooled features size: torch.Size([145, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1768.jpg\n",
      "1768\n",
      "Original image size:  (2160, 3840)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([130, 4])\n",
      "Pooled features size: torch.Size([130, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/498.jpg\n",
      "498\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([147, 4])\n",
      "Pooled features size: torch.Size([147, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/723.jpg\n",
      "723\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([137, 4])\n",
      "Pooled features size: torch.Size([137, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1261.jpg\n",
      "1261\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([144, 4])\n",
      "Pooled features size: torch.Size([144, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1488.jpg\n",
      "1488\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([135, 4])\n",
      "Pooled features size: torch.Size([135, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1784.jpg\n",
      "1784\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([139, 4])\n",
      "Pooled features size: torch.Size([139, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/352.jpg\n",
      "352\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([127, 4])\n",
      "Pooled features size: torch.Size([127, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1328.jpg\n",
      "1328\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([149, 4])\n",
      "Pooled features size: torch.Size([149, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/469.jpg\n",
      "469\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([154, 4])\n",
      "Pooled features size: torch.Size([154, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/490.jpg\n",
      "490\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([130, 4])\n",
      "Pooled features size: torch.Size([130, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1818.jpg\n",
      "1818\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([140, 4])\n",
      "Pooled features size: torch.Size([140, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1163.jpg\n",
      "1163\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([128, 4])\n",
      "Pooled features size: torch.Size([128, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/330.jpg\n",
      "330\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([122, 4])\n",
      "Pooled features size: torch.Size([122, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1679.jpg\n",
      "1679\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([129, 4])\n",
      "Pooled features size: torch.Size([129, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/20.jpg\n",
      "20\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([133, 4])\n",
      "Pooled features size: torch.Size([133, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/238.jpg\n",
      "238\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([125, 4])\n",
      "Pooled features size: torch.Size([125, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/846.jpg\n",
      "846\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([119, 4])\n",
      "Pooled features size: torch.Size([119, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/344.jpg\n",
      "344\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([126, 4])\n",
      "Pooled features size: torch.Size([126, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/771.jpg\n",
      "771\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([132, 4])\n",
      "Pooled features size: torch.Size([132, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1616.jpg\n",
      "1616\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([142, 4])\n",
      "Pooled features size: torch.Size([142, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/615.jpg\n",
      "615\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([119, 4])\n",
      "Pooled features size: torch.Size([119, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1448.jpg\n",
      "1448\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([127, 4])\n",
      "Pooled features size: torch.Size([127, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1009.jpg\n",
      "1009\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([134, 4])\n",
      "Pooled features size: torch.Size([134, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/666.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "666\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([142, 4])\n",
      "Pooled features size: torch.Size([142, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/294.jpg\n",
      "294\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([123, 4])\n",
      "Pooled features size: torch.Size([123, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/516.jpg\n",
      "516\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([127, 4])\n",
      "Pooled features size: torch.Size([127, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1724.jpg\n",
      "1724\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([129, 4])\n",
      "Pooled features size: torch.Size([129, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1441.jpg\n",
      "1441\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([142, 4])\n",
      "Pooled features size: torch.Size([142, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/340.jpg\n",
      "340\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([154, 4])\n",
      "Pooled features size: torch.Size([154, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/69.jpg\n",
      "69\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([161, 4])\n",
      "Pooled features size: torch.Size([161, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/454.jpg\n",
      "454\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([148, 4])\n",
      "Pooled features size: torch.Size([148, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1504.jpg\n",
      "1504\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([136, 4])\n",
      "Pooled features size: torch.Size([136, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1420.jpg\n",
      "1420\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([127, 4])\n",
      "Pooled features size: torch.Size([127, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1815.jpg\n",
      "1815\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([125, 4])\n",
      "Pooled features size: torch.Size([125, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/549.jpg\n",
      "549\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([134, 4])\n",
      "Pooled features size: torch.Size([134, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1169.jpg\n",
      "1169\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([113, 4])\n",
      "Pooled features size: torch.Size([113, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/72.jpg\n",
      "72\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([134, 4])\n",
      "Pooled features size: torch.Size([134, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/699.jpg\n",
      "699\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([121, 4])\n",
      "Pooled features size: torch.Size([121, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1.jpg\n",
      "1\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([125, 4])\n",
      "Pooled features size: torch.Size([125, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/304.jpg\n",
      "304\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([158, 4])\n",
      "Pooled features size: torch.Size([158, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/167.jpg\n",
      "167\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([129, 4])\n",
      "Pooled features size: torch.Size([129, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/333.jpg\n",
      "333\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([129, 4])\n",
      "Pooled features size: torch.Size([129, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/891.jpg\n",
      "891\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([171, 4])\n",
      "Pooled features size: torch.Size([171, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1425.jpg\n",
      "1425\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([151, 4])\n",
      "Pooled features size: torch.Size([151, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1200.jpg\n",
      "1200\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([162, 4])\n",
      "Pooled features size: torch.Size([162, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1346.jpg\n",
      "1346\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([128, 4])\n",
      "Pooled features size: torch.Size([128, 2048])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1044.jpg\n",
      "1044\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([125, 4])\n",
      "Pooled features size: torch.Size([125, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1517.jpg\n",
      "1517\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([113, 4])\n",
      "Pooled features size: torch.Size([113, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/717.jpg\n",
      "717\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([116, 4])\n",
      "Pooled features size: torch.Size([116, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1327.jpg\n",
      "1327\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([120, 4])\n",
      "Pooled features size: torch.Size([120, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/218.jpg\n",
      "218\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([180, 4])\n",
      "Pooled features size: torch.Size([180, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/143.jpg\n",
      "143\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([145, 4])\n",
      "Pooled features size: torch.Size([145, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/703.jpg\n",
      "703\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([121, 4])\n",
      "Pooled features size: torch.Size([121, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/441.jpg\n",
      "441\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([134, 4])\n",
      "Pooled features size: torch.Size([134, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1373.jpg\n",
      "1373\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([148, 4])\n",
      "Pooled features size: torch.Size([148, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1604.jpg\n",
      "1604\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([175, 4])\n",
      "Pooled features size: torch.Size([175, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/880.jpg\n",
      "880\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([123, 4])\n",
      "Pooled features size: torch.Size([123, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/224.jpg\n",
      "224\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([140, 4])\n",
      "Pooled features size: torch.Size([140, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1797.jpg\n",
      "1797\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([136, 4])\n",
      "Pooled features size: torch.Size([136, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1880.jpg\n",
      "1880\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([121, 4])\n",
      "Pooled features size: torch.Size([121, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/621.jpg\n",
      "621\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([137, 4])\n",
      "Pooled features size: torch.Size([137, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1668.jpg\n",
      "1668\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([145, 4])\n",
      "Pooled features size: torch.Size([145, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/670.jpg\n",
      "670\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([136, 4])\n",
      "Pooled features size: torch.Size([136, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1371.jpg\n",
      "1371\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([123, 4])\n",
      "Pooled features size: torch.Size([123, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/645.jpg\n",
      "645\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([137, 4])\n",
      "Pooled features size: torch.Size([137, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/743.jpg\n",
      "743\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([136, 4])\n",
      "Pooled features size: torch.Size([136, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/94.jpg\n",
      "94\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([128, 4])\n",
      "Pooled features size: torch.Size([128, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/375.jpg\n",
      "375\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([126, 4])\n",
      "Pooled features size: torch.Size([126, 2048])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/398.jpg\n",
      "398\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([134, 4])\n",
      "Pooled features size: torch.Size([134, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/625.jpg\n",
      "625\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([169, 4])\n",
      "Pooled features size: torch.Size([169, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1017.jpg\n",
      "1017\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([167, 4])\n",
      "Pooled features size: torch.Size([167, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1370.jpg\n",
      "1370\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([138, 4])\n",
      "Pooled features size: torch.Size([138, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1131.jpg\n",
      "1131\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([137, 4])\n",
      "Pooled features size: torch.Size([137, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/142.jpg\n",
      "142\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([134, 4])\n",
      "Pooled features size: torch.Size([134, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1802.jpg\n",
      "1802\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([129, 4])\n",
      "Pooled features size: torch.Size([129, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/59.jpg\n",
      "59\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([140, 4])\n",
      "Pooled features size: torch.Size([140, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1893.jpg\n",
      "1893\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([141, 4])\n",
      "Pooled features size: torch.Size([141, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1258.jpg\n",
      "1258\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([171, 4])\n",
      "Pooled features size: torch.Size([171, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1537.jpg\n",
      "1537\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([136, 4])\n",
      "Pooled features size: torch.Size([136, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1085.jpg\n",
      "1085\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([123, 4])\n",
      "Pooled features size: torch.Size([123, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/766.jpg\n",
      "766\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([132, 4])\n",
      "Pooled features size: torch.Size([132, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1777.jpg\n",
      "1777\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([125, 4])\n",
      "Pooled features size: torch.Size([125, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1055.jpg\n",
      "1055\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([136, 4])\n",
      "Pooled features size: torch.Size([136, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1150.jpg\n",
      "1150\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([119, 4])\n",
      "Pooled features size: torch.Size([119, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1516.jpg\n",
      "1516\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([123, 4])\n",
      "Pooled features size: torch.Size([123, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/317.jpg\n",
      "317\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([141, 4])\n",
      "Pooled features size: torch.Size([141, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/653.jpg\n",
      "653\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([130, 4])\n",
      "Pooled features size: torch.Size([130, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/695.jpg\n",
      "695\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([131, 4])\n",
      "Pooled features size: torch.Size([131, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/634.jpg\n",
      "634\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([149, 4])\n",
      "Pooled features size: torch.Size([149, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1780.jpg\n",
      "1780\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([147, 4])\n",
      "Pooled features size: torch.Size([147, 2048])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/191.jpg\n",
      "191\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([133, 4])\n",
      "Pooled features size: torch.Size([133, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/524.jpg\n",
      "524\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([150, 4])\n",
      "Pooled features size: torch.Size([150, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/442.jpg\n",
      "442\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([131, 4])\n",
      "Pooled features size: torch.Size([131, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1557.jpg\n",
      "1557\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([151, 4])\n",
      "Pooled features size: torch.Size([151, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/519.jpg\n",
      "519\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([152, 4])\n",
      "Pooled features size: torch.Size([152, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1808.jpg\n",
      "1808\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([117, 4])\n",
      "Pooled features size: torch.Size([117, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1591.jpg\n",
      "1591\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([123, 4])\n",
      "Pooled features size: torch.Size([123, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1843.jpg\n",
      "1843\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([125, 4])\n",
      "Pooled features size: torch.Size([125, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/443.jpg\n",
      "443\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([155, 4])\n",
      "Pooled features size: torch.Size([155, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/363.jpg\n",
      "363\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([114, 4])\n",
      "Pooled features size: torch.Size([114, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1860.jpg\n",
      "1860\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([130, 4])\n",
      "Pooled features size: torch.Size([130, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1493.jpg\n",
      "1493\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([137, 4])\n",
      "Pooled features size: torch.Size([137, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/400.jpg\n",
      "400\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([136, 4])\n",
      "Pooled features size: torch.Size([136, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1858.jpg\n",
      "1858\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([137, 4])\n",
      "Pooled features size: torch.Size([137, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1522.jpg\n",
      "1522\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([135, 4])\n",
      "Pooled features size: torch.Size([135, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/276.jpg\n",
      "276\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([156, 4])\n",
      "Pooled features size: torch.Size([156, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1525.jpg\n",
      "1525\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([151, 4])\n",
      "Pooled features size: torch.Size([151, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/769.jpg\n",
      "769\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([122, 4])\n",
      "Pooled features size: torch.Size([122, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/74.jpg\n",
      "74\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([143, 4])\n",
      "Pooled features size: torch.Size([143, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/256.jpg\n",
      "256\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([147, 4])\n",
      "Pooled features size: torch.Size([147, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1289.jpg\n",
      "1289\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([127, 4])\n",
      "Pooled features size: torch.Size([127, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1133.jpg\n",
      "1133\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([154, 4])\n",
      "Pooled features size: torch.Size([154, 2048])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1206.jpg\n",
      "1206\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([134, 4])\n",
      "Pooled features size: torch.Size([134, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1807.jpg\n",
      "1807\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([127, 4])\n",
      "Pooled features size: torch.Size([127, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1302.jpg\n",
      "1302\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([126, 4])\n",
      "Pooled features size: torch.Size([126, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/525.jpg\n",
      "525\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([125, 4])\n",
      "Pooled features size: torch.Size([125, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1492.jpg\n",
      "1492\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([154, 4])\n",
      "Pooled features size: torch.Size([154, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1213.jpg\n",
      "1213\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([119, 4])\n",
      "Pooled features size: torch.Size([119, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/656.jpg\n",
      "656\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([128, 4])\n",
      "Pooled features size: torch.Size([128, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1071.jpg\n",
      "1071\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([130, 4])\n",
      "Pooled features size: torch.Size([130, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1355.jpg\n",
      "1355\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([116, 4])\n",
      "Pooled features size: torch.Size([116, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/885.jpg\n",
      "885\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([127, 4])\n",
      "Pooled features size: torch.Size([127, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/661.jpg\n",
      "661\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([161, 4])\n",
      "Pooled features size: torch.Size([161, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1830.jpg\n",
      "1830\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([155, 4])\n",
      "Pooled features size: torch.Size([155, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1291.jpg\n",
      "1291\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([112, 4])\n",
      "Pooled features size: torch.Size([112, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/796.jpg\n",
      "796\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([164, 4])\n",
      "Pooled features size: torch.Size([164, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1615.jpg\n",
      "1615\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([143, 4])\n",
      "Pooled features size: torch.Size([143, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1443.jpg\n",
      "1443\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([143, 4])\n",
      "Pooled features size: torch.Size([143, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1889.jpg\n",
      "1889\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([121, 4])\n",
      "Pooled features size: torch.Size([121, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/946.jpg\n",
      "946\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([154, 4])\n",
      "Pooled features size: torch.Size([154, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1708.jpg\n",
      "1708\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([155, 4])\n",
      "Pooled features size: torch.Size([155, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1378.jpg\n",
      "1378\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([135, 4])\n",
      "Pooled features size: torch.Size([135, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1438.jpg\n",
      "1438\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([148, 4])\n",
      "Pooled features size: torch.Size([148, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1852.jpg\n",
      "1852\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([133, 4])\n",
      "Pooled features size: torch.Size([133, 2048])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/515.jpg\n",
      "515\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([126, 4])\n",
      "Pooled features size: torch.Size([126, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/76.jpg\n",
      "76\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([163, 4])\n",
      "Pooled features size: torch.Size([163, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/814.jpg\n",
      "814\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([150, 4])\n",
      "Pooled features size: torch.Size([150, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/230.jpg\n",
      "230\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([125, 4])\n",
      "Pooled features size: torch.Size([125, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1751.jpg\n",
      "1751\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([135, 4])\n",
      "Pooled features size: torch.Size([135, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1095.jpg\n",
      "1095\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([142, 4])\n",
      "Pooled features size: torch.Size([142, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/410.jpg\n",
      "410\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([129, 4])\n",
      "Pooled features size: torch.Size([129, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1695.jpg\n",
      "1695\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([126, 4])\n",
      "Pooled features size: torch.Size([126, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1157.jpg\n",
      "1157\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([167, 4])\n",
      "Pooled features size: torch.Size([167, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1714.jpg\n",
      "1714\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([147, 4])\n",
      "Pooled features size: torch.Size([147, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/788.jpg\n",
      "788\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([197, 4])\n",
      "Pooled features size: torch.Size([197, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/364.jpg\n",
      "364\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([144, 4])\n",
      "Pooled features size: torch.Size([144, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1776.jpg\n",
      "1776\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([149, 4])\n",
      "Pooled features size: torch.Size([149, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1644.jpg\n",
      "1644\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([141, 4])\n",
      "Pooled features size: torch.Size([141, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/821.jpg\n",
      "821\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([160, 4])\n",
      "Pooled features size: torch.Size([160, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/665.jpg\n",
      "665\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([123, 4])\n",
      "Pooled features size: torch.Size([123, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1464.jpg\n",
      "1464\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([127, 4])\n",
      "Pooled features size: torch.Size([127, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1683.jpg\n",
      "1683\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([143, 4])\n",
      "Pooled features size: torch.Size([143, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/8.jpg\n",
      "8\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([155, 4])\n",
      "Pooled features size: torch.Size([155, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/535.jpg\n",
      "535\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([130, 4])\n",
      "Pooled features size: torch.Size([130, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1816.jpg\n",
      "1816\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([125, 4])\n",
      "Pooled features size: torch.Size([125, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/959.jpg\n",
      "959\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([133, 4])\n",
      "Pooled features size: torch.Size([133, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/315.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "315\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([162, 4])\n",
      "Pooled features size: torch.Size([162, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1399.jpg\n",
      "1399\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([128, 4])\n",
      "Pooled features size: torch.Size([128, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1049.jpg\n",
      "1049\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([124, 4])\n",
      "Pooled features size: torch.Size([124, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1031.jpg\n",
      "1031\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([136, 4])\n",
      "Pooled features size: torch.Size([136, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/907.jpg\n",
      "907\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([149, 4])\n",
      "Pooled features size: torch.Size([149, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/128.jpg\n",
      "128\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([139, 4])\n",
      "Pooled features size: torch.Size([139, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1487.jpg\n",
      "1487\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([126, 4])\n",
      "Pooled features size: torch.Size([126, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/95.jpg\n",
      "95\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([133, 4])\n",
      "Pooled features size: torch.Size([133, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1454.jpg\n",
      "1454\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([137, 4])\n",
      "Pooled features size: torch.Size([137, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1839.jpg\n",
      "1839\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([145, 4])\n",
      "Pooled features size: torch.Size([145, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/910.jpg\n",
      "910\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([146, 4])\n",
      "Pooled features size: torch.Size([146, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1284.jpg\n",
      "1284\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([136, 4])\n",
      "Pooled features size: torch.Size([136, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/437.jpg\n",
      "437\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([152, 4])\n",
      "Pooled features size: torch.Size([152, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/90.jpg\n",
      "90\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([125, 4])\n",
      "Pooled features size: torch.Size([125, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/672.jpg\n",
      "672\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([133, 4])\n",
      "Pooled features size: torch.Size([133, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1647.jpg\n",
      "1647\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([144, 4])\n",
      "Pooled features size: torch.Size([144, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1269.jpg\n",
      "1269\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([129, 4])\n",
      "Pooled features size: torch.Size([129, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1321.jpg\n",
      "1321\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([142, 4])\n",
      "Pooled features size: torch.Size([142, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1703.jpg\n",
      "1703\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([164, 4])\n",
      "Pooled features size: torch.Size([164, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/268.jpg\n",
      "268\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([147, 4])\n",
      "Pooled features size: torch.Size([147, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/424.jpg\n",
      "424\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([119, 4])\n",
      "Pooled features size: torch.Size([119, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1306.jpg\n",
      "1306\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([145, 4])\n",
      "Pooled features size: torch.Size([145, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/600.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([126, 4])\n",
      "Pooled features size: torch.Size([126, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1105.jpg\n",
      "1105\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([135, 4])\n",
      "Pooled features size: torch.Size([135, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1205.jpg\n",
      "1205\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([139, 4])\n",
      "Pooled features size: torch.Size([139, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1649.jpg\n",
      "1649\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([121, 4])\n",
      "Pooled features size: torch.Size([121, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/624.jpg\n",
      "624\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([141, 4])\n",
      "Pooled features size: torch.Size([141, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/488.jpg\n",
      "488\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([141, 4])\n",
      "Pooled features size: torch.Size([141, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1276.jpg\n",
      "1276\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([120, 4])\n",
      "Pooled features size: torch.Size([120, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/778.jpg\n",
      "778\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([137, 4])\n",
      "Pooled features size: torch.Size([137, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1014.jpg\n",
      "1014\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([160, 4])\n",
      "Pooled features size: torch.Size([160, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/513.jpg\n",
      "513\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([132, 4])\n",
      "Pooled features size: torch.Size([132, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/147.jpg\n",
      "147\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([150, 4])\n",
      "Pooled features size: torch.Size([150, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1609.jpg\n",
      "1609\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([150, 4])\n",
      "Pooled features size: torch.Size([150, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/253.jpg\n",
      "253\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([150, 4])\n",
      "Pooled features size: torch.Size([150, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1350.jpg\n",
      "1350\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([131, 4])\n",
      "Pooled features size: torch.Size([131, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/214.jpg\n",
      "214\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([131, 4])\n",
      "Pooled features size: torch.Size([131, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/225.jpg\n",
      "225\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([159, 4])\n",
      "Pooled features size: torch.Size([159, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/913.jpg\n",
      "913\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([136, 4])\n",
      "Pooled features size: torch.Size([136, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1242.jpg\n",
      "1242\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([123, 4])\n",
      "Pooled features size: torch.Size([123, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1180.jpg\n",
      "1180\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([143, 4])\n",
      "Pooled features size: torch.Size([143, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/691.jpg\n",
      "691\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([123, 4])\n",
      "Pooled features size: torch.Size([123, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1156.jpg\n",
      "1156\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([145, 4])\n",
      "Pooled features size: torch.Size([145, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/101.jpg\n",
      "101\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([108, 4])\n",
      "Pooled features size: torch.Size([108, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/567.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "567\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([141, 4])\n",
      "Pooled features size: torch.Size([141, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1465.jpg\n",
      "1465\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([168, 4])\n",
      "Pooled features size: torch.Size([168, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/883.jpg\n",
      "883\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([134, 4])\n",
      "Pooled features size: torch.Size([134, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/349.jpg\n",
      "349\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([135, 4])\n",
      "Pooled features size: torch.Size([135, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1326.jpg\n",
      "1326\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([150, 4])\n",
      "Pooled features size: torch.Size([150, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/731.jpg\n",
      "731\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([146, 4])\n",
      "Pooled features size: torch.Size([146, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/603.jpg\n",
      "603\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([125, 4])\n",
      "Pooled features size: torch.Size([125, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/983.jpg\n",
      "983\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([134, 4])\n",
      "Pooled features size: torch.Size([134, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1539.jpg\n",
      "1539\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([150, 4])\n",
      "Pooled features size: torch.Size([150, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1064.jpg\n",
      "1064\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([133, 4])\n",
      "Pooled features size: torch.Size([133, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1766.jpg\n",
      "1766\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([124, 4])\n",
      "Pooled features size: torch.Size([124, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1718.jpg\n",
      "1718\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([160, 4])\n",
      "Pooled features size: torch.Size([160, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/277.jpg\n",
      "277\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([129, 4])\n",
      "Pooled features size: torch.Size([129, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1862.jpg\n",
      "1862\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([157, 4])\n",
      "Pooled features size: torch.Size([157, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1824.jpg\n",
      "1824\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([143, 4])\n",
      "Pooled features size: torch.Size([143, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/281.jpg\n",
      "281\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([135, 4])\n",
      "Pooled features size: torch.Size([135, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1374.jpg\n",
      "1374\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([177, 4])\n",
      "Pooled features size: torch.Size([177, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1478.jpg\n",
      "1478\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([153, 4])\n",
      "Pooled features size: torch.Size([153, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1124.jpg\n",
      "1124\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([135, 4])\n",
      "Pooled features size: torch.Size([135, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/425.jpg\n",
      "425\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([156, 4])\n",
      "Pooled features size: torch.Size([156, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/632.jpg\n",
      "632\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([135, 4])\n",
      "Pooled features size: torch.Size([135, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1629.jpg\n",
      "1629\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([97, 4])\n",
      "Pooled features size: torch.Size([97, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/895.jpg\n",
      "895\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([130, 4])\n",
      "Pooled features size: torch.Size([130, 2048])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1878.jpg\n",
      "1878\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([144, 4])\n",
      "Pooled features size: torch.Size([144, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1625.jpg\n",
      "1625\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([145, 4])\n",
      "Pooled features size: torch.Size([145, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/432.jpg\n",
      "432\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([142, 4])\n",
      "Pooled features size: torch.Size([142, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/226.jpg\n",
      "226\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([128, 4])\n",
      "Pooled features size: torch.Size([128, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/580.jpg\n",
      "580\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([125, 4])\n",
      "Pooled features size: torch.Size([125, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/547.jpg\n",
      "547\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([147, 4])\n",
      "Pooled features size: torch.Size([147, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1406.jpg\n",
      "1406\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([123, 4])\n",
      "Pooled features size: torch.Size([123, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/790.jpg\n",
      "790\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([139, 4])\n",
      "Pooled features size: torch.Size([139, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/313.jpg\n",
      "313\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([130, 4])\n",
      "Pooled features size: torch.Size([130, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1660.jpg\n",
      "1660\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([129, 4])\n",
      "Pooled features size: torch.Size([129, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/311.jpg\n",
      "311\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([132, 4])\n",
      "Pooled features size: torch.Size([132, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1405.jpg\n",
      "1405\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([199, 4])\n",
      "Pooled features size: torch.Size([199, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1121.jpg\n",
      "1121\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([137, 4])\n",
      "Pooled features size: torch.Size([137, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/446.jpg\n",
      "446\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([132, 4])\n",
      "Pooled features size: torch.Size([132, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/309.jpg\n",
      "309\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([152, 4])\n",
      "Pooled features size: torch.Size([152, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/863.jpg\n",
      "863\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([137, 4])\n",
      "Pooled features size: torch.Size([137, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/644.jpg\n",
      "644\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([146, 4])\n",
      "Pooled features size: torch.Size([146, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/992.jpg\n",
      "992\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([128, 4])\n",
      "Pooled features size: torch.Size([128, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/3.jpg\n",
      "3\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([134, 4])\n",
      "Pooled features size: torch.Size([134, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/19.jpg\n",
      "19\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([167, 4])\n",
      "Pooled features size: torch.Size([167, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/241.jpg\n",
      "241\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([133, 4])\n",
      "Pooled features size: torch.Size([133, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/384.jpg\n",
      "384\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([140, 4])\n",
      "Pooled features size: torch.Size([140, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/607.jpg\n",
      "607\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([124, 4])\n",
      "Pooled features size: torch.Size([124, 2048])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/761.jpg\n",
      "761\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([138, 4])\n",
      "Pooled features size: torch.Size([138, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1310.jpg\n",
      "1310\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([178, 4])\n",
      "Pooled features size: torch.Size([178, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/679.jpg\n",
      "679\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([156, 4])\n",
      "Pooled features size: torch.Size([156, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1025.jpg\n",
      "1025\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([143, 4])\n",
      "Pooled features size: torch.Size([143, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/387.jpg\n",
      "387\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([125, 4])\n",
      "Pooled features size: torch.Size([125, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/145.jpg\n",
      "145\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([130, 4])\n",
      "Pooled features size: torch.Size([130, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/851.jpg\n",
      "851\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([116, 4])\n",
      "Pooled features size: torch.Size([116, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/202.jpg\n",
      "202\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([128, 4])\n",
      "Pooled features size: torch.Size([128, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/26.jpg\n",
      "26\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([129, 4])\n",
      "Pooled features size: torch.Size([129, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1850.jpg\n",
      "1850\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([137, 4])\n",
      "Pooled features size: torch.Size([137, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1429.jpg\n",
      "1429\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([130, 4])\n",
      "Pooled features size: torch.Size([130, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1499.jpg\n",
      "1499\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([153, 4])\n",
      "Pooled features size: torch.Size([153, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1726.jpg\n",
      "1726\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([163, 4])\n",
      "Pooled features size: torch.Size([163, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/50.jpg\n",
      "50\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([148, 4])\n",
      "Pooled features size: torch.Size([148, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1603.jpg\n",
      "1603\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([132, 4])\n",
      "Pooled features size: torch.Size([132, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/901.jpg\n",
      "901\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([122, 4])\n",
      "Pooled features size: torch.Size([122, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/157.jpg\n",
      "157\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([167, 4])\n",
      "Pooled features size: torch.Size([167, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1643.jpg\n",
      "1643\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([130, 4])\n",
      "Pooled features size: torch.Size([130, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/928.jpg\n",
      "928\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([146, 4])\n",
      "Pooled features size: torch.Size([146, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1552.jpg\n",
      "1552\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([148, 4])\n",
      "Pooled features size: torch.Size([148, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1225.jpg\n",
      "1225\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([137, 4])\n",
      "Pooled features size: torch.Size([137, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1100.jpg\n",
      "1100\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([133, 4])\n",
      "Pooled features size: torch.Size([133, 2048])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1070.jpg\n",
      "1070\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([138, 4])\n",
      "Pooled features size: torch.Size([138, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/33.jpg\n",
      "33\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([153, 4])\n",
      "Pooled features size: torch.Size([153, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1028.jpg\n",
      "1028\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([121, 4])\n",
      "Pooled features size: torch.Size([121, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1736.jpg\n",
      "1736\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([147, 4])\n",
      "Pooled features size: torch.Size([147, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/887.jpg\n",
      "887\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([126, 4])\n",
      "Pooled features size: torch.Size([126, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/728.jpg\n",
      "728\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([153, 4])\n",
      "Pooled features size: torch.Size([153, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1723.jpg\n",
      "1723\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([128, 4])\n",
      "Pooled features size: torch.Size([128, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/674.jpg\n",
      "674\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([152, 4])\n",
      "Pooled features size: torch.Size([152, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1576.jpg\n",
      "1576\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([145, 4])\n",
      "Pooled features size: torch.Size([145, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1759.jpg\n",
      "1759\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([164, 4])\n",
      "Pooled features size: torch.Size([164, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/847.jpg\n",
      "847\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([152, 4])\n",
      "Pooled features size: torch.Size([152, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/706.jpg\n",
      "706\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([126, 4])\n",
      "Pooled features size: torch.Size([126, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/620.jpg\n",
      "620\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([156, 4])\n",
      "Pooled features size: torch.Size([156, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/687.jpg\n",
      "687\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([118, 4])\n",
      "Pooled features size: torch.Size([118, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/740.jpg\n",
      "740\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([148, 4])\n",
      "Pooled features size: torch.Size([148, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1826.jpg\n",
      "1826\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([131, 4])\n",
      "Pooled features size: torch.Size([131, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/293.jpg\n",
      "293\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([134, 4])\n",
      "Pooled features size: torch.Size([134, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/379.jpg\n",
      "379\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([137, 4])\n",
      "Pooled features size: torch.Size([137, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1844.jpg\n",
      "1844\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([130, 4])\n",
      "Pooled features size: torch.Size([130, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1778.jpg\n",
      "1778\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([125, 4])\n",
      "Pooled features size: torch.Size([125, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/234.jpg\n",
      "234\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([155, 4])\n",
      "Pooled features size: torch.Size([155, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/733.jpg\n",
      "733\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([137, 4])\n",
      "Pooled features size: torch.Size([137, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/264.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "264\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([109, 4])\n",
      "Pooled features size: torch.Size([109, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/339.jpg\n",
      "339\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([131, 4])\n",
      "Pooled features size: torch.Size([131, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/856.jpg\n",
      "856\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([146, 4])\n",
      "Pooled features size: torch.Size([146, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/927.jpg\n",
      "927\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([142, 4])\n",
      "Pooled features size: torch.Size([142, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/205.jpg\n",
      "205\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([128, 4])\n",
      "Pooled features size: torch.Size([128, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1750.jpg\n",
      "1750\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([149, 4])\n",
      "Pooled features size: torch.Size([149, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1268.jpg\n",
      "1268\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([146, 4])\n",
      "Pooled features size: torch.Size([146, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1803.jpg\n",
      "1803\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([131, 4])\n",
      "Pooled features size: torch.Size([131, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1515.jpg\n",
      "1515\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([153, 4])\n",
      "Pooled features size: torch.Size([153, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/747.jpg\n",
      "747\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([116, 4])\n",
      "Pooled features size: torch.Size([116, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/58.jpg\n",
      "58\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([129, 4])\n",
      "Pooled features size: torch.Size([129, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1104.jpg\n",
      "1104\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([121, 4])\n",
      "Pooled features size: torch.Size([121, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1568.jpg\n",
      "1568\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([124, 4])\n",
      "Pooled features size: torch.Size([124, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1007.jpg\n",
      "1007\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([117, 4])\n",
      "Pooled features size: torch.Size([117, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/336.jpg\n",
      "336\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([108, 4])\n",
      "Pooled features size: torch.Size([108, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1474.jpg\n",
      "1474\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([150, 4])\n",
      "Pooled features size: torch.Size([150, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/456.jpg\n",
      "456\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([154, 4])\n",
      "Pooled features size: torch.Size([154, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/370.jpg\n",
      "370\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([134, 4])\n",
      "Pooled features size: torch.Size([134, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1743.jpg\n",
      "1743\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([130, 4])\n",
      "Pooled features size: torch.Size([130, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/757.jpg\n",
      "757\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([117, 4])\n",
      "Pooled features size: torch.Size([117, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/324.jpg\n",
      "324\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([129, 4])\n",
      "Pooled features size: torch.Size([129, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/161.jpg\n",
      "161\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([172, 4])\n",
      "Pooled features size: torch.Size([172, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/466.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "466\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([136, 4])\n",
      "Pooled features size: torch.Size([136, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/692.jpg\n",
      "692\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([132, 4])\n",
      "Pooled features size: torch.Size([132, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1296.jpg\n",
      "1296\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([137, 4])\n",
      "Pooled features size: torch.Size([137, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1912.jpg\n",
      "1912\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([161, 4])\n",
      "Pooled features size: torch.Size([161, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1417.jpg\n",
      "1417\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([125, 4])\n",
      "Pooled features size: torch.Size([125, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1671.jpg\n",
      "1671\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([140, 4])\n",
      "Pooled features size: torch.Size([140, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/350.jpg\n",
      "350\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([144, 4])\n",
      "Pooled features size: torch.Size([144, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1364.jpg\n",
      "1364\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([128, 4])\n",
      "Pooled features size: torch.Size([128, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1435.jpg\n",
      "1435\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([136, 4])\n",
      "Pooled features size: torch.Size([136, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/881.jpg\n",
      "881\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([148, 4])\n",
      "Pooled features size: torch.Size([148, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/381.jpg\n",
      "381\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([122, 4])\n",
      "Pooled features size: torch.Size([122, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/27.jpg\n",
      "27\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([142, 4])\n",
      "Pooled features size: torch.Size([142, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1666.jpg\n",
      "1666\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([156, 4])\n",
      "Pooled features size: torch.Size([156, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1146.jpg\n",
      "1146\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([132, 4])\n",
      "Pooled features size: torch.Size([132, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/235.jpg\n",
      "235\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([134, 4])\n",
      "Pooled features size: torch.Size([134, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/637.jpg\n",
      "637\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([140, 4])\n",
      "Pooled features size: torch.Size([140, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1498.jpg\n",
      "1498\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([169, 4])\n",
      "Pooled features size: torch.Size([169, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1472.jpg\n",
      "1472\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([139, 4])\n",
      "Pooled features size: torch.Size([139, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1873.jpg\n",
      "1873\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([150, 4])\n",
      "Pooled features size: torch.Size([150, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/643.jpg\n",
      "643\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([151, 4])\n",
      "Pooled features size: torch.Size([151, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1650.jpg\n",
      "1650\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([125, 4])\n",
      "Pooled features size: torch.Size([125, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/613.jpg\n",
      "613\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([117, 4])\n",
      "Pooled features size: torch.Size([117, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/83.jpg\n",
      "83\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proposal Boxes size: torch.Size([159, 4])\n",
      "Pooled features size: torch.Size([159, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1317.jpg\n",
      "1317\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([112, 4])\n",
      "Pooled features size: torch.Size([112, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/329.jpg\n",
      "329\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([169, 4])\n",
      "Pooled features size: torch.Size([169, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/719.jpg\n",
      "719\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([126, 4])\n",
      "Pooled features size: torch.Size([126, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1496.jpg\n",
      "1496\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([147, 4])\n",
      "Pooled features size: torch.Size([147, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/583.jpg\n",
      "583\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([138, 4])\n",
      "Pooled features size: torch.Size([138, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1187.jpg\n",
      "1187\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([129, 4])\n",
      "Pooled features size: torch.Size([129, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1407.jpg\n",
      "1407\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([116, 4])\n",
      "Pooled features size: torch.Size([116, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/791.jpg\n",
      "791\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([137, 4])\n",
      "Pooled features size: torch.Size([137, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/819.jpg\n",
      "819\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([126, 4])\n",
      "Pooled features size: torch.Size([126, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1712.jpg\n",
      "1712\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([139, 4])\n",
      "Pooled features size: torch.Size([139, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/334.jpg\n",
      "334\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([128, 4])\n",
      "Pooled features size: torch.Size([128, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/635.jpg\n",
      "635\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([132, 4])\n",
      "Pooled features size: torch.Size([132, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1806.jpg\n",
      "1806\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([175, 4])\n",
      "Pooled features size: torch.Size([175, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/865.jpg\n",
      "865\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([135, 4])\n",
      "Pooled features size: torch.Size([135, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1362.jpg\n",
      "1362\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([140, 4])\n",
      "Pooled features size: torch.Size([140, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/6.jpg\n",
      "6\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([138, 4])\n",
      "Pooled features size: torch.Size([138, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/875.jpg\n",
      "875\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([166, 4])\n",
      "Pooled features size: torch.Size([166, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/537.jpg\n",
      "537\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([142, 4])\n",
      "Pooled features size: torch.Size([142, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1638.jpg\n",
      "1638\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([134, 4])\n",
      "Pooled features size: torch.Size([134, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/764.jpg\n",
      "764\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([154, 4])\n",
      "Pooled features size: torch.Size([154, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/762.jpg\n",
      "762\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([134, 4])\n",
      "Pooled features size: torch.Size([134, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1502.jpg\n",
      "1502\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([129, 4])\n",
      "Pooled features size: torch.Size([129, 2048])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1646.jpg\n",
      "1646\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([130, 4])\n",
      "Pooled features size: torch.Size([130, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/697.jpg\n",
      "697\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([142, 4])\n",
      "Pooled features size: torch.Size([142, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/793.jpg\n",
      "793\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([117, 4])\n",
      "Pooled features size: torch.Size([117, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/447.jpg\n",
      "447\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([131, 4])\n",
      "Pooled features size: torch.Size([131, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1555.jpg\n",
      "1555\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([129, 4])\n",
      "Pooled features size: torch.Size([129, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/872.jpg\n",
      "872\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([148, 4])\n",
      "Pooled features size: torch.Size([148, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1035.jpg\n",
      "1035\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([123, 4])\n",
      "Pooled features size: torch.Size([123, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1297.jpg\n",
      "1297\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([132, 4])\n",
      "Pooled features size: torch.Size([132, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1280.jpg\n",
      "1280\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([131, 4])\n",
      "Pooled features size: torch.Size([131, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/318.jpg\n",
      "318\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([132, 4])\n",
      "Pooled features size: torch.Size([132, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/305.jpg\n",
      "305\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([134, 4])\n",
      "Pooled features size: torch.Size([134, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/585.jpg\n",
      "585\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([115, 4])\n",
      "Pooled features size: torch.Size([115, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1547.jpg\n",
      "1547\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([121, 4])\n",
      "Pooled features size: torch.Size([121, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/158.jpg\n",
      "158\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([123, 4])\n",
      "Pooled features size: torch.Size([123, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1348.jpg\n",
      "1348\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([134, 4])\n",
      "Pooled features size: torch.Size([134, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/878.jpg\n",
      "878\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([150, 4])\n",
      "Pooled features size: torch.Size([150, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1002.jpg\n",
      "1002\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([134, 4])\n",
      "Pooled features size: torch.Size([134, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1428.jpg\n",
      "1428\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([164, 4])\n",
      "Pooled features size: torch.Size([164, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/461.jpg\n",
      "461\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([120, 4])\n",
      "Pooled features size: torch.Size([120, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/406.jpg\n",
      "406\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([148, 4])\n",
      "Pooled features size: torch.Size([148, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1544.jpg\n",
      "1544\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([129, 4])\n",
      "Pooled features size: torch.Size([129, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1159.jpg\n",
      "1159\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([146, 4])\n",
      "Pooled features size: torch.Size([146, 2048])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1389.jpg\n",
      "1389\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([134, 4])\n",
      "Pooled features size: torch.Size([134, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/165.jpg\n",
      "165\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([121, 4])\n",
      "Pooled features size: torch.Size([121, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/768.jpg\n",
      "768\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([119, 4])\n",
      "Pooled features size: torch.Size([119, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1831.jpg\n",
      "1831\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([156, 4])\n",
      "Pooled features size: torch.Size([156, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/606.jpg\n",
      "606\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([122, 4])\n",
      "Pooled features size: torch.Size([122, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1601.jpg\n",
      "1601\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([171, 4])\n",
      "Pooled features size: torch.Size([171, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/563.jpg\n",
      "563\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([144, 4])\n",
      "Pooled features size: torch.Size([144, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1011.jpg\n",
      "1011\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([122, 4])\n",
      "Pooled features size: torch.Size([122, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1239.jpg\n",
      "1239\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([120, 4])\n",
      "Pooled features size: torch.Size([120, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1278.jpg\n",
      "1278\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([149, 4])\n",
      "Pooled features size: torch.Size([149, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/237.jpg\n",
      "237\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([144, 4])\n",
      "Pooled features size: torch.Size([144, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/49.jpg\n",
      "49\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([128, 4])\n",
      "Pooled features size: torch.Size([128, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/118.jpg\n",
      "118\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([148, 4])\n",
      "Pooled features size: torch.Size([148, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1027.jpg\n",
      "1027\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([117, 4])\n",
      "Pooled features size: torch.Size([117, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1526.jpg\n",
      "1526\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([145, 4])\n",
      "Pooled features size: torch.Size([145, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1710.jpg\n",
      "1710\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([119, 4])\n",
      "Pooled features size: torch.Size([119, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/955.jpg\n",
      "955\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([135, 4])\n",
      "Pooled features size: torch.Size([135, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/782.jpg\n",
      "782\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([161, 4])\n",
      "Pooled features size: torch.Size([161, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1859.jpg\n",
      "1859\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([149, 4])\n",
      "Pooled features size: torch.Size([149, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1352.jpg\n",
      "1352\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([175, 4])\n",
      "Pooled features size: torch.Size([175, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/465.jpg\n",
      "465\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([172, 4])\n",
      "Pooled features size: torch.Size([172, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/727.jpg\n",
      "727\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([123, 4])\n",
      "Pooled features size: torch.Size([123, 2048])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1047.jpg\n",
      "1047\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([117, 4])\n",
      "Pooled features size: torch.Size([117, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/359.jpg\n",
      "359\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([150, 4])\n",
      "Pooled features size: torch.Size([150, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/289.jpg\n",
      "289\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([147, 4])\n",
      "Pooled features size: torch.Size([147, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/103.jpg\n",
      "103\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([140, 4])\n",
      "Pooled features size: torch.Size([140, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/633.jpg\n",
      "633\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([130, 4])\n",
      "Pooled features size: torch.Size([130, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1097.jpg\n",
      "1097\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([143, 4])\n",
      "Pooled features size: torch.Size([143, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/655.jpg\n",
      "655\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([146, 4])\n",
      "Pooled features size: torch.Size([146, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1786.jpg\n",
      "1786\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([131, 4])\n",
      "Pooled features size: torch.Size([131, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1333.jpg\n",
      "1333\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([135, 4])\n",
      "Pooled features size: torch.Size([135, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1298.jpg\n",
      "1298\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([141, 4])\n",
      "Pooled features size: torch.Size([141, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/122.jpg\n",
      "122\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([158, 4])\n",
      "Pooled features size: torch.Size([158, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/199.jpg\n",
      "199\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([131, 4])\n",
      "Pooled features size: torch.Size([131, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/550.jpg\n",
      "550\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([147, 4])\n",
      "Pooled features size: torch.Size([147, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/676.jpg\n",
      "676\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([128, 4])\n",
      "Pooled features size: torch.Size([128, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1828.jpg\n",
      "1828\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([133, 4])\n",
      "Pooled features size: torch.Size([133, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/112.jpg\n",
      "112\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([130, 4])\n",
      "Pooled features size: torch.Size([130, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1482.jpg\n",
      "1482\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([162, 4])\n",
      "Pooled features size: torch.Size([162, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1613.jpg\n",
      "1613\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([126, 4])\n",
      "Pooled features size: torch.Size([126, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/182.jpg\n",
      "182\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([136, 4])\n",
      "Pooled features size: torch.Size([136, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1080.jpg\n",
      "1080\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([118, 4])\n",
      "Pooled features size: torch.Size([118, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/588.jpg\n",
      "588\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([147, 4])\n",
      "Pooled features size: torch.Size([147, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1195.jpg\n",
      "1195\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([132, 4])\n",
      "Pooled features size: torch.Size([132, 2048])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/146.jpg\n",
      "146\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([119, 4])\n",
      "Pooled features size: torch.Size([119, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1318.jpg\n",
      "1318\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([147, 4])\n",
      "Pooled features size: torch.Size([147, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1770.jpg\n",
      "1770\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([147, 4])\n",
      "Pooled features size: torch.Size([147, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/686.jpg\n",
      "686\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([135, 4])\n",
      "Pooled features size: torch.Size([135, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1590.jpg\n",
      "1590\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([123, 4])\n",
      "Pooled features size: torch.Size([123, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/42.jpg\n",
      "42\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([128, 4])\n",
      "Pooled features size: torch.Size([128, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/500.jpg\n",
      "500\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([139, 4])\n",
      "Pooled features size: torch.Size([139, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1805.jpg\n",
      "1805\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([144, 4])\n",
      "Pooled features size: torch.Size([144, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/48.jpg\n",
      "48\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([105, 4])\n",
      "Pooled features size: torch.Size([105, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1145.jpg\n",
      "1145\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([142, 4])\n",
      "Pooled features size: torch.Size([142, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/252.jpg\n",
      "252\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([128, 4])\n",
      "Pooled features size: torch.Size([128, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/749.jpg\n",
      "749\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([155, 4])\n",
      "Pooled features size: torch.Size([155, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/314.jpg\n",
      "314\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([120, 4])\n",
      "Pooled features size: torch.Size([120, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/43.jpg\n",
      "43\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([138, 4])\n",
      "Pooled features size: torch.Size([138, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/243.jpg\n",
      "243\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([120, 4])\n",
      "Pooled features size: torch.Size([120, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/207.jpg\n",
      "207\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([124, 4])\n",
      "Pooled features size: torch.Size([124, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1677.jpg\n",
      "1677\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([168, 4])\n",
      "Pooled features size: torch.Size([168, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1503.jpg\n",
      "1503\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([131, 4])\n",
      "Pooled features size: torch.Size([131, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/261.jpg\n",
      "261\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([150, 4])\n",
      "Pooled features size: torch.Size([150, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/948.jpg\n",
      "948\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([165, 4])\n",
      "Pooled features size: torch.Size([165, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/61.jpg\n",
      "61\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([142, 4])\n",
      "Pooled features size: torch.Size([142, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/402.jpg\n",
      "402\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([139, 4])\n",
      "Pooled features size: torch.Size([139, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/507.jpg\n",
      "507\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([127, 4])\n",
      "Pooled features size: torch.Size([127, 2048])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/532.jpg\n",
      "532\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([141, 4])\n",
      "Pooled features size: torch.Size([141, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1196.jpg\n",
      "1196\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([118, 4])\n",
      "Pooled features size: torch.Size([118, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/669.jpg\n",
      "669\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([151, 4])\n",
      "Pooled features size: torch.Size([151, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1103.jpg\n",
      "1103\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([138, 4])\n",
      "Pooled features size: torch.Size([138, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1741.jpg\n",
      "1741\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([139, 4])\n",
      "Pooled features size: torch.Size([139, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1810.jpg\n",
      "1810\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([124, 4])\n",
      "Pooled features size: torch.Size([124, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/779.jpg\n",
      "779\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([130, 4])\n",
      "Pooled features size: torch.Size([130, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1368.jpg\n",
      "1368\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([140, 4])\n",
      "Pooled features size: torch.Size([140, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1636.jpg\n",
      "1636\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([157, 4])\n",
      "Pooled features size: torch.Size([157, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1194.jpg\n",
      "1194\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([136, 4])\n",
      "Pooled features size: torch.Size([136, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1833.jpg\n",
      "1833\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([113, 4])\n",
      "Pooled features size: torch.Size([113, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1567.jpg\n",
      "1567\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([145, 4])\n",
      "Pooled features size: torch.Size([145, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1393.jpg\n",
      "1393\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([124, 4])\n",
      "Pooled features size: torch.Size([124, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1546.jpg\n",
      "1546\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([152, 4])\n",
      "Pooled features size: torch.Size([152, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/506.jpg\n",
      "506\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([130, 4])\n",
      "Pooled features size: torch.Size([130, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1832.jpg\n",
      "1832\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([127, 4])\n",
      "Pooled features size: torch.Size([127, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/109.jpg\n",
      "109\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([133, 4])\n",
      "Pooled features size: torch.Size([133, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1234.jpg\n",
      "1234\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([130, 4])\n",
      "Pooled features size: torch.Size([130, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1827.jpg\n",
      "1827\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([128, 4])\n",
      "Pooled features size: torch.Size([128, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1728.jpg\n",
      "1728\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([128, 4])\n",
      "Pooled features size: torch.Size([128, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1391.jpg\n",
      "1391\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([140, 4])\n",
      "Pooled features size: torch.Size([140, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1301.jpg\n",
      "1301\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([120, 4])\n",
      "Pooled features size: torch.Size([120, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/231.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "231\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([126, 4])\n",
      "Pooled features size: torch.Size([126, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/926.jpg\n",
      "926\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([147, 4])\n",
      "Pooled features size: torch.Size([147, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/952.jpg\n",
      "952\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([116, 4])\n",
      "Pooled features size: torch.Size([116, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/173.jpg\n",
      "173\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([130, 4])\n",
      "Pooled features size: torch.Size([130, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1632.jpg\n",
      "1632\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([153, 4])\n",
      "Pooled features size: torch.Size([153, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/578.jpg\n",
      "578\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([124, 4])\n",
      "Pooled features size: torch.Size([124, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1221.jpg\n",
      "1221\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([129, 4])\n",
      "Pooled features size: torch.Size([129, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1600.jpg\n",
      "1600\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([131, 4])\n",
      "Pooled features size: torch.Size([131, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/206.jpg\n",
      "206\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([148, 4])\n",
      "Pooled features size: torch.Size([148, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/77.jpg\n",
      "77\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([127, 4])\n",
      "Pooled features size: torch.Size([127, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1608.jpg\n",
      "1608\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([129, 4])\n",
      "Pooled features size: torch.Size([129, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/918.jpg\n",
      "918\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([139, 4])\n",
      "Pooled features size: torch.Size([139, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/40.jpg\n",
      "40\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([119, 4])\n",
      "Pooled features size: torch.Size([119, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1170.jpg\n",
      "1170\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([141, 4])\n",
      "Pooled features size: torch.Size([141, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1772.jpg\n",
      "1772\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([138, 4])\n",
      "Pooled features size: torch.Size([138, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/531.jpg\n",
      "531\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([141, 4])\n",
      "Pooled features size: torch.Size([141, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/575.jpg\n",
      "575\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([148, 4])\n",
      "Pooled features size: torch.Size([148, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1026.jpg\n",
      "1026\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([120, 4])\n",
      "Pooled features size: torch.Size([120, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/227.jpg\n",
      "227\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([135, 4])\n",
      "Pooled features size: torch.Size([135, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1337.jpg\n",
      "1337\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([131, 4])\n",
      "Pooled features size: torch.Size([131, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/308.jpg\n",
      "308\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([143, 4])\n",
      "Pooled features size: torch.Size([143, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1137.jpg\n",
      "1137\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([122, 4])\n",
      "Pooled features size: torch.Size([122, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/780.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "780\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([138, 4])\n",
      "Pooled features size: torch.Size([138, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/39.jpg\n",
      "39\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([155, 4])\n",
      "Pooled features size: torch.Size([155, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/434.jpg\n",
      "434\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([137, 4])\n",
      "Pooled features size: torch.Size([137, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/720.jpg\n",
      "720\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([154, 4])\n",
      "Pooled features size: torch.Size([154, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1036.jpg\n",
      "1036\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([126, 4])\n",
      "Pooled features size: torch.Size([126, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/116.jpg\n",
      "116\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([135, 4])\n",
      "Pooled features size: torch.Size([135, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1569.jpg\n",
      "1569\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([130, 4])\n",
      "Pooled features size: torch.Size([130, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1745.jpg\n",
      "1745\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([138, 4])\n",
      "Pooled features size: torch.Size([138, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/106.jpg\n",
      "106\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([141, 4])\n",
      "Pooled features size: torch.Size([141, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/279.jpg\n",
      "279\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([151, 4])\n",
      "Pooled features size: torch.Size([151, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1698.jpg\n",
      "1698\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([139, 4])\n",
      "Pooled features size: torch.Size([139, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/929.jpg\n",
      "929\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([125, 4])\n",
      "Pooled features size: torch.Size([125, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/177.jpg\n",
      "177\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([121, 4])\n",
      "Pooled features size: torch.Size([121, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1345.jpg\n",
      "1345\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([137, 4])\n",
      "Pooled features size: torch.Size([137, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1272.jpg\n",
      "1272\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([124, 4])\n",
      "Pooled features size: torch.Size([124, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1273.jpg\n",
      "1273\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([129, 4])\n",
      "Pooled features size: torch.Size([129, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/700.jpg\n",
      "700\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([126, 4])\n",
      "Pooled features size: torch.Size([126, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/288.jpg\n",
      "288\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([155, 4])\n",
      "Pooled features size: torch.Size([155, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/980.jpg\n",
      "980\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([132, 4])\n",
      "Pooled features size: torch.Size([132, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/411.jpg\n",
      "411\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([144, 4])\n",
      "Pooled features size: torch.Size([144, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1895.jpg\n",
      "1895\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([162, 4])\n",
      "Pooled features size: torch.Size([162, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/892.jpg\n",
      "892\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([146, 4])\n",
      "Pooled features size: torch.Size([146, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1597.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1597\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([149, 4])\n",
      "Pooled features size: torch.Size([149, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/190.jpg\n",
      "190\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([137, 4])\n",
      "Pooled features size: torch.Size([137, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/312.jpg\n",
      "312\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([132, 4])\n",
      "Pooled features size: torch.Size([132, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/428.jpg\n",
      "428\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([167, 4])\n",
      "Pooled features size: torch.Size([167, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/659.jpg\n",
      "659\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([112, 4])\n",
      "Pooled features size: torch.Size([112, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/475.jpg\n",
      "475\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([115, 4])\n",
      "Pooled features size: torch.Size([115, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/541.jpg\n",
      "541\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([134, 4])\n",
      "Pooled features size: torch.Size([134, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1684.jpg\n",
      "1684\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([137, 4])\n",
      "Pooled features size: torch.Size([137, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/414.jpg\n",
      "414\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([136, 4])\n",
      "Pooled features size: torch.Size([136, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1281.jpg\n",
      "1281\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([143, 4])\n",
      "Pooled features size: torch.Size([143, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/805.jpg\n",
      "805\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([124, 4])\n",
      "Pooled features size: torch.Size([124, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/82.jpg\n",
      "82\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([128, 4])\n",
      "Pooled features size: torch.Size([128, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/544.jpg\n",
      "544\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([160, 4])\n",
      "Pooled features size: torch.Size([160, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1247.jpg\n",
      "1247\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([146, 4])\n",
      "Pooled features size: torch.Size([146, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1702.jpg\n",
      "1702\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([131, 4])\n",
      "Pooled features size: torch.Size([131, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1222.jpg\n",
      "1222\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([125, 4])\n",
      "Pooled features size: torch.Size([125, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1274.jpg\n",
      "1274\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([164, 4])\n",
      "Pooled features size: torch.Size([164, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1877.jpg\n",
      "1877\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([113, 4])\n",
      "Pooled features size: torch.Size([113, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/772.jpg\n",
      "772\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([124, 4])\n",
      "Pooled features size: torch.Size([124, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/319.jpg\n",
      "319\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([117, 4])\n",
      "Pooled features size: torch.Size([117, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1185.jpg\n",
      "1185\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([129, 4])\n",
      "Pooled features size: torch.Size([129, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/631.jpg\n",
      "631\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([151, 4])\n",
      "Pooled features size: torch.Size([151, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/694.jpg\n",
      "694\n",
      "Original image size:  (2160, 3840)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([113, 4])\n",
      "Pooled features size: torch.Size([113, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1495.jpg\n",
      "1495\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([138, 4])\n",
      "Pooled features size: torch.Size([138, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/944.jpg\n",
      "944\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([154, 4])\n",
      "Pooled features size: torch.Size([154, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1010.jpg\n",
      "1010\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([140, 4])\n",
      "Pooled features size: torch.Size([140, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/774.jpg\n",
      "774\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([122, 4])\n",
      "Pooled features size: torch.Size([122, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/920.jpg\n",
      "920\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([128, 4])\n",
      "Pooled features size: torch.Size([128, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/163.jpg\n",
      "163\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([144, 4])\n",
      "Pooled features size: torch.Size([144, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/29.jpg\n",
      "29\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([120, 4])\n",
      "Pooled features size: torch.Size([120, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/404.jpg\n",
      "404\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([159, 4])\n",
      "Pooled features size: torch.Size([159, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1433.jpg\n",
      "1433\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([114, 4])\n",
      "Pooled features size: torch.Size([114, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1325.jpg\n",
      "1325\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([139, 4])\n",
      "Pooled features size: torch.Size([139, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1711.jpg\n",
      "1711\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([134, 4])\n",
      "Pooled features size: torch.Size([134, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/468.jpg\n",
      "468\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([173, 4])\n",
      "Pooled features size: torch.Size([173, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/958.jpg\n",
      "958\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([149, 4])\n",
      "Pooled features size: torch.Size([149, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1879.jpg\n",
      "1879\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([136, 4])\n",
      "Pooled features size: torch.Size([136, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1834.jpg\n",
      "1834\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([161, 4])\n",
      "Pooled features size: torch.Size([161, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/203.jpg\n",
      "203\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([154, 4])\n",
      "Pooled features size: torch.Size([154, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/732.jpg\n",
      "732\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([124, 4])\n",
      "Pooled features size: torch.Size([124, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1334.jpg\n",
      "1334\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([135, 4])\n",
      "Pooled features size: torch.Size([135, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/953.jpg\n",
      "953\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([130, 4])\n",
      "Pooled features size: torch.Size([130, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1126.jpg\n",
      "1126\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([124, 4])\n",
      "Pooled features size: torch.Size([124, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1536.jpg\n",
      "1536\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([130, 4])\n",
      "Pooled features size: torch.Size([130, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1467.jpg\n",
      "1467\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([143, 4])\n",
      "Pooled features size: torch.Size([143, 2048])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/21.jpg\n",
      "21\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([124, 4])\n",
      "Pooled features size: torch.Size([124, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1255.jpg\n",
      "1255\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([142, 4])\n",
      "Pooled features size: torch.Size([142, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/509.jpg\n",
      "509\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([118, 4])\n",
      "Pooled features size: torch.Size([118, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/368.jpg\n",
      "368\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([148, 4])\n",
      "Pooled features size: torch.Size([148, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/87.jpg\n",
      "87\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([127, 4])\n",
      "Pooled features size: torch.Size([127, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1447.jpg\n",
      "1447\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([131, 4])\n",
      "Pooled features size: torch.Size([131, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/581.jpg\n",
      "581\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([161, 4])\n",
      "Pooled features size: torch.Size([161, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/841.jpg\n",
      "841\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([135, 4])\n",
      "Pooled features size: torch.Size([135, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1155.jpg\n",
      "1155\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([134, 4])\n",
      "Pooled features size: torch.Size([134, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1694.jpg\n",
      "1694\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([125, 4])\n",
      "Pooled features size: torch.Size([125, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1562.jpg\n",
      "1562\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([138, 4])\n",
      "Pooled features size: torch.Size([138, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/996.jpg\n",
      "996\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([127, 4])\n",
      "Pooled features size: torch.Size([127, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1716.jpg\n",
      "1716\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([122, 4])\n",
      "Pooled features size: torch.Size([122, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/763.jpg\n",
      "763\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([134, 4])\n",
      "Pooled features size: torch.Size([134, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/838.jpg\n",
      "838\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([136, 4])\n",
      "Pooled features size: torch.Size([136, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1207.jpg\n",
      "1207\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([139, 4])\n",
      "Pooled features size: torch.Size([139, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/718.jpg\n",
      "718\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([134, 4])\n",
      "Pooled features size: torch.Size([134, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/912.jpg\n",
      "912\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([128, 4])\n",
      "Pooled features size: torch.Size([128, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1415.jpg\n",
      "1415\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([131, 4])\n",
      "Pooled features size: torch.Size([131, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1485.jpg\n",
      "1485\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([133, 4])\n",
      "Pooled features size: torch.Size([133, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1127.jpg\n",
      "1127\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([152, 4])\n",
      "Pooled features size: torch.Size([152, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1648.jpg\n",
      "1648\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([129, 4])\n",
      "Pooled features size: torch.Size([129, 2048])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1045.jpg\n",
      "1045\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([133, 4])\n",
      "Pooled features size: torch.Size([133, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1592.jpg\n",
      "1592\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([150, 4])\n",
      "Pooled features size: torch.Size([150, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/275.jpg\n",
      "275\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([139, 4])\n",
      "Pooled features size: torch.Size([139, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1307.jpg\n",
      "1307\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([151, 4])\n",
      "Pooled features size: torch.Size([151, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/582.jpg\n",
      "582\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([153, 4])\n",
      "Pooled features size: torch.Size([153, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1550.jpg\n",
      "1550\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([119, 4])\n",
      "Pooled features size: torch.Size([119, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1505.jpg\n",
      "1505\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([124, 4])\n",
      "Pooled features size: torch.Size([124, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1237.jpg\n",
      "1237\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([133, 4])\n",
      "Pooled features size: torch.Size([133, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/10.jpg\n",
      "10\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([144, 4])\n",
      "Pooled features size: torch.Size([144, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1792.jpg\n",
      "1792\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([121, 4])\n",
      "Pooled features size: torch.Size([121, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1375.jpg\n",
      "1375\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([143, 4])\n",
      "Pooled features size: torch.Size([143, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/960.jpg\n",
      "960\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([114, 4])\n",
      "Pooled features size: torch.Size([114, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1790.jpg\n",
      "1790\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([124, 4])\n",
      "Pooled features size: torch.Size([124, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1416.jpg\n",
      "1416\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([119, 4])\n",
      "Pooled features size: torch.Size([119, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1050.jpg\n",
      "1050\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([151, 4])\n",
      "Pooled features size: torch.Size([151, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/627.jpg\n",
      "627\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([167, 4])\n",
      "Pooled features size: torch.Size([167, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/904.jpg\n",
      "904\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([151, 4])\n",
      "Pooled features size: torch.Size([151, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/689.jpg\n",
      "689\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([115, 4])\n",
      "Pooled features size: torch.Size([115, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/610.jpg\n",
      "610\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([128, 4])\n",
      "Pooled features size: torch.Size([128, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/270.jpg\n",
      "270\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([130, 4])\n",
      "Pooled features size: torch.Size([130, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/251.jpg\n",
      "251\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([140, 4])\n",
      "Pooled features size: torch.Size([140, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1183.jpg\n",
      "1183\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([151, 4])\n",
      "Pooled features size: torch.Size([151, 2048])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/961.jpg\n",
      "961\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([146, 4])\n",
      "Pooled features size: torch.Size([146, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/709.jpg\n",
      "709\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([132, 4])\n",
      "Pooled features size: torch.Size([132, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1875.jpg\n",
      "1875\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([128, 4])\n",
      "Pooled features size: torch.Size([128, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1359.jpg\n",
      "1359\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([133, 4])\n",
      "Pooled features size: torch.Size([133, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1343.jpg\n",
      "1343\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([150, 4])\n",
      "Pooled features size: torch.Size([150, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/299.jpg\n",
      "299\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([162, 4])\n",
      "Pooled features size: torch.Size([162, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1846.jpg\n",
      "1846\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([153, 4])\n",
      "Pooled features size: torch.Size([153, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1796.jpg\n",
      "1796\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([124, 4])\n",
      "Pooled features size: torch.Size([124, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/871.jpg\n",
      "871\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([123, 4])\n",
      "Pooled features size: torch.Size([123, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1704.jpg\n",
      "1704\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([140, 4])\n",
      "Pooled features size: torch.Size([140, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/183.jpg\n",
      "183\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([118, 4])\n",
      "Pooled features size: torch.Size([118, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/760.jpg\n",
      "760\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([138, 4])\n",
      "Pooled features size: torch.Size([138, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1361.jpg\n",
      "1361\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([130, 4])\n",
      "Pooled features size: torch.Size([130, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1366.jpg\n",
      "1366\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([116, 4])\n",
      "Pooled features size: torch.Size([116, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/280.jpg\n",
      "280\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([140, 4])\n",
      "Pooled features size: torch.Size([140, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/997.jpg\n",
      "997\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([177, 4])\n",
      "Pooled features size: torch.Size([177, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1737.jpg\n",
      "1737\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([126, 4])\n",
      "Pooled features size: torch.Size([126, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/287.jpg\n",
      "287\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([134, 4])\n",
      "Pooled features size: torch.Size([134, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1353.jpg\n",
      "1353\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([141, 4])\n",
      "Pooled features size: torch.Size([141, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1379.jpg\n",
      "1379\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([152, 4])\n",
      "Pooled features size: torch.Size([152, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/459.jpg\n",
      "459\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([131, 4])\n",
      "Pooled features size: torch.Size([131, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1773.jpg\n",
      "1773\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([151, 4])\n",
      "Pooled features size: torch.Size([151, 2048])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1894.jpg\n",
      "1894\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([148, 4])\n",
      "Pooled features size: torch.Size([148, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1691.jpg\n",
      "1691\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([129, 4])\n",
      "Pooled features size: torch.Size([129, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/868.jpg\n",
      "868\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([121, 4])\n",
      "Pooled features size: torch.Size([121, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/178.jpg\n",
      "178\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([128, 4])\n",
      "Pooled features size: torch.Size([128, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/188.jpg\n",
      "188\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([150, 4])\n",
      "Pooled features size: torch.Size([150, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1016.jpg\n",
      "1016\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([161, 4])\n",
      "Pooled features size: torch.Size([161, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/646.jpg\n",
      "646\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([133, 4])\n",
      "Pooled features size: torch.Size([133, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1914.jpg\n",
      "1914\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([139, 4])\n",
      "Pooled features size: torch.Size([139, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1082.jpg\n",
      "1082\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([135, 4])\n",
      "Pooled features size: torch.Size([135, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1501.jpg\n",
      "1501\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([134, 4])\n",
      "Pooled features size: torch.Size([134, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/431.jpg\n",
      "431\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([126, 4])\n",
      "Pooled features size: torch.Size([126, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1182.jpg\n",
      "1182\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([141, 4])\n",
      "Pooled features size: torch.Size([141, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/332.jpg\n",
      "332\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([161, 4])\n",
      "Pooled features size: torch.Size([161, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/458.jpg\n",
      "458\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([124, 4])\n",
      "Pooled features size: torch.Size([124, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/560.jpg\n",
      "560\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([126, 4])\n",
      "Pooled features size: torch.Size([126, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1460.jpg\n",
      "1460\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([136, 4])\n",
      "Pooled features size: torch.Size([136, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1320.jpg\n",
      "1320\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([125, 4])\n",
      "Pooled features size: torch.Size([125, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1837.jpg\n",
      "1837\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([128, 4])\n",
      "Pooled features size: torch.Size([128, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/345.jpg\n",
      "345\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([168, 4])\n",
      "Pooled features size: torch.Size([168, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1611.jpg\n",
      "1611\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([125, 4])\n",
      "Pooled features size: torch.Size([125, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/652.jpg\n",
      "652\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([148, 4])\n",
      "Pooled features size: torch.Size([148, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/795.jpg\n",
      "795\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([137, 4])\n",
      "Pooled features size: torch.Size([137, 2048])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1166.jpg\n",
      "1166\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([139, 4])\n",
      "Pooled features size: torch.Size([139, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/286.jpg\n",
      "286\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([136, 4])\n",
      "Pooled features size: torch.Size([136, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/914.jpg\n",
      "914\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([120, 4])\n",
      "Pooled features size: torch.Size([120, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1869.jpg\n",
      "1869\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([136, 4])\n",
      "Pooled features size: torch.Size([136, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/232.jpg\n",
      "232\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([139, 4])\n",
      "Pooled features size: torch.Size([139, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/405.jpg\n",
      "405\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([137, 4])\n",
      "Pooled features size: torch.Size([137, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/916.jpg\n",
      "916\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([145, 4])\n",
      "Pooled features size: torch.Size([145, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/415.jpg\n",
      "415\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([136, 4])\n",
      "Pooled features size: torch.Size([136, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1322.jpg\n",
      "1322\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([140, 4])\n",
      "Pooled features size: torch.Size([140, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/464.jpg\n",
      "464\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([134, 4])\n",
      "Pooled features size: torch.Size([134, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1756.jpg\n",
      "1756\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([118, 4])\n",
      "Pooled features size: torch.Size([118, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/452.jpg\n",
      "452\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([138, 4])\n",
      "Pooled features size: torch.Size([138, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/586.jpg\n",
      "586\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([135, 4])\n",
      "Pooled features size: torch.Size([135, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1681.jpg\n",
      "1681\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([142, 4])\n",
      "Pooled features size: torch.Size([142, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/129.jpg\n",
      "129\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([139, 4])\n",
      "Pooled features size: torch.Size([139, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1212.jpg\n",
      "1212\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([124, 4])\n",
      "Pooled features size: torch.Size([124, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/876.jpg\n",
      "876\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([121, 4])\n",
      "Pooled features size: torch.Size([121, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/175.jpg\n",
      "175\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([126, 4])\n",
      "Pooled features size: torch.Size([126, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/662.jpg\n",
      "662\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([142, 4])\n",
      "Pooled features size: torch.Size([142, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/436.jpg\n",
      "436\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([147, 4])\n",
      "Pooled features size: torch.Size([147, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/940.jpg\n",
      "940\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([148, 4])\n",
      "Pooled features size: torch.Size([148, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1715.jpg\n",
      "1715\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([143, 4])\n",
      "Pooled features size: torch.Size([143, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/930.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "930\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([142, 4])\n",
      "Pooled features size: torch.Size([142, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/209.jpg\n",
      "209\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([129, 4])\n",
      "Pooled features size: torch.Size([129, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1114.jpg\n",
      "1114\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([148, 4])\n",
      "Pooled features size: torch.Size([148, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/240.jpg\n",
      "240\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([125, 4])\n",
      "Pooled features size: torch.Size([125, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1654.jpg\n",
      "1654\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([122, 4])\n",
      "Pooled features size: torch.Size([122, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1377.jpg\n",
      "1377\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([156, 4])\n",
      "Pooled features size: torch.Size([156, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/893.jpg\n",
      "893\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([143, 4])\n",
      "Pooled features size: torch.Size([143, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1747.jpg\n",
      "1747\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([116, 4])\n",
      "Pooled features size: torch.Size([116, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/974.jpg\n",
      "974\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([172, 4])\n",
      "Pooled features size: torch.Size([172, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/107.jpg\n",
      "107\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([157, 4])\n",
      "Pooled features size: torch.Size([157, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1241.jpg\n",
      "1241\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([127, 4])\n",
      "Pooled features size: torch.Size([127, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1473.jpg\n",
      "1473\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([160, 4])\n",
      "Pooled features size: torch.Size([160, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1387.jpg\n",
      "1387\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([153, 4])\n",
      "Pooled features size: torch.Size([153, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1721.jpg\n",
      "1721\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([143, 4])\n",
      "Pooled features size: torch.Size([143, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/654.jpg\n",
      "654\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([151, 4])\n",
      "Pooled features size: torch.Size([151, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/47.jpg\n",
      "47\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([170, 4])\n",
      "Pooled features size: torch.Size([170, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1340.jpg\n",
      "1340\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([113, 4])\n",
      "Pooled features size: torch.Size([113, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/540.jpg\n",
      "540\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([130, 4])\n",
      "Pooled features size: torch.Size([130, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/153.jpg\n",
      "153\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([123, 4])\n",
      "Pooled features size: torch.Size([123, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/301.jpg\n",
      "301\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([123, 4])\n",
      "Pooled features size: torch.Size([123, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/750.jpg\n",
      "750\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([134, 4])\n",
      "Pooled features size: torch.Size([134, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1787.jpg\n",
      "1787\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([122, 4])\n",
      "Pooled features size: torch.Size([122, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1012.jpg\n",
      "1012\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([134, 4])\n",
      "Pooled features size: torch.Size([134, 2048])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/792.jpg\n",
      "792\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([129, 4])\n",
      "Pooled features size: torch.Size([129, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/63.jpg\n",
      "63\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([145, 4])\n",
      "Pooled features size: torch.Size([145, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1876.jpg\n",
      "1876\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([147, 4])\n",
      "Pooled features size: torch.Size([147, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1369.jpg\n",
      "1369\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([129, 4])\n",
      "Pooled features size: torch.Size([129, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/356.jpg\n",
      "356\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([126, 4])\n",
      "Pooled features size: torch.Size([126, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/136.jpg\n",
      "136\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([138, 4])\n",
      "Pooled features size: torch.Size([138, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/988.jpg\n",
      "988\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([174, 4])\n",
      "Pooled features size: torch.Size([174, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/396.jpg\n",
      "396\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([137, 4])\n",
      "Pooled features size: torch.Size([137, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1595.jpg\n",
      "1595\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([151, 4])\n",
      "Pooled features size: torch.Size([151, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/102.jpg\n",
      "102\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([116, 4])\n",
      "Pooled features size: torch.Size([116, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1696.jpg\n",
      "1696\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([151, 4])\n",
      "Pooled features size: torch.Size([151, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/543.jpg\n",
      "543\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([126, 4])\n",
      "Pooled features size: torch.Size([126, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/426.jpg\n",
      "426\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([133, 4])\n",
      "Pooled features size: torch.Size([133, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/81.jpg\n",
      "81\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([133, 4])\n",
      "Pooled features size: torch.Size([133, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/985.jpg\n",
      "985\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([119, 4])\n",
      "Pooled features size: torch.Size([119, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/156.jpg\n",
      "156\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([127, 4])\n",
      "Pooled features size: torch.Size([127, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/751.jpg\n",
      "751\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([133, 4])\n",
      "Pooled features size: torch.Size([133, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1339.jpg\n",
      "1339\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([130, 4])\n",
      "Pooled features size: torch.Size([130, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1304.jpg\n",
      "1304\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([141, 4])\n",
      "Pooled features size: torch.Size([141, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1855.jpg\n",
      "1855\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([124, 4])\n",
      "Pooled features size: torch.Size([124, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1147.jpg\n",
      "1147\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([154, 4])\n",
      "Pooled features size: torch.Size([154, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1421.jpg\n",
      "1421\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([136, 4])\n",
      "Pooled features size: torch.Size([136, 2048])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1186.jpg\n",
      "1186\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([123, 4])\n",
      "Pooled features size: torch.Size([123, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/32.jpg\n",
      "32\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([140, 4])\n",
      "Pooled features size: torch.Size([140, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1528.jpg\n",
      "1528\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([134, 4])\n",
      "Pooled features size: torch.Size([134, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/801.jpg\n",
      "801\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([127, 4])\n",
      "Pooled features size: torch.Size([127, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/813.jpg\n",
      "813\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([122, 4])\n",
      "Pooled features size: torch.Size([122, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1572.jpg\n",
      "1572\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([138, 4])\n",
      "Pooled features size: torch.Size([138, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1039.jpg\n",
      "1039\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([133, 4])\n",
      "Pooled features size: torch.Size([133, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1426.jpg\n",
      "1426\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([109, 4])\n",
      "Pooled features size: torch.Size([109, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1906.jpg\n",
      "1906\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([121, 4])\n",
      "Pooled features size: torch.Size([121, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1653.jpg\n",
      "1653\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([122, 4])\n",
      "Pooled features size: torch.Size([122, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/934.jpg\n",
      "934\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([131, 4])\n",
      "Pooled features size: torch.Size([131, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1765.jpg\n",
      "1765\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([147, 4])\n",
      "Pooled features size: torch.Size([147, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/630.jpg\n",
      "630\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([137, 4])\n",
      "Pooled features size: torch.Size([137, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/677.jpg\n",
      "677\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([140, 4])\n",
      "Pooled features size: torch.Size([140, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/978.jpg\n",
      "978\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([147, 4])\n",
      "Pooled features size: torch.Size([147, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/565.jpg\n",
      "565\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([140, 4])\n",
      "Pooled features size: torch.Size([140, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/836.jpg\n",
      "836\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([130, 4])\n",
      "Pooled features size: torch.Size([130, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1588.jpg\n",
      "1588\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([133, 4])\n",
      "Pooled features size: torch.Size([133, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1395.jpg\n",
      "1395\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([145, 4])\n",
      "Pooled features size: torch.Size([145, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1841.jpg\n",
      "1841\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([134, 4])\n",
      "Pooled features size: torch.Size([134, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/523.jpg\n",
      "523\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([127, 4])\n",
      "Pooled features size: torch.Size([127, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/152.jpg\n",
      "152\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([129, 4])\n",
      "Pooled features size: torch.Size([129, 2048])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/397.jpg\n",
      "397\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([134, 4])\n",
      "Pooled features size: torch.Size([134, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/898.jpg\n",
      "898\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([135, 4])\n",
      "Pooled features size: torch.Size([135, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1422.jpg\n",
      "1422\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([143, 4])\n",
      "Pooled features size: torch.Size([143, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/834.jpg\n",
      "834\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([125, 4])\n",
      "Pooled features size: torch.Size([125, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1217.jpg\n",
      "1217\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([130, 4])\n",
      "Pooled features size: torch.Size([130, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/11.jpg\n",
      "11\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([134, 4])\n",
      "Pooled features size: torch.Size([134, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1463.jpg\n",
      "1463\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([145, 4])\n",
      "Pooled features size: torch.Size([145, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/462.jpg\n",
      "462\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([142, 4])\n",
      "Pooled features size: torch.Size([142, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/64.jpg\n",
      "64\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([178, 4])\n",
      "Pooled features size: torch.Size([178, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1446.jpg\n",
      "1446\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([146, 4])\n",
      "Pooled features size: torch.Size([146, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/430.jpg\n",
      "430\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([125, 4])\n",
      "Pooled features size: torch.Size([125, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1518.jpg\n",
      "1518\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([134, 4])\n",
      "Pooled features size: torch.Size([134, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/93.jpg\n",
      "93\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([159, 4])\n",
      "Pooled features size: torch.Size([159, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1335.jpg\n",
      "1335\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([140, 4])\n",
      "Pooled features size: torch.Size([140, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1848.jpg\n",
      "1848\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([147, 4])\n",
      "Pooled features size: torch.Size([147, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1193.jpg\n",
      "1193\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([137, 4])\n",
      "Pooled features size: torch.Size([137, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/174.jpg\n",
      "174\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([136, 4])\n",
      "Pooled features size: torch.Size([136, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/35.jpg\n",
      "35\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([161, 4])\n",
      "Pooled features size: torch.Size([161, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/435.jpg\n",
      "435\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([125, 4])\n",
      "Pooled features size: torch.Size([125, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/346.jpg\n",
      "346\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([146, 4])\n",
      "Pooled features size: torch.Size([146, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1270.jpg\n",
      "1270\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([145, 4])\n",
      "Pooled features size: torch.Size([145, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1030.jpg\n",
      "1030\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([157, 4])\n",
      "Pooled features size: torch.Size([157, 2048])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1887.jpg\n",
      "1887\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([144, 4])\n",
      "Pooled features size: torch.Size([144, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/584.jpg\n",
      "584\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([137, 4])\n",
      "Pooled features size: torch.Size([137, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1084.jpg\n",
      "1084\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([132, 4])\n",
      "Pooled features size: torch.Size([132, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/640.jpg\n",
      "640\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([163, 4])\n",
      "Pooled features size: torch.Size([163, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/552.jpg\n",
      "552\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([115, 4])\n",
      "Pooled features size: torch.Size([115, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1059.jpg\n",
      "1059\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([128, 4])\n",
      "Pooled features size: torch.Size([128, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1847.jpg\n",
      "1847\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([140, 4])\n",
      "Pooled features size: torch.Size([140, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1197.jpg\n",
      "1197\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([168, 4])\n",
      "Pooled features size: torch.Size([168, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/133.jpg\n",
      "133\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([126, 4])\n",
      "Pooled features size: torch.Size([126, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1279.jpg\n",
      "1279\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([170, 4])\n",
      "Pooled features size: torch.Size([170, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/746.jpg\n",
      "746\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([160, 4])\n",
      "Pooled features size: torch.Size([160, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1902.jpg\n",
      "1902\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([146, 4])\n",
      "Pooled features size: torch.Size([146, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1123.jpg\n",
      "1123\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([122, 4])\n",
      "Pooled features size: torch.Size([122, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/388.jpg\n",
      "388\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([126, 4])\n",
      "Pooled features size: torch.Size([126, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1527.jpg\n",
      "1527\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([120, 4])\n",
      "Pooled features size: torch.Size([120, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/527.jpg\n",
      "527\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([156, 4])\n",
      "Pooled features size: torch.Size([156, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/745.jpg\n",
      "745\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([119, 4])\n",
      "Pooled features size: torch.Size([119, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1479.jpg\n",
      "1479\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([127, 4])\n",
      "Pooled features size: torch.Size([127, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1738.jpg\n",
      "1738\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([144, 4])\n",
      "Pooled features size: torch.Size([144, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/139.jpg\n",
      "139\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([131, 4])\n",
      "Pooled features size: torch.Size([131, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1232.jpg\n",
      "1232\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([131, 4])\n",
      "Pooled features size: torch.Size([131, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1511.jpg\n",
      "1511\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([139, 4])\n",
      "Pooled features size: torch.Size([139, 2048])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1005.jpg\n",
      "1005\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([124, 4])\n",
      "Pooled features size: torch.Size([124, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1483.jpg\n",
      "1483\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([149, 4])\n",
      "Pooled features size: torch.Size([149, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/154.jpg\n",
      "154\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([153, 4])\n",
      "Pooled features size: torch.Size([153, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/755.jpg\n",
      "755\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([144, 4])\n",
      "Pooled features size: torch.Size([144, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/800.jpg\n",
      "800\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([153, 4])\n",
      "Pooled features size: torch.Size([153, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1400.jpg\n",
      "1400\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([163, 4])\n",
      "Pooled features size: torch.Size([163, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/1214.jpg\n",
      "1214\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([139, 4])\n",
      "Pooled features size: torch.Size([139, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/499.jpg\n",
      "499\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([137, 4])\n",
      "Pooled features size: torch.Size([137, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n",
      "/data2/zhongkai/VIP/real_world_dataset/random_split/train/images/546.jpg\n",
      "546\n",
      "Original image size:  (2160, 3840)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([122, 4])\n",
      "Pooled features size: torch.Size([122, 2048])\n",
      "Instances(num_instances=30, image_height=2160, image_width=3840, fields=[pred_boxes, scores, pred_classes, attr_scores, attr_classes])\n"
     ]
    }
   ],
   "source": [
    "generate_tsv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41da4928",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb4fc84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd54d19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefbc586",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6659027",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea322a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da5f119",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd2add5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35156ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa05844d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8e6a32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
