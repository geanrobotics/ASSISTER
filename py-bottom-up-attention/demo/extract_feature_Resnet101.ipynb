{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0c4d647",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import csv\n",
    "import base64\n",
    "import detectron2\n",
    "import glob\n",
    "# import some common detectron2 utilities\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog\n",
    "\n",
    "# import some common libraries\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Show the image in ipynb\n",
    "from IPython.display import clear_output, Image, display\n",
    "import PIL.Image\n",
    "def showarray(a, fmt='jpeg'):\n",
    "    a = np.uint8(np.clip(a, 0, 255))\n",
    "    f = io.BytesIO()\n",
    "    PIL.Image.fromarray(a).save(f, fmt)\n",
    "    display(Image(data=f.getvalue()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6382cb0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fbdae9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"../../../real_world_dataset/cross_subject/train/images/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f0a5b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.modeling.postprocessing import detector_postprocess\n",
    "from detectron2.modeling.roi_heads.fast_rcnn import FastRCNNOutputLayers, FastRCNNOutputs, fast_rcnn_inference_single_image\n",
    "\n",
    "def doit(raw_image, NUM_OBJECTS):\n",
    "    with torch.no_grad():\n",
    "        raw_height, raw_width = raw_image.shape[:2]\n",
    "        print(\"Original image size: \", (raw_height, raw_width))\n",
    "        \n",
    "        # Preprocessing\n",
    "        image = predictor.transform_gen.get_transform(raw_image).apply_image(raw_image)\n",
    "        print(\"Transformed image size: \", image.shape[:2])\n",
    "        image = torch.as_tensor(image.astype(\"float32\").transpose(2, 0, 1))\n",
    "        inputs = [{\"image\": image, \"height\": raw_height, \"width\": raw_width}]\n",
    "        images = predictor.model.preprocess_image(inputs)\n",
    "        \n",
    "        # Run Backbone Res1-Res4\n",
    "        features = predictor.model.backbone(images.tensor)\n",
    "        \n",
    "        # Generate proposals with RPN\n",
    "        proposals, _ = predictor.model.proposal_generator(images, features, None)\n",
    "        proposal = proposals[0]\n",
    "        print('Proposal Boxes size:', proposal.proposal_boxes.tensor.shape)\n",
    "        \n",
    "        # Run RoI head for each proposal (RoI Pooling + Res5)\n",
    "        proposal_boxes = [x.proposal_boxes for x in proposals]\n",
    "        features = [features[f] for f in predictor.model.roi_heads.in_features]\n",
    "        box_features = predictor.model.roi_heads._shared_roi_transform(\n",
    "            features, proposal_boxes\n",
    "        )\n",
    "        feature_pooled = box_features.mean(dim=[2, 3])  # pooled to 1x1\n",
    "        print('Pooled features size:', feature_pooled.shape)\n",
    "        \n",
    "        # Predict classes and boxes for each proposal.\n",
    "#         print(predictor.model.roi_heads.box_predictor(feature_pooled))\n",
    "        pred_class_logits, pred_attr_logits, pred_proposal_deltas = predictor.model.roi_heads.box_predictor(feature_pooled)\n",
    "        outputs = FastRCNNOutputs(\n",
    "            predictor.model.roi_heads.box2box_transform,\n",
    "            pred_class_logits,\n",
    "            pred_proposal_deltas,\n",
    "            proposals,\n",
    "            predictor.model.roi_heads.smooth_l1_beta,\n",
    "        )\n",
    "        probs = outputs.predict_probs()[0]\n",
    "        boxes = outputs.predict_boxes()[0]\n",
    "        \n",
    "        attr_prob = pred_attr_logits[..., :-1].softmax(-1)\n",
    "        max_attr_prob, max_attr_label = attr_prob.max(-1)\n",
    "        \n",
    "        # Note: BUTD uses raw RoI predictions,\n",
    "        #       we use the predicted boxes instead.\n",
    "        # boxes = proposal_boxes[0].tensor    \n",
    "        \n",
    "        # NMS\n",
    "        for nms_thresh in np.arange(0.5, 1.0, 0.1):\n",
    "            instances, ids = fast_rcnn_inference_single_image(\n",
    "                boxes, probs, image.shape[1:], \n",
    "                score_thresh=0.2, nms_thresh=nms_thresh, topk_per_image=NUM_OBJECTS\n",
    "            )\n",
    "            if len(ids) == NUM_OBJECTS:\n",
    "                break\n",
    "                \n",
    "        instances = detector_postprocess(instances, raw_height, raw_width)\n",
    "        roi_features = feature_pooled[ids].detach()\n",
    "        max_attr_prob = max_attr_prob[ids].detach()\n",
    "        max_attr_label = max_attr_label[ids].detach()\n",
    "        instances.attr_scores = max_attr_prob\n",
    "        instances.attr_classes = max_attr_label\n",
    "        \n",
    "        print(instances)\n",
    "        \n",
    "        return instances, roi_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce5aec51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MetadataCatalog.get(\"coco\").thing_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9903714b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load VG Classes\n",
    "# data_path = 'data/genome/1600-400-20'\n",
    "\n",
    "# vg_classes = []\n",
    "# with open(os.path.join(data_path, 'objects_vocab.txt')) as f:\n",
    "#     for object in f.readlines():\n",
    "#         vg_classes.append(object.split(',')[0].lower().strip())\n",
    "\n",
    "# MetadataCatalog.get(\"vg\").thing_classes = vg_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7ab9918",
   "metadata": {},
   "outputs": [],
   "source": [
    "COCO_CATEGORIES = [\n",
    "    {\"color\": [220, 20, 60], \"isthing\": 1, \"id\": 1, \"name\": \"person\"},\n",
    "    {\"color\": [119, 11, 32], \"isthing\": 1, \"id\": 2, \"name\": \"bicycle\"},\n",
    "    {\"color\": [0, 0, 142], \"isthing\": 1, \"id\": 3, \"name\": \"car\"},\n",
    "    {\"color\": [0, 0, 230], \"isthing\": 1, \"id\": 4, \"name\": \"motorcycle\"},\n",
    "    {\"color\": [106, 0, 228], \"isthing\": 1, \"id\": 5, \"name\": \"airplane\"},\n",
    "    {\"color\": [0, 60, 100], \"isthing\": 1, \"id\": 6, \"name\": \"bus\"},\n",
    "    {\"color\": [0, 80, 100], \"isthing\": 1, \"id\": 7, \"name\": \"train\"},\n",
    "    {\"color\": [0, 0, 70], \"isthing\": 1, \"id\": 8, \"name\": \"truck\"},\n",
    "    {\"color\": [0, 0, 192], \"isthing\": 1, \"id\": 9, \"name\": \"boat\"},\n",
    "    {\"color\": [250, 170, 30], \"isthing\": 1, \"id\": 10, \"name\": \"traffic light\"},\n",
    "    {\"color\": [100, 170, 30], \"isthing\": 1, \"id\": 11, \"name\": \"fire hydrant\"},\n",
    "    {\"color\": [220, 220, 0], \"isthing\": 1, \"id\": 13, \"name\": \"stop sign\"},\n",
    "    {\"color\": [175, 116, 175], \"isthing\": 1, \"id\": 14, \"name\": \"parking meter\"},\n",
    "    {\"color\": [250, 0, 30], \"isthing\": 1, \"id\": 15, \"name\": \"bench\"},\n",
    "    {\"color\": [165, 42, 42], \"isthing\": 1, \"id\": 16, \"name\": \"bird\"},\n",
    "    {\"color\": [255, 77, 255], \"isthing\": 1, \"id\": 17, \"name\": \"cat\"},\n",
    "    {\"color\": [0, 226, 252], \"isthing\": 1, \"id\": 18, \"name\": \"dog\"},\n",
    "    {\"color\": [182, 182, 255], \"isthing\": 1, \"id\": 19, \"name\": \"horse\"},\n",
    "    {\"color\": [0, 82, 0], \"isthing\": 1, \"id\": 20, \"name\": \"sheep\"},\n",
    "    {\"color\": [120, 166, 157], \"isthing\": 1, \"id\": 21, \"name\": \"cow\"},\n",
    "    {\"color\": [110, 76, 0], \"isthing\": 1, \"id\": 22, \"name\": \"elephant\"},\n",
    "    {\"color\": [174, 57, 255], \"isthing\": 1, \"id\": 23, \"name\": \"bear\"},\n",
    "    {\"color\": [199, 100, 0], \"isthing\": 1, \"id\": 24, \"name\": \"zebra\"},\n",
    "    {\"color\": [72, 0, 118], \"isthing\": 1, \"id\": 25, \"name\": \"giraffe\"},\n",
    "    {\"color\": [255, 179, 240], \"isthing\": 1, \"id\": 27, \"name\": \"backpack\"},\n",
    "    {\"color\": [0, 125, 92], \"isthing\": 1, \"id\": 28, \"name\": \"umbrella\"},\n",
    "    {\"color\": [209, 0, 151], \"isthing\": 1, \"id\": 31, \"name\": \"handbag\"},\n",
    "    {\"color\": [188, 208, 182], \"isthing\": 1, \"id\": 32, \"name\": \"tie\"},\n",
    "    {\"color\": [0, 220, 176], \"isthing\": 1, \"id\": 33, \"name\": \"suitcase\"},\n",
    "    {\"color\": [255, 99, 164], \"isthing\": 1, \"id\": 34, \"name\": \"frisbee\"},\n",
    "    {\"color\": [92, 0, 73], \"isthing\": 1, \"id\": 35, \"name\": \"skis\"},\n",
    "    {\"color\": [133, 129, 255], \"isthing\": 1, \"id\": 36, \"name\": \"snowboard\"},\n",
    "    {\"color\": [78, 180, 255], \"isthing\": 1, \"id\": 37, \"name\": \"sports ball\"},\n",
    "    {\"color\": [0, 228, 0], \"isthing\": 1, \"id\": 38, \"name\": \"kite\"},\n",
    "    {\"color\": [174, 255, 243], \"isthing\": 1, \"id\": 39, \"name\": \"baseball bat\"},\n",
    "    {\"color\": [45, 89, 255], \"isthing\": 1, \"id\": 40, \"name\": \"baseball glove\"},\n",
    "    {\"color\": [134, 134, 103], \"isthing\": 1, \"id\": 41, \"name\": \"skateboard\"},\n",
    "    {\"color\": [145, 148, 174], \"isthing\": 1, \"id\": 42, \"name\": \"surfboard\"},\n",
    "    {\"color\": [255, 208, 186], \"isthing\": 1, \"id\": 43, \"name\": \"tennis racket\"},\n",
    "    {\"color\": [197, 226, 255], \"isthing\": 1, \"id\": 44, \"name\": \"bottle\"},\n",
    "    {\"color\": [171, 134, 1], \"isthing\": 1, \"id\": 46, \"name\": \"wine glass\"},\n",
    "    {\"color\": [109, 63, 54], \"isthing\": 1, \"id\": 47, \"name\": \"cup\"},\n",
    "    {\"color\": [207, 138, 255], \"isthing\": 1, \"id\": 48, \"name\": \"fork\"},\n",
    "    {\"color\": [151, 0, 95], \"isthing\": 1, \"id\": 49, \"name\": \"knife\"},\n",
    "    {\"color\": [9, 80, 61], \"isthing\": 1, \"id\": 50, \"name\": \"spoon\"},\n",
    "    {\"color\": [84, 105, 51], \"isthing\": 1, \"id\": 51, \"name\": \"bowl\"},\n",
    "    {\"color\": [74, 65, 105], \"isthing\": 1, \"id\": 52, \"name\": \"banana\"},\n",
    "    {\"color\": [166, 196, 102], \"isthing\": 1, \"id\": 53, \"name\": \"apple\"},\n",
    "    {\"color\": [208, 195, 210], \"isthing\": 1, \"id\": 54, \"name\": \"sandwich\"},\n",
    "    {\"color\": [255, 109, 65], \"isthing\": 1, \"id\": 55, \"name\": \"orange\"},\n",
    "    {\"color\": [0, 143, 149], \"isthing\": 1, \"id\": 56, \"name\": \"broccoli\"},\n",
    "    {\"color\": [179, 0, 194], \"isthing\": 1, \"id\": 57, \"name\": \"carrot\"},\n",
    "    {\"color\": [209, 99, 106], \"isthing\": 1, \"id\": 58, \"name\": \"hot dog\"},\n",
    "    {\"color\": [5, 121, 0], \"isthing\": 1, \"id\": 59, \"name\": \"pizza\"},\n",
    "    {\"color\": [227, 255, 205], \"isthing\": 1, \"id\": 60, \"name\": \"donut\"},\n",
    "    {\"color\": [147, 186, 208], \"isthing\": 1, \"id\": 61, \"name\": \"cake\"},\n",
    "    {\"color\": [153, 69, 1], \"isthing\": 1, \"id\": 62, \"name\": \"chair\"},\n",
    "    {\"color\": [3, 95, 161], \"isthing\": 1, \"id\": 63, \"name\": \"couch\"},\n",
    "    {\"color\": [163, 255, 0], \"isthing\": 1, \"id\": 64, \"name\": \"potted plant\"},\n",
    "    {\"color\": [119, 0, 170], \"isthing\": 1, \"id\": 65, \"name\": \"bed\"},\n",
    "    {\"color\": [0, 182, 199], \"isthing\": 1, \"id\": 67, \"name\": \"dining table\"},\n",
    "    {\"color\": [0, 165, 120], \"isthing\": 1, \"id\": 70, \"name\": \"toilet\"},\n",
    "    {\"color\": [183, 130, 88], \"isthing\": 1, \"id\": 72, \"name\": \"tv\"},\n",
    "    {\"color\": [95, 32, 0], \"isthing\": 1, \"id\": 73, \"name\": \"laptop\"},\n",
    "    {\"color\": [130, 114, 135], \"isthing\": 1, \"id\": 74, \"name\": \"mouse\"},\n",
    "    {\"color\": [110, 129, 133], \"isthing\": 1, \"id\": 75, \"name\": \"remote\"},\n",
    "    {\"color\": [166, 74, 118], \"isthing\": 1, \"id\": 76, \"name\": \"keyboard\"},\n",
    "    {\"color\": [219, 142, 185], \"isthing\": 1, \"id\": 77, \"name\": \"cell phone\"},\n",
    "    {\"color\": [79, 210, 114], \"isthing\": 1, \"id\": 78, \"name\": \"microwave\"},\n",
    "    {\"color\": [178, 90, 62], \"isthing\": 1, \"id\": 79, \"name\": \"oven\"},\n",
    "    {\"color\": [65, 70, 15], \"isthing\": 1, \"id\": 80, \"name\": \"toaster\"},\n",
    "    {\"color\": [127, 167, 115], \"isthing\": 1, \"id\": 81, \"name\": \"sink\"},\n",
    "    {\"color\": [59, 105, 106], \"isthing\": 1, \"id\": 82, \"name\": \"refrigerator\"},\n",
    "    {\"color\": [142, 108, 45], \"isthing\": 1, \"id\": 84, \"name\": \"book\"},\n",
    "    {\"color\": [196, 172, 0], \"isthing\": 1, \"id\": 85, \"name\": \"clock\"},\n",
    "    {\"color\": [95, 54, 80], \"isthing\": 1, \"id\": 86, \"name\": \"vase\"},\n",
    "    {\"color\": [128, 76, 255], \"isthing\": 1, \"id\": 87, \"name\": \"scissors\"},\n",
    "    {\"color\": [201, 57, 1], \"isthing\": 1, \"id\": 88, \"name\": \"teddy bear\"},\n",
    "    {\"color\": [246, 0, 122], \"isthing\": 1, \"id\": 89, \"name\": \"hair drier\"},\n",
    "    {\"color\": [191, 162, 208], \"isthing\": 1, \"id\": 90, \"name\": \"toothbrush\"},\n",
    "    {\"color\": [255, 255, 128], \"isthing\": 0, \"id\": 92, \"name\": \"banner\"},\n",
    "    {\"color\": [147, 211, 203], \"isthing\": 0, \"id\": 93, \"name\": \"blanket\"},\n",
    "    {\"color\": [150, 100, 100], \"isthing\": 0, \"id\": 95, \"name\": \"bridge\"},\n",
    "    {\"color\": [168, 171, 172], \"isthing\": 0, \"id\": 100, \"name\": \"cardboard\"},\n",
    "    {\"color\": [146, 112, 198], \"isthing\": 0, \"id\": 107, \"name\": \"counter\"},\n",
    "    {\"color\": [210, 170, 100], \"isthing\": 0, \"id\": 109, \"name\": \"curtain\"},\n",
    "    {\"color\": [92, 136, 89], \"isthing\": 0, \"id\": 112, \"name\": \"door-stuff\"},\n",
    "    {\"color\": [218, 88, 184], \"isthing\": 0, \"id\": 118, \"name\": \"floor-wood\"},\n",
    "    {\"color\": [241, 129, 0], \"isthing\": 0, \"id\": 119, \"name\": \"flower\"},\n",
    "    {\"color\": [217, 17, 255], \"isthing\": 0, \"id\": 122, \"name\": \"fruit\"},\n",
    "    {\"color\": [124, 74, 181], \"isthing\": 0, \"id\": 125, \"name\": \"gravel\"},\n",
    "    {\"color\": [70, 70, 70], \"isthing\": 0, \"id\": 128, \"name\": \"house\"},\n",
    "    {\"color\": [255, 228, 255], \"isthing\": 0, \"id\": 130, \"name\": \"light\"},\n",
    "    {\"color\": [154, 208, 0], \"isthing\": 0, \"id\": 133, \"name\": \"mirror-stuff\"},\n",
    "    {\"color\": [193, 0, 92], \"isthing\": 0, \"id\": 138, \"name\": \"net\"},\n",
    "    {\"color\": [76, 91, 113], \"isthing\": 0, \"id\": 141, \"name\": \"pillow\"},\n",
    "    {\"color\": [255, 180, 195], \"isthing\": 0, \"id\": 144, \"name\": \"platform\"},\n",
    "    {\"color\": [106, 154, 176], \"isthing\": 0, \"id\": 145, \"name\": \"playingfield\"},\n",
    "    {\"color\": [230, 150, 140], \"isthing\": 0, \"id\": 147, \"name\": \"railroad\"},\n",
    "    {\"color\": [60, 143, 255], \"isthing\": 0, \"id\": 148, \"name\": \"river\"},\n",
    "    {\"color\": [128, 64, 128], \"isthing\": 0, \"id\": 149, \"name\": \"road\"},\n",
    "    {\"color\": [92, 82, 55], \"isthing\": 0, \"id\": 151, \"name\": \"roof\"},\n",
    "    {\"color\": [254, 212, 124], \"isthing\": 0, \"id\": 154, \"name\": \"sand\"},\n",
    "    {\"color\": [73, 77, 174], \"isthing\": 0, \"id\": 155, \"name\": \"sea\"},\n",
    "    {\"color\": [255, 160, 98], \"isthing\": 0, \"id\": 156, \"name\": \"shelf\"},\n",
    "    {\"color\": [255, 255, 255], \"isthing\": 0, \"id\": 159, \"name\": \"snow\"},\n",
    "    {\"color\": [104, 84, 109], \"isthing\": 0, \"id\": 161, \"name\": \"stairs\"},\n",
    "    {\"color\": [169, 164, 131], \"isthing\": 0, \"id\": 166, \"name\": \"tent\"},\n",
    "    {\"color\": [225, 199, 255], \"isthing\": 0, \"id\": 168, \"name\": \"towel\"},\n",
    "    {\"color\": [137, 54, 74], \"isthing\": 0, \"id\": 171, \"name\": \"wall-brick\"},\n",
    "    {\"color\": [135, 158, 223], \"isthing\": 0, \"id\": 175, \"name\": \"wall-stone\"},\n",
    "    {\"color\": [7, 246, 231], \"isthing\": 0, \"id\": 176, \"name\": \"wall-tile\"},\n",
    "    {\"color\": [107, 255, 200], \"isthing\": 0, \"id\": 177, \"name\": \"wall-wood\"},\n",
    "    {\"color\": [58, 41, 149], \"isthing\": 0, \"id\": 178, \"name\": \"water-other\"},\n",
    "    {\"color\": [183, 121, 142], \"isthing\": 0, \"id\": 180, \"name\": \"window-blind\"},\n",
    "    {\"color\": [255, 73, 97], \"isthing\": 0, \"id\": 181, \"name\": \"window-other\"},\n",
    "    {\"color\": [107, 142, 35], \"isthing\": 0, \"id\": 184, \"name\": \"tree-merged\"},\n",
    "    {\"color\": [190, 153, 153], \"isthing\": 0, \"id\": 185, \"name\": \"fence-merged\"},\n",
    "    {\"color\": [146, 139, 141], \"isthing\": 0, \"id\": 186, \"name\": \"ceiling-merged\"},\n",
    "    {\"color\": [70, 130, 180], \"isthing\": 0, \"id\": 187, \"name\": \"sky-other-merged\"},\n",
    "    {\"color\": [134, 199, 156], \"isthing\": 0, \"id\": 188, \"name\": \"cabinet-merged\"},\n",
    "    {\"color\": [209, 226, 140], \"isthing\": 0, \"id\": 189, \"name\": \"table-merged\"},\n",
    "    {\"color\": [96, 36, 108], \"isthing\": 0, \"id\": 190, \"name\": \"floor-other-merged\"},\n",
    "    {\"color\": [96, 96, 96], \"isthing\": 0, \"id\": 191, \"name\": \"pavement-merged\"},\n",
    "    {\"color\": [64, 170, 64], \"isthing\": 0, \"id\": 192, \"name\": \"mountain-merged\"},\n",
    "    {\"color\": [152, 251, 152], \"isthing\": 0, \"id\": 193, \"name\": \"grass-merged\"},\n",
    "    {\"color\": [208, 229, 228], \"isthing\": 0, \"id\": 194, \"name\": \"dirt-merged\"},\n",
    "    {\"color\": [206, 186, 171], \"isthing\": 0, \"id\": 195, \"name\": \"paper-merged\"},\n",
    "    {\"color\": [152, 161, 64], \"isthing\": 0, \"id\": 196, \"name\": \"food-other-merged\"},\n",
    "    {\"color\": [116, 112, 0], \"isthing\": 0, \"id\": 197, \"name\": \"building-other-merged\"},\n",
    "    {\"color\": [0, 114, 143], \"isthing\": 0, \"id\": 198, \"name\": \"rock-merged\"},\n",
    "    {\"color\": [102, 102, 156], \"isthing\": 0, \"id\": 199, \"name\": \"wall-other-merged\"},\n",
    "    {\"color\": [250, 141, 255], \"isthing\": 0, \"id\": 200, \"name\": \"rug-merged\"},\n",
    "]\n",
    "def _get_coco_instances_meta():\n",
    "    thing_ids = [k[\"id\"] for k in COCO_CATEGORIES if k[\"isthing\"] == 1]\n",
    "    thing_colors = [k[\"color\"] for k in COCO_CATEGORIES if k[\"isthing\"] == 1]\n",
    "    assert len(thing_ids) == 80, len(thing_ids)\n",
    "    # Mapping from the incontiguous COCO category id to an id in [0, 79]\n",
    "    thing_dataset_id_to_contiguous_id = {k: i for i, k in enumerate(thing_ids)}\n",
    "    thing_classes = [k[\"name\"] for k in COCO_CATEGORIES if k[\"isthing\"] == 1]\n",
    "    ret = {\n",
    "        \"thing_dataset_id_to_contiguous_id\": thing_dataset_id_to_contiguous_id,\n",
    "        \"thing_classes\": thing_classes,\n",
    "        \"thing_colors\": thing_colors,\n",
    "    }\n",
    "    return ret\n",
    "vg_classes = _get_coco_instances_meta()[\"thing_classes\"]\n",
    "MetadataCatalog.get(\"vg\").thing_classes = vg_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ae1ec5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config '../configs/VG-Detection/faster_rcnn_R_101_C4_attr_caffemaxpool.yaml' has no VERSION. Assuming it to be compatible with latest v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modifications for VG in ResNet Backbone (modeling/backbone/resnet.py):\n",
      "\tUsing pad 0 in stem max_pool instead of pad 1.\n",
      "\n",
      "Modifications for VG in RPN (modeling/proposal_generator/rpn.py):\n",
      "\tUse hidden dim 512 instead fo the same dim as Res4 (1024).\n",
      "\n",
      "Modifications for VG in RoI heads (modeling/roi_heads/roi_heads.py):\n",
      "\t1. Change the stride of conv1 and shortcut in Res5.Block1 from 2 to 1.\n",
      "\t2. Modifying all conv2 with (padding: 1 --> 2) and (dilation: 1 --> 2).\n",
      "\tFor more details, please check 'https://github.com/peteanderson80/bottom-up-attention/blob/master/models/vg/ResNet-101/faster_rcnn_end2end_final/test.prototxt'.\n",
      "\n",
      "Modifications for VG in RoI heads (modeling/roi_heads/fast_rcnn.py))\n",
      "\tEmbedding: 1601 --> 256\tLinear: 2304 --> 512\tLinear: 512 --> 401\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(\"../configs/VG-Detection/faster_rcnn_R_101_C4_attr_caffemaxpool.yaml\")\n",
    "# cfg.merge_from_file(\"../configs/COCO-Detection/faster_rcnn_R_50_C4_3x.yaml\")\n",
    "# cfg.merge_from_file(\"../configs/COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml\")\n",
    "\n",
    "cfg.MODEL.RPN.POST_NMS_TOPK_TEST = 300\n",
    "cfg.MODEL.ROI_HEADS.NMS_THRESH_TEST = 0.6\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.2\n",
    "# VG Weight\n",
    "cfg.MODEL.WEIGHTS = \"http://nlp.cs.unc.edu/models/faster_rcnn_from_caffe_attr.pkl\"\n",
    "# cfg.MODEL.WEIGHTS = \"https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/faster_rcnn_R_50_C4_3x/137849393/model_final_f97cb7.pkl\"\n",
    "# cfg.MODEL.WEIGHTS = \"https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x/139173657/model_final_68b088.pkl\"\n",
    "predictor = DefaultPredictor(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c7a9c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_feature(image, ids, num_objects=30):\n",
    "    instances, features = doit(image, num_objects)\n",
    "\n",
    "    boxes = instances.pred_boxes.tensor.cpu().numpy()\n",
    "    image_height, image_width = instances.image_size\n",
    "    feature = features.cpu().numpy()\n",
    "\n",
    "    box_width = boxes[:, 2] - boxes[:, 0]\n",
    "    box_height = boxes[:, 3] - boxes[:, 1]\n",
    "    scaled_width = box_width/image_width\n",
    "    scaled_height = box_height/image_height\n",
    "    scaled_x = boxes[:, 0]/image_width\n",
    "    scaled_y = boxes[:, 1] / image_height\n",
    "    scaled_width = scaled_width[..., np.newaxis]\n",
    "    scaled_height = scaled_height[..., np.newaxis]\n",
    "    scaled_x = scaled_x[..., np.newaxis]\n",
    "    scaled_y = scaled_y[..., np.newaxis]\n",
    "\n",
    "    spatial_features = np.concatenate((scaled_x, scaled_y, scaled_x+scaled_width, scaled_y+scaled_height, scaled_width, scaled_height), axis=1)\n",
    "\n",
    "    full_features = np.concatenate((feature, spatial_features), axis=1)\n",
    "    fea_base64 = base64.b64encode(full_features).decode('utf-8')\n",
    "    fea_info = {\"num_boxes\": boxes.shape[0], \"features\": fea_base64}\n",
    "\n",
    "    objects = instances.pred_classes.cpu().numpy()\n",
    "    conf = instances.scores.cpu().numpy()\n",
    "\n",
    "    labels = []\n",
    "    for i in range(len(boxes)):\n",
    "        labels.append({\n",
    "            \"class\": vg_classes[objects[i]],\n",
    "            \"rect\": list(boxes[i, :]), \n",
    "            \"conf\": conf[i]\n",
    "        })\n",
    "    return {'image_id': ids, 'images': fea_info}, {'image_id': ids, 'list':labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f28ab589",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_feature_(rst, ids, num_objects=30):\n",
    "    return {'image_id': ids, 'images': rst[0]['images']}, {'image_id': ids, 'list':rst[1]['list']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c2c5bb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_lineidx_file(filein, idxout):\n",
    "    idxout_tmp = idxout + '.tmp'\n",
    "    with open(filein, 'r') as tsvin, open(idxout_tmp,'w') as tsvout:\n",
    "        fsize = os.fstat(tsvin.fileno()).st_size\n",
    "        fpos = 0\n",
    "        while fpos!=fsize:\n",
    "            tsvout.write(str(fpos)+\"\\n\")\n",
    "            tsvin.readline()\n",
    "            fpos = tsvin.tell()\n",
    "    os.rename(idxout_tmp, idxout)\n",
    "\n",
    "def generate_tsv(feature_path=\"/data2/zhongkai/dataset/sim_features_526/train.feature.tsv\", label_path=\"/data2/zhongkai/dataset/sim_features_526/train.label.tsv\"):\n",
    "    with open(feature_path, 'a') as tsvfile, open(label_path, 'a') as labeltsvfile:\n",
    "        writer = csv.DictWriter(tsvfile, delimiter='\\t', fieldnames=['image_id', 'images'])\n",
    "        Labelwriter=csv.DictWriter(labeltsvfile, delimiter='\\t', fieldnames=['image_id', 'list'])\n",
    "        img_tmp = ''\n",
    "        for i in tqdm(range(df.shape[0])):\n",
    "            img_path = PATH + f\"{df.file_name.iloc[i][1:]}\"\n",
    "            if i%1000 == 0:\n",
    "                print(img_path, img_tmp)\n",
    "            if img_path == img_tmp:\n",
    "                rst = generate_feature_(rst, df.image_id.iloc[i])\n",
    "            else:\n",
    "                im = cv2.imread(img_path)\n",
    "                im_rgb = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "                index = df.image_id.iloc[i]\n",
    "                rst = generate_feature(im_rgb, ids=int(index))\n",
    "                img_tmp = img_path\n",
    "\n",
    "            writer.writerow(rst[0])\n",
    "            Labelwriter.writerow(rst[1])\n",
    "    generate_lineidx_file(feature_path, feature_path.replace(\"tsv\", \"lineidx\"))\n",
    "    generate_lineidx_file(label_path, label_path.replace(\"tsv\", \"lineidx\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a271010",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove '../test/train*.tsv': No such file or directory\r\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "742cd3be7b0e4c6cb7de5405930d4b5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/301677 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/zhongkai/Downloads/batch_05032022_merged/Town05/rgb/000000000000.jpg \n",
      "Original image size:  (1080, 1920)\n",
      "Transformed image size:  (750, 1333)\n",
      "Proposal Boxes size: torch.Size([300, 4])\n",
      "Pooled features size: torch.Size([300, 2048])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrm ../test/train*.tsv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mgenerate_tsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36mgenerate_tsv\u001b[0;34m(feature_path, label_path)\u001b[0m\n\u001b[1;32m     25\u001b[0m     im_rgb \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(im, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[1;32m     26\u001b[0m     index \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mimage_id\u001b[38;5;241m.\u001b[39miloc[i]\n\u001b[0;32m---> 27\u001b[0m     rst \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_feature\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim_rgb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     img_tmp \u001b[38;5;241m=\u001b[39m img_path\n\u001b[1;32m     30\u001b[0m writer\u001b[38;5;241m.\u001b[39mwriterow(rst[\u001b[38;5;241m0\u001b[39m])\n",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36mgenerate_feature\u001b[0;34m(image, ids, num_objects)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_feature\u001b[39m(image, ids, num_objects\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     instances, features \u001b[38;5;241m=\u001b[39m \u001b[43mdoit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_objects\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     boxes \u001b[38;5;241m=\u001b[39m instances\u001b[38;5;241m.\u001b[39mpred_boxes\u001b[38;5;241m.\u001b[39mtensor\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m      5\u001b[0m     image_height, image_width \u001b[38;5;241m=\u001b[39m instances\u001b[38;5;241m.\u001b[39mimage_size\n",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36mdoit\u001b[0;34m(raw_image, NUM_OBJECTS)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPooled features size:\u001b[39m\u001b[38;5;124m'\u001b[39m, feature_pooled\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;66;03m# Predict classes and boxes for each proposal.\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m#         print(predictor.model.roi_heads.box_predictor(feature_pooled))\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m         pred_class_logits, pred_attr_logits, pred_proposal_deltas \u001b[38;5;241m=\u001b[39m predictor\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mroi_heads\u001b[38;5;241m.\u001b[39mbox_predictor(feature_pooled)\n\u001b[1;32m     36\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m FastRCNNOutputs(\n\u001b[1;32m     37\u001b[0m             predictor\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mroi_heads\u001b[38;5;241m.\u001b[39mbox2box_transform,\n\u001b[1;32m     38\u001b[0m             pred_class_logits,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     41\u001b[0m             predictor\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mroi_heads\u001b[38;5;241m.\u001b[39msmooth_l1_beta,\n\u001b[1;32m     42\u001b[0m         )\n\u001b[1;32m     43\u001b[0m         probs \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mpredict_probs()[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "!rm ../test/train*.tsv\n",
    "generate_tsv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8b8f10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d2d735",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08b60a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57400ba9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4488e131",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6b0980",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm ../test/train*.tsv\n",
    "generate_tsv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6c023b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a6ec99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2666f452",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7280b6f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9a97e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d78c62b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27271570",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_lineidx_file(filein, idxout):\n",
    "    idxout_tmp = idxout + '.tmp'\n",
    "    with open(filein, 'r') as tsvin, open(idxout_tmp,'w') as tsvout:\n",
    "        fsize = os.fstat(tsvin.fileno()).st_size\n",
    "        fpos = 0\n",
    "        while fpos!=fsize:\n",
    "            tsvout.write(str(fpos)+\"\\n\")\n",
    "            tsvin.readline()\n",
    "            fpos = tsvin.tell()\n",
    "    os.rename(idxout_tmp, idxout)\n",
    "\n",
    "def generate_tsv(feature_path=\"../test/train.feature.tsv\", label_path=\"../test/train.label.tsv\"):\n",
    "    with open(feature_path, 'a') as tsvfile, open(label_path, 'a') as labeltsvfile:\n",
    "        writer = csv.DictWriter(tsvfile, delimiter='\\t', fieldnames=['image_id', 'images'])\n",
    "        Labelwriter=csv.DictWriter(labeltsvfile, delimiter='\\t', fieldnames=['image_id', 'list'])\n",
    "        for img_path in glob.glob(f\"{PATH}/*.png\"):\n",
    "            print(img_path)\n",
    "            im = cv2.imread(img_path)\n",
    "            im_rgb = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "            index = img_path.split('.')[0].split('/')[-1]\n",
    "            rst = generate_feature(im_rgb, ids=int(index))\n",
    "\n",
    "            writer.writerow(rst[0])\n",
    "            Labelwriter.writerow(rst[1])\n",
    "    generate_lineidx_file(feature_path, feature_path.replace(\"tsv\", \"lineidx\"))\n",
    "    generate_lineidx_file(label_path, label_path.replace(\"tsv\", \"lineidx\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3ef870",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm ../test/train*.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aff4f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_tsv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed69efa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85583004",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad3861a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4060f3c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575ffea1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9792ce2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bba6fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c006eec6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e247a612",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d285ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68a0267",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
